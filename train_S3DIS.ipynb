{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      0=================================0\n",
    "#      |    Kernel Point Convolutions    |\n",
    "#      0=================================0\n",
    "#\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Callable script to start a training on S3DIS dataset\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Hugues THOMAS - 06/03/2020\n",
    "#\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Imports and global variables\n",
    "#       \\**********************************/\n",
    "#\n",
    "\n",
    "# Common libs\n",
    "import signal\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "from datasets.S3DIS import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.trainer import ModelTrainer\n",
    "from models.architectures import KPFCNN\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Config Class\n",
    "#       \\******************/\n",
    "#\n",
    "class S3DISConfig(Config):\n",
    "    \"\"\"\n",
    "    Override the parameters you want to modify for this dataset\n",
    "    \"\"\"\n",
    "\n",
    "    ####################\n",
    "    # Dataset parameters\n",
    "    ####################\n",
    "\n",
    "    # Dataset name\n",
    "    dataset = 'S3DIS'\n",
    "\n",
    "    # Number of classes in the dataset (This value is overwritten by dataset class when Initializating dataset).\n",
    "    num_classes = None\n",
    "\n",
    "    # Type of task performed on this dataset (also overwritten)\n",
    "    dataset_task = ''\n",
    "\n",
    "    # Number of CPU threads for the input pipeline\n",
    "    input_threads = 10  # 10 kuramin changed\n",
    "\n",
    "    #########################\n",
    "    # Architecture definition\n",
    "    #########################\n",
    "\n",
    "    # Define layers\n",
    "    architecture = ['simple',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary']\n",
    "\n",
    "    ###################\n",
    "    # KPConv parameters\n",
    "    ###################\n",
    "\n",
    "    # Radius of the input sphere\n",
    "    in_radius = 1.5\n",
    "\n",
    "    # Number of kernel points\n",
    "    num_kernel_points = 15  # kuramin changed back from 9\n",
    "\n",
    "    # Size of the first subsampling grid in meter\n",
    "    first_subsampling_dl = 0.03\n",
    "\n",
    "    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\n",
    "    conv_radius = 2.5\n",
    "\n",
    "    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\n",
    "    deform_radius = 6.0\n",
    "\n",
    "    # Radius of the area of influence of each kernel point in \"number grid cell\". (1.0 is the standard value)\n",
    "    KP_extent = 1.2\n",
    "\n",
    "    # Behavior of convolutions in ('constant', 'linear', 'gaussian')\n",
    "    KP_influence = 'linear'\n",
    "\n",
    "    # Aggregation function of KPConv in ('closest', 'sum')\n",
    "    aggregation_mode = 'sum'\n",
    "\n",
    "    # Choice of input features\n",
    "    first_features_dim = 128 # kuramin changed back from 8\n",
    "    in_features_dim = 5 # kuramin changed back from 4\n",
    "\n",
    "    # Can the network learn modulations\n",
    "    modulated = False\n",
    "\n",
    "    # Batch normalization parameters\n",
    "    use_batch_norm = True\n",
    "    batch_norm_momentum = 0.02\n",
    "\n",
    "    # Deformable offset loss\n",
    "    # 'point2point' fitting geometry by penalizing distance from deform point to input points\n",
    "    # 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\n",
    "    deform_fitting_mode = 'point2point'\n",
    "    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\n",
    "    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\n",
    "    repulse_extent = 1.2                    # Distance of repulsion for deformed kernel points\n",
    "\n",
    "    #####################\n",
    "    # Training parameters\n",
    "    #####################\n",
    "\n",
    "    # Maximal number of epochs\n",
    "    max_epoch = 1  # 500  kuramin changed\n",
    "\n",
    "    # Learning rate management\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.98\n",
    "    lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}\n",
    "    grad_clip_norm = 100.0\n",
    "\n",
    "    # Number of batch\n",
    "    batch_num = 6  # target_aver_batch_size will be set equal to it\n",
    "\n",
    "    # Number of steps per epoch (how many batches will be created from dataloader by enumerate(dataloader))\n",
    "    steps_per_epoch = 500  # kuramin changed back from 100\n",
    "\n",
    "    # Number of validation examples per epoch\n",
    "    validation_size = 50\n",
    "\n",
    "    # Number of epoch between each checkpoint\n",
    "    checkpoint_gap = 50\n",
    "\n",
    "    # Augmentations\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_symmetries = [True, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_scale_min = 0.8\n",
    "    augment_scale_max = 1.2\n",
    "    augment_noise = 0.001\n",
    "    augment_color = 0.8\n",
    "\n",
    "    # The way we balance segmentation loss\n",
    "    #   > 'none': Each point in the whole batch has the same contribution.\n",
    "    #   > 'class': Each class has the same contribution (points are weighted according to class balance)\n",
    "    #   > 'batch': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)\n",
    "    segloss_balance = 'none'\n",
    "\n",
    "    # Do we need to save convergence\n",
    "    saving = True\n",
    "    saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs is 4\n",
      "GPU_ID is 3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Main Call\n",
    "#       \\***************/\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "############################\n",
    "# Initialize the environment\n",
    "############################\n",
    "\n",
    "# Set which gpu is going to be used\n",
    "number_of_gpus = str(subprocess.check_output([\"nvidia-smi\", \"-L\"])).count('UUID')\n",
    "print('Number of GPUs is', number_of_gpus)\n",
    "\n",
    "if number_of_gpus == 1:\n",
    "    GPU_ID = '0'\n",
    "else:\n",
    "    GPU_ID = '3'\n",
    "print('GPU_ID is', GPU_ID)\n",
    "\n",
    "# Set GPU visible device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Previous chkp\n",
    "###############\n",
    "\n",
    "# Choose here if you want to start training from a previous snapshot (None for new training)\n",
    "# previous_training_path = 'Log_2020-03-19_19-53-27'\n",
    "previous_training_path = ''\n",
    "\n",
    "# Choose index of checkpoint to start from. If None, uses the latest chkp\n",
    "chkp_idx = None\n",
    "if previous_training_path:\n",
    "\n",
    "    # Find all snapshot in the chosen training folder\n",
    "    chkp_path = os.path.join('results', previous_training_path, 'checkpoints')\n",
    "    chkps = [f for f in os.listdir(chkp_path) if f[:4] == 'chkp']\n",
    "\n",
    "    # Find which snapshot to restore\n",
    "    if chkp_idx is None:\n",
    "        chosen_chkp = 'current_chkp.tar'\n",
    "    else:\n",
    "        chosen_chkp = np.sort(chkps)[chkp_idx]\n",
    "    chosen_chkp = os.path.join('results', previous_training_path, 'checkpoints', chosen_chkp)\n",
    "\n",
    "else:\n",
    "    chosen_chkp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preparation\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Prepare Data (several cells)\n",
    "##############\n",
    "\n",
    "print()\n",
    "print('Data Preparation')\n",
    "print('****************')\n",
    "\n",
    "# Initialize configuration class\n",
    "config = S3DISConfig()\n",
    "if previous_training_path:\n",
    "    config.load(os.path.join('results', previous_training_path))\n",
    "    config.saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.saving_path is None\n"
     ]
    }
   ],
   "source": [
    "# Get path from argument if given\n",
    "if len(sys.argv) > 1:\n",
    "    config.saving_path = None  #sys.argv[1]\n",
    "    print('config.saving_path is', config.saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ply-files are already created based on txt-files\n",
      "\n",
      "Found KDTree ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_1_KDTree.pkl for cloud Area_1 with path ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_1.ply, subsampled at 0.030\n",
      "146.6 MB loaded in 0.3s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "Ply-files are already created based on txt-files\n",
      "\n",
      "Found KDTree ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_3_KDTree.pkl for cloud Area_3 with path ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_3.ply, subsampled at 0.030\n",
      "62.6 MB loaded in 0.1s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "Preparing reprojection indices for testing\n",
      "Area_3 done in 0.3s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets\n",
    "training_dataset = S3DISDataset(config, set='training', use_potentials=True)  # kuramin commented\n",
    "test_dataset = S3DISDataset(config, set='validation', use_potentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samplers\n",
    "training_sampler = S3DISSampler(training_dataset)  # defines the strategy to draw samples from the dataset\n",
    "test_sampler = S3DISSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataloader\n",
    "r\"\"\"\n",
    "    dataset (Dataset): dataset from which to load the data.\n",
    "    batch_size (int, optional): how many samples per batch to load\n",
    "        (default: ``1``).\n",
    "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "        at every epoch (default: ``False``).\n",
    "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
    "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
    "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
    "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
    "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
    "    num_workers (int, optional): how many subprocesses to use for data\n",
    "        loading. ``0`` means that the data will be loaded in the main process.\n",
    "        (default: ``0``)\n",
    "    collate_fn (callable, optional): merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset.\n",
    "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "        into CUDA pinned memory before returning them.  If your data elements\n",
    "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "        see the example below.\n",
    "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "        the size of dataset is not divisible by the batch size, then the last batch\n",
    "        will be smaller. (default: ``False``)\n",
    "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "        from workers. Should always be non-negative. (default: ``0``)\n",
    "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "        input, after seeding and before data loading. (default: ``None``)\n",
    "\"\"\"\n",
    "training_loader = DataLoader(training_dataset,\n",
    "                             batch_size=1,\n",
    "                             sampler=training_sampler,\n",
    "                             collate_fn=S3DISCollate,\n",
    "                             num_workers=config.input_threads,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=S3DISCollate,\n",
    "                         num_workers=config.input_threads,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_1.500_0.030_6\": 78801\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"0.030_0.075\": 29\u001b[0m\n",
      "\u001b[92m\"0.060_0.150\": 32\u001b[0m\n",
      "\u001b[92m\"0.120_0.720\": 259\u001b[0m\n",
      "\u001b[92m\"0.240_1.440\": 222\u001b[0m\n",
      "\u001b[92m\"0.480_2.880\": 104\u001b[0m\n",
      "self.dataset.batch_limit tensor([78801.]) self.dataset.neighborhood_limits [29, 32, 259, 222, 104]\n",
      "Calibration done in 0.0s\n",
      "\n",
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_1.500_0.030_6\": 78801\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"0.030_0.075\": 29\u001b[0m\n",
      "\u001b[92m\"0.060_0.150\": 32\u001b[0m\n",
      "\u001b[92m\"0.120_0.720\": 259\u001b[0m\n",
      "\u001b[92m\"0.240_1.440\": 222\u001b[0m\n",
      "\u001b[92m\"0.480_2.880\": 104\u001b[0m\n",
      "self.dataset.batch_limit tensor([78801.]) self.dataset.neighborhood_limits [29, 32, 259, 222, 104]\n",
      "Calibration done in 0.0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibrate samplers\n",
    "training_sampler.calibration(training_loader, verbose=True)\n",
    "test_sampler.calibration(test_loader, verbose=True)\n",
    "\n",
    "# Optional debug functions\n",
    "# debug_timing(training_dataset, training_loader)\n",
    "# debug_timing(test_dataset, test_loader)\n",
    "# debug_upsampling(training_dataset, training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Preparation\n",
      "*****************\n",
      "encoder is ModuleList(\n",
      "  (0): SimpleBlock(\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "    (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (1): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (2): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (3): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (4): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (5): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (6): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (7): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (8): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (9): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (10): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (11): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (12): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (13): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      ")\n",
      "decoder is ModuleList(\n",
      "  (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "  (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "  (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "  (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "  (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "  (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "  (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "  (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('\\nModel Preparation')\n",
    "print('*****************')\n",
    "\n",
    "# Define network model\n",
    "t1 = time.time()\n",
    "net = KPFCNN(config, training_dataset.label_values, training_dataset.ignored_labels)\n",
    "\n",
    "# debug = False\n",
    "# if debug:\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(net)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     for param in net.parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(param.shape)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(\"Model size %i\" % sum(param.numel() for param in net.parameters() if param.requires_grad))\n",
    "#     print('\\n*************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 6.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a trainer class\n",
    "trainer = ModelTrainer(net, config, chkp_path=chosen_chkp)\n",
    "print('Done in {:.1f}s\\n'.format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "**************\n",
      "e000-i0000 => L=14.956 acc=  5% / t(ms): 3476.5 433.3 210.8)\n",
      "e000-i0003 => L=12.693 acc= 17% / t(ms):  54.0 232.3 224.7)\n",
      "e000-i0005 => L=13.392 acc= 35% / t(ms):  54.3 230.1 224.4)\n",
      "e000-i0008 => L=12.205 acc= 37% / t(ms):  53.6 225.6 221.8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-273df7d3ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net, training_loader, val_loader, config)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(self, net, val_loader, config)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_segmentation_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcloud_segmentation_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mcloud_segmentation_validation\u001b[0;34m(self, net, val_loader, config, debug)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlabel_value\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignored_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     self.val_proportions[i] = np.sum([np.sum(labels == label_value)\n\u001b[0;32m--> 321\u001b[0;31m                                                       for labels in val_loader.dataset.validation_labels])\n\u001b[0m\u001b[1;32m    322\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlabel_value\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignored_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     self.val_proportions[i] = np.sum([np.sum(labels == label_value)\n\u001b[0;32m--> 321\u001b[0;31m                                                       for labels in val_loader.dataset.validation_labels])\n\u001b[0m\u001b[1;32m    322\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('\\nStart training')\n",
    "print('**************')\n",
    "\n",
    "# Training\n",
    "trainer.train(net, training_loader, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Forcing exit now')\n",
    "#os.kill(os.getpid(), signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

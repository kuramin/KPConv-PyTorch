{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      0=================================0\n",
    "#      |    Kernel Point Convolutions    |\n",
    "#      0=================================0\n",
    "#\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Callable script to start a training on S3DIS dataset\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Hugues THOMAS - 06/03/2020\n",
    "#\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Imports and global variables\n",
    "#       \\**********************************/\n",
    "#\n",
    "\n",
    "# Common libs\n",
    "import signal\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "from datasets.S3DIS import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.trainer import ModelTrainer\n",
    "from models.architectures import KPFCNN\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Config Class\n",
    "#       \\******************/\n",
    "#\n",
    "class S3DISConfig(Config):\n",
    "    \"\"\"\n",
    "    Override the parameters you want to modify for this dataset\n",
    "    \"\"\"\n",
    "\n",
    "    ####################\n",
    "    # Dataset parameters\n",
    "    ####################\n",
    "\n",
    "    # Dataset name\n",
    "    dataset = 'S3DIS'\n",
    "\n",
    "    # Number of classes in the dataset (This value is overwritten by dataset class when Initializating dataset).\n",
    "    num_classes = None\n",
    "\n",
    "    # Type of task performed on this dataset (also overwritten)\n",
    "    dataset_task = ''\n",
    "\n",
    "    # Number of CPU threads for the input pipeline\n",
    "    input_threads = 10  # 10 kuramin changed\n",
    "\n",
    "    #########################\n",
    "    # Architecture definition\n",
    "    #########################\n",
    "\n",
    "    # Define layers\n",
    "    architecture = ['simple',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary']\n",
    "\n",
    "    ###################\n",
    "    # KPConv parameters\n",
    "    ###################\n",
    "\n",
    "    # Radius of the input sphere\n",
    "    in_radius = 1.5\n",
    "\n",
    "    # Number of kernel points\n",
    "    num_kernel_points = 15  # kuramin changed back from 9\n",
    "\n",
    "    # Size of the first subsampling grid in meter\n",
    "    first_subsampling_dl = 0.03\n",
    "\n",
    "    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\n",
    "    conv_radius = 2.5\n",
    "\n",
    "    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\n",
    "    deform_radius = 6.0\n",
    "\n",
    "    # Radius of the area of influence of each kernel point in \"number grid cell\". (1.0 is the standard value)\n",
    "    KP_extent = 1.2\n",
    "\n",
    "    # Behavior of convolutions in ('constant', 'linear', 'gaussian')\n",
    "    KP_influence = 'linear'\n",
    "\n",
    "    # Aggregation function of KPConv in ('closest', 'sum')\n",
    "    aggregation_mode = 'sum'\n",
    "\n",
    "    # Choice of input features\n",
    "    first_features_dim = 128 # kuramin changed back from 8\n",
    "    in_features_dim = 5 # kuramin changed back from 4\n",
    "\n",
    "    # Can the network learn modulations\n",
    "    modulated = False\n",
    "\n",
    "    # Batch normalization parameters\n",
    "    use_batch_norm = True\n",
    "    batch_norm_momentum = 0.02\n",
    "\n",
    "    # Deformable offset loss\n",
    "    # 'point2point' fitting geometry by penalizing distance from deform point to input points\n",
    "    # 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\n",
    "    deform_fitting_mode = 'point2point'\n",
    "    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\n",
    "    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\n",
    "    repulse_extent = 1.2                    # Distance of repulsion for deformed kernel points\n",
    "\n",
    "    #####################\n",
    "    # Training parameters\n",
    "    #####################\n",
    "\n",
    "    # Maximal number of epochs\n",
    "    max_epoch = 10  # 500  kuramin changed\n",
    "\n",
    "    # Learning rate management\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.98\n",
    "    lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}\n",
    "    grad_clip_norm = 100.0\n",
    "\n",
    "    # Number of batch\n",
    "    batch_num = 6  # target_aver_batch_size will be set equal to it\n",
    "\n",
    "    # Number of steps per epoch (how many batches will be created from dataloader by enumerate(dataloader))\n",
    "    steps_per_epoch = 50  # kuramin changed back from 100\n",
    "\n",
    "    # Number of validation examples per epoch\n",
    "    validation_size = 50\n",
    "\n",
    "    # Number of epoch between each checkpoint\n",
    "    checkpoint_gap = 50\n",
    "\n",
    "    # Augmentations\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_symmetries = [True, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_scale_min = 0.8\n",
    "    augment_scale_max = 1.2\n",
    "    augment_noise = 0.001\n",
    "    augment_color = 0.8\n",
    "\n",
    "    # The way we balance segmentation loss\n",
    "    #   > 'none': Each point in the whole batch has the same contribution.\n",
    "    #   > 'class': Each class has the same contribution (points are weighted according to class balance)\n",
    "    #   > 'batch': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)\n",
    "    segloss_balance = 'none'\n",
    "\n",
    "    # Do we need to save convergence\n",
    "    saving = True\n",
    "    saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs is 4\n",
      "GPU_ID is 3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Main Call\n",
    "#       \\***************/\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "############################\n",
    "# Initialize the environment\n",
    "############################\n",
    "\n",
    "# Set which gpu is going to be used\n",
    "number_of_gpus = str(subprocess.check_output([\"nvidia-smi\", \"-L\"])).count('UUID')\n",
    "print('Number of GPUs is', number_of_gpus)\n",
    "\n",
    "if number_of_gpus == 1:\n",
    "    GPU_ID = '0'\n",
    "else:\n",
    "    GPU_ID = '3'\n",
    "print('GPU_ID is', GPU_ID)\n",
    "\n",
    "# Set GPU visible device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Previous chkp\n",
    "###############\n",
    "\n",
    "# Choose here if you want to start training from a previous snapshot (None for new training)\n",
    "# previous_training_path = 'Log_2020-03-19_19-53-27'\n",
    "previous_training_path = ''\n",
    "\n",
    "# Choose index of checkpoint to start from. If None, uses the latest chkp\n",
    "chkp_idx = None\n",
    "if previous_training_path:\n",
    "\n",
    "    # Find all snapshot in the chosen training folder\n",
    "    chkp_path = os.path.join('results', previous_training_path, 'checkpoints')\n",
    "    chkps = [f for f in os.listdir(chkp_path) if f[:4] == 'chkp']\n",
    "\n",
    "    # Find which snapshot to restore\n",
    "    if chkp_idx is None:\n",
    "        chosen_chkp = 'current_chkp.tar'\n",
    "    else:\n",
    "        chosen_chkp = np.sort(chkps)[chkp_idx]\n",
    "    chosen_chkp = os.path.join('results', previous_training_path, 'checkpoints', chosen_chkp)\n",
    "\n",
    "else:\n",
    "    chosen_chkp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preparation\n",
      "****************\n",
      "self.deform_layers set to [False, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Prepare Data (several cells)\n",
    "##############\n",
    "\n",
    "print()\n",
    "print('Data Preparation')\n",
    "print('****************')\n",
    "\n",
    "# Initialize configuration class\n",
    "config = S3DISConfig()\n",
    "if previous_training_path:\n",
    "    config.load(os.path.join('results', previous_training_path))\n",
    "    config.saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.saving_path is None\n"
     ]
    }
   ],
   "source": [
    "# Get path from argument if given\n",
    "if len(sys.argv) > 1:\n",
    "    config.saving_path = None  #sys.argv[1]\n",
    "    print('config.saving_path is', config.saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.deform_layers set to []\n",
      "Ply-files are already created based on txt-files\n",
      "\n",
      "Found KDTree ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_1_KDTree.pkl for cloud Area_1 with path ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_1.ply, subsampled at 0.030\n",
      "146.6 MB loaded in 0.3s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "self.deform_layers set to []\n",
      "Ply-files are already created based on txt-files\n",
      "\n",
      "Found KDTree ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_3_KDTree.pkl for cloud Area_3 with path ../datasets/Stanford3dDataset_v1.2/input_0.030/Area_3.ply, subsampled at 0.030\n",
      "62.6 MB loaded in 0.1s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "Preparing reprojection indices for testing\n",
      "Area_3 done in 0.3s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets\n",
    "training_dataset = S3DISDataset(config, set='training', use_potentials=True)  # kuramin commented\n",
    "test_dataset = S3DISDataset(config, set='validation', use_potentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samplers\n",
    "training_sampler = S3DISSampler(training_dataset)  # defines the strategy to draw samples from the dataset\n",
    "test_sampler = S3DISSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataloader\n",
    "r\"\"\"\n",
    "    dataset (Dataset): dataset from which to load the data.\n",
    "    batch_size (int, optional): how many samples per batch to load\n",
    "        (default: ``1``).\n",
    "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "        at every epoch (default: ``False``).\n",
    "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
    "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
    "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
    "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
    "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
    "    num_workers (int, optional): how many subprocesses to use for data\n",
    "        loading. ``0`` means that the data will be loaded in the main process.\n",
    "        (default: ``0``)\n",
    "    collate_fn (callable, optional): merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset.\n",
    "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "        into CUDA pinned memory before returning them.  If your data elements\n",
    "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "        see the example below.\n",
    "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "        the size of dataset is not divisible by the batch size, then the last batch\n",
    "        will be smaller. (default: ``False``)\n",
    "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "        from workers. Should always be non-negative. (default: ``0``)\n",
    "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "        input, after seeding and before data loading. (default: ``None``)\n",
    "\"\"\"\n",
    "training_loader = DataLoader(training_dataset,\n",
    "                             batch_size=1,\n",
    "                             sampler=training_sampler,\n",
    "                             collate_fn=S3DISCollate,\n",
    "                             num_workers=config.input_threads,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=S3DISCollate,\n",
    "                         num_workers=config.input_threads,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_1.500_0.030_6\": 78801\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"0.030_0.075\": 29\u001b[0m\n",
      "\u001b[92m\"0.060_0.150\": 32\u001b[0m\n",
      "\u001b[92m\"0.120_0.720\": 259\u001b[0m\n",
      "\u001b[92m\"0.240_1.440\": 222\u001b[0m\n",
      "\u001b[92m\"0.480_2.880\": 104\u001b[0m\n",
      "self.dataset.batch_limit tensor([78801.]) self.dataset.neighborhood_limits [29, 32, 259, 222, 104]\n",
      "Calibration done in 0.0s\n",
      "\n",
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_1.500_0.030_6\": 78801\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"0.030_0.075\": 29\u001b[0m\n",
      "\u001b[92m\"0.060_0.150\": 32\u001b[0m\n",
      "\u001b[92m\"0.120_0.720\": 259\u001b[0m\n",
      "\u001b[92m\"0.240_1.440\": 222\u001b[0m\n",
      "\u001b[92m\"0.480_2.880\": 104\u001b[0m\n",
      "self.dataset.batch_limit tensor([78801.]) self.dataset.neighborhood_limits [29, 32, 259, 222, 104]\n",
      "Calibration done in 0.0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibrate samplers\n",
    "training_sampler.calibration(training_loader, verbose=True)\n",
    "test_sampler.calibration(test_loader, verbose=True)\n",
    "\n",
    "# Optional debug functions\n",
    "# debug_timing(training_dataset, training_loader)\n",
    "# debug_timing(test_dataset, test_loader)\n",
    "# debug_upsampling(training_dataset, training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Preparation\n",
      "*****************\n",
      "encoder_blocks is calculated as ModuleList(\n",
      "  (0): SimpleBlock(\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "    (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (1): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (2): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (3): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (4): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (5): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (6): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (7): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (8): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (9): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (10): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (11): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (12): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (13): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      ")\n",
      "layer after encoder is 4\n",
      "r after encoder is 1.2\n",
      "out_dim after encoder is 2048\n",
      "decoder.blocks is ModuleList(\n",
      "  (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "  (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "  (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "  (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "  (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "  (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "  (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "  (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      ")\n",
      "layer after decoder is 0\n",
      "r after decoder is 0.075\n",
      "out_dim after decoder is 128\n",
      "Initialized the following KPFCNN architecture KPFCNN(\n",
      "  (encoder_blocks): ModuleList(\n",
      "    (0): SimpleBlock(\n",
      "      (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "      (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (1): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (2): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (3): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (4): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (5): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (6): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (7): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (8): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (9): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (10): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (11): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (12): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (13): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "  )\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "    (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "    (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "    (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "    (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "    (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "    (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "    (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      "  )\n",
      "  (head_mlp): UnaryBlock(in_feat: 128, out_feat: 128, BN: False, ReLU: True)\n",
      "  (head_softmax): UnaryBlock(in_feat: 128, out_feat: 13, BN: False, ReLU: True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (l1): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('\\nModel Preparation')\n",
    "print('*****************')\n",
    "\n",
    "# Define network model\n",
    "t1 = time.time()\n",
    "net = KPFCNN(config, training_dataset.label_values, training_dataset.ignored_labels)\n",
    "\n",
    "# debug = False\n",
    "# if debug:\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(net)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     for param in net.parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(param.shape)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(\"Model size %i\" % sum(param.numel() for param in net.parameters() if param.requires_grad))\n",
    "#     print('\\n*************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a trainer class\n",
    "trainer = ModelTrainer(net, config, chkp_path=chosen_chkp)\n",
    "print('Done in {:.1f}s\\n'.format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "**************\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 81467\n",
      "len(input_list[1]) 23784\n",
      "len(input_list[2]) 6103\n",
      "len(input_list[3]) 1635\n",
      "len(input_list[4]) 414\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 82925\n",
      "len(input_list[1]) 24721\n",
      "len(input_list[2]) 6525\n",
      "len(input_list[3]) 1783\n",
      "len(input_list[4]) 468\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 79562\n",
      "len(input_list[1]) 24596\n",
      "len(input_list[2]) 6602\n",
      "len(input_list[3]) 1717\n",
      "len(input_list[4]) 422\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 86931\n",
      "len(input_list[1]) 26880\n",
      "len(input_list[2]) 7089\n",
      "len(input_list[3]) 1798\n",
      "len(input_list[4]) 454\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 81467\n",
      "len(batch.points[1]) 23784\n",
      "len(batch.points[2]) 6103\n",
      "len(batch.points[3]) 1635\n",
      "len(batch.points[4]) 414\n",
      "x.size() before for torch.Size([81467, 5])\n",
      "len(batch.features) 81467\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([81467, 5])\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 86471\n",
      "len(input_list[1]) 25015\n",
      "len(input_list[2]) 6389\n",
      "len(input_list[3]) 1598\n",
      "len(input_list[4]) 413\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 92911\n",
      "len(input_list[1]) 26470\n",
      "len(input_list[2])len(input_list)  687427\n",
      "\n",
      "len(input_list[3])len(input_list[0])  173286953\n",
      "\n",
      "len(input_list[4])len(input_list[1])  44627131\n",
      "\n",
      "len(input_list[2]) 7215\n",
      "len(input_list[3]) 1754\n",
      "len(input_list[4]) 457\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 91725\n",
      "len(input_list[1]) 27215\n",
      "len(input_list[2]) 7165\n",
      "len(input_list[3]) 1784\n",
      "len(input_list[4]) 457\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 94214\n",
      "len(input_list[1]) 29341\n",
      "len(input_list[2]) 7606\n",
      "len(input_list[3]) 1993\n",
      "len(input_list[4]) 538\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 88753\n",
      "len(input_list[1]) 27063\n",
      "len(input_list[2]) 6919\n",
      "len(input_list[3]) 1806\n",
      "len(input_list[4]) 438\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([81467, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([81467, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([81467, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([81467, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([81467, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([23784, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([23784, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([23784, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([23784, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([23784, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([23784, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([23784, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6103, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6103, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6103, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6103, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6103, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6103, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([6103, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1635, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1635, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1635, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1635, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1635, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1635, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1635, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([414, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([414, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([414, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([414, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([414, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-0.0628, -0.0358, -0.1520,  ..., -0.0189,  0.6307, -0.1151],\n",
      "        [-0.0391, -0.0135, -0.0542,  ..., -0.0251,  0.7529, -0.0993],\n",
      "        [-0.0436, -0.0219, -0.1422,  ..., -0.0330,  0.9417, -0.1061],\n",
      "        ...,\n",
      "        [ 0.6301,  0.1072,  0.6173,  ..., -0.0893,  0.6493, -0.0900],\n",
      "        [ 0.8375,  0.0713,  1.0241,  ..., -0.0461,  0.5391, -0.1123],\n",
      "        [ 0.4423,  0.3600,  0.6592,  ..., -0.0694,  0.5655, -0.1181]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 8.9573e-01, -3.2047e-03,  2.9068e-01,  ...,  4.3390e-01,\n",
      "          5.4590e-01,  1.2415e-01],\n",
      "        [ 1.2766e+00,  1.8648e-02,  3.3866e-01,  ...,  6.2118e-01,\n",
      "          5.3686e-01,  2.0411e-01],\n",
      "        [ 1.0061e+00, -3.3701e-02,  3.4307e-02,  ...,  5.0047e-01,\n",
      "          4.6027e-01,  3.6166e-01],\n",
      "        ...,\n",
      "        [-1.0481e-02, -1.1623e-01,  1.2276e+00,  ..., -2.5430e-03,\n",
      "          2.7930e-01,  2.6826e-01],\n",
      "        [ 5.4590e-01, -4.2074e-02,  1.5767e+00,  ...,  5.3597e-01,\n",
      "          5.9525e-02,  2.2712e-01],\n",
      "        [ 4.2709e-02, -8.2418e-02,  1.1660e+00,  ...,  5.0244e-01,\n",
      "          1.8614e-01, -1.0369e-04]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.2324, -0.0911, -0.0542,  ..., -0.0945,  2.1637,  0.5875],\n",
      "        [-0.1020,  0.9284, -0.0903,  ...,  0.4659,  0.6643,  0.1942],\n",
      "        [-0.0061,  0.3868,  0.4603,  ...,  0.4755,  0.8848, -0.0346],\n",
      "        ...,\n",
      "        [-0.0173,  0.1508, -0.1076,  ..., -0.0142,  0.9936,  0.4566],\n",
      "        [ 2.2193,  0.3502, -0.0306,  ...,  1.7064,  2.5072,  2.4881],\n",
      "        [ 0.2216,  0.0952, -0.0470,  ...,  0.0937,  0.0375, -0.1189]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.5726,  1.3889, -0.1700,  ...,  1.4321,  2.1740, -0.2226],\n",
      "        [ 0.5630,  0.1726, -0.0202,  ..., -0.0154,  1.6033,  1.0360],\n",
      "        [-0.0132, -0.0739,  0.6479,  ..., -0.1089,  0.9176,  1.4609],\n",
      "        ...,\n",
      "        [ 0.3724,  0.5988, -0.0690,  ..., -0.0760,  2.4406,  0.5714],\n",
      "        [ 1.6826,  0.1885,  0.4816,  ...,  0.0695,  2.3630,  0.5611],\n",
      "        [ 0.6919, -0.0061, -0.0318,  ..., -0.0297,  2.6872,  0.7792]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 81467\n",
      "skip_x[1] len is 23784\n",
      "skip_x[2] len is 6103\n",
      "skip_x[3] len is 1635\n",
      "e000-i0000 => L=15.036 acc=  2% / t(ms): 3275.0 1401.4 217.4)\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 86931\n",
      "len(batch.points[1]) 26880\n",
      "len(batch.points[2]) 7089\n",
      "len(batch.points[3]) 1798\n",
      "len(batch.points[4]) 454\n",
      "x.size() before for torch.Size([86931, 5])\n",
      "len(batch.features) 86931\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86931, 5])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([86931, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86931, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([86931, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86931, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([86931, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([26880, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([26880, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([26880, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([26880, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([26880, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([26880, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([26880, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7089, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7089, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7089, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7089, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7089, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7089, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([7089, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1798, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1798, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1798, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1798, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1798, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1798, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1798, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([454, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([454, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([454, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([454, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([454, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-0.0959,  0.0873, -0.2303,  ..., -0.0402,  0.6277, -0.1287],\n",
      "        [-0.1576, -0.0614, -0.2151,  ..., -0.1025,  1.4384, -0.1303],\n",
      "        [-0.0992,  0.2404, -0.0897,  ..., -0.0679,  0.8095, -0.1453],\n",
      "        ...,\n",
      "        [-0.0950,  0.3536, -0.1348,  ..., -0.0558,  0.5570, -0.1244],\n",
      "        [-0.0939,  0.2403, -0.0963,  ..., -0.0122,  0.7612, -0.1374],\n",
      "        [-0.1068,  0.5475, -0.0806,  ..., -0.0240,  0.7727, -0.1354]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 0.5797,  0.0960,  0.4754,  ...,  0.2616,  1.3535,  0.1502],\n",
      "        [ 0.7734,  2.6855,  1.1222,  ..., -0.0480,  2.5541,  1.8136],\n",
      "        [ 0.5905,  2.4144,  0.9078,  ...,  0.3829,  0.6666, -0.0061],\n",
      "        ...,\n",
      "        [ 0.1443, -0.0737,  1.1627,  ...,  0.5376,  0.8201,  0.8658],\n",
      "        [ 0.7216, -0.0700, -0.0055,  ..., -0.0039,  0.4285,  0.1344],\n",
      "        [ 0.1855, -0.0977,  0.9882,  ...,  0.3387,  0.7263,  0.8693]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 0.7611,  1.2378, -0.1816,  ...,  1.8003, -0.0075, -0.0166],\n",
      "        [ 2.6036, -0.0648,  0.3765,  ...,  0.1901, -0.0123, -0.0917],\n",
      "        [ 1.6326,  0.7343,  1.0640,  ...,  0.3027,  0.1378,  1.2361],\n",
      "        ...,\n",
      "        [ 1.2577,  1.0723,  1.8260,  ..., -0.1495,  0.9082,  0.1483],\n",
      "        [ 0.2250,  1.0858, -0.0460,  ...,  0.3481,  0.1726,  0.2496],\n",
      "        [ 1.1722,  1.8425, -0.0427,  ...,  0.0334,  0.1977,  0.3611]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 3.5126, -0.1114, -0.2663,  ...,  0.7563,  0.2659,  0.5549],\n",
      "        [-0.0197, -0.0310, -0.0101,  ...,  1.1876,  0.8899, -0.0086],\n",
      "        [ 0.6126, -0.1150,  0.2356,  ..., -0.0786,  1.4227,  0.7808],\n",
      "        ...,\n",
      "        [ 0.8463,  3.3772, -0.1984,  ...,  1.2382, -0.2080,  0.7966],\n",
      "        [ 0.8124,  1.0206,  0.9055,  ...,  1.8955, -0.1224,  1.0024],\n",
      "        [-0.0228,  0.5508, -0.0162,  ...,  0.5723,  0.5394,  0.9088]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 86931\n",
      "skip_x[1] len is 26880\n",
      "skip_x[2] len is 7089\n",
      "skip_x[3] len is 1798\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 82925\n",
      "len(batch.points[1]) 24721\n",
      "len(batch.points[2]) 6525\n",
      "len(batch.points[3]) 1783\n",
      "len(batch.points[4]) 468\n",
      "x.size() before for torch.Size([82925, 5])\n",
      "len(batch.features) 82925\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([82925, 5])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([82925, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([82925, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([82925, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([82925, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([82925, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([24721, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([24721, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([24721, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([24721, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([24721, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([24721, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([24721, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6525, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6525, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6525, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6525, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([6525, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([6525, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([6525, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1783, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1783, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1783, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1783, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1783, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1783, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1783, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([468, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([468, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([468, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([468, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([468, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-0.0809,  0.1912, -0.1375,  ..., -0.0163,  0.9981, -0.0941],\n",
      "        [-0.0590,  0.2731, -0.1784,  ...,  0.0523,  0.9064, -0.0747],\n",
      "        [-0.2017, -0.0398, -0.3355,  ..., -0.1603,  1.0587, -0.0263],\n",
      "        ...,\n",
      "        [-0.0807,  1.2451,  1.9304,  ..., -0.0793, -0.0026,  0.7767],\n",
      "        [-0.0666,  1.6339,  1.7478,  ..., -0.1740, -0.0471,  0.9178],\n",
      "        [-0.0984,  1.4071,  1.4523,  ..., -0.1295, -0.0451,  0.2387]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 7.3220e-01, -2.5352e-02,  1.5262e-01,  ...,  4.4586e-01,\n",
      "          4.1883e-01, -3.6640e-02],\n",
      "        [ 1.4463e+00, -4.3005e-02,  7.7303e-02,  ...,  3.5872e-01,\n",
      "          2.1264e-01, -2.6316e-02],\n",
      "        [ 7.3259e-01, -2.6657e-02,  1.8399e-01,  ...,  2.1017e-01,\n",
      "          1.6834e-01, -1.2175e-02],\n",
      "        ...,\n",
      "        [ 1.1943e-01, -1.9693e-02, -1.9218e-02,  ...,  9.2621e-01,\n",
      "          5.5583e-01,  1.2184e+00],\n",
      "        [-1.2011e-01,  9.0104e+00,  6.8200e+00,  ..., -3.5880e-01,\n",
      "          1.0973e+01, -9.0404e-01],\n",
      "        [-2.4659e-02,  5.4109e-01, -8.9000e-03,  ...,  1.0100e+00,\n",
      "          9.0839e-01,  9.1815e-01]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[-0.0247,  0.3910,  1.5923,  ..., -0.0581,  1.0149,  0.6820],\n",
      "        [ 0.0350,  0.1143,  0.2346,  ..., -0.0575, -0.0603,  1.1604],\n",
      "        [-0.0995,  0.0273,  3.4771,  ..., -0.0142,  2.1782,  1.3372],\n",
      "        ...,\n",
      "        [-0.1200,  0.8578, -0.0334,  ...,  1.6948,  0.1627,  0.4455],\n",
      "        [ 0.8353,  1.6436, -0.0056,  ...,  0.1407,  0.3597,  0.0224],\n",
      "        [ 1.1984,  1.1614,  0.2709,  ..., -0.0176, -0.0092, -0.0173]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[-5.2998e-02,  1.3317e-01, -5.6980e-02,  ...,  6.4796e-01,\n",
      "          4.4666e-01,  1.3600e+00],\n",
      "        [-6.3132e-03,  9.9520e-01,  2.8486e+00,  ...,  3.9046e-02,\n",
      "          3.9068e-01, -4.0700e-02],\n",
      "        [ 7.1917e-01,  1.7693e-01,  6.5346e-02,  ...,  7.4190e-01,\n",
      "          7.5850e-01,  7.2606e-01],\n",
      "        ...,\n",
      "        [ 5.6791e-01,  4.1453e+00,  1.2028e+00,  ...,  7.6539e-01,\n",
      "          6.5783e-01,  1.9025e+00],\n",
      "        [ 1.5242e-01,  2.9215e-01,  5.2667e-01,  ..., -9.0707e-02,\n",
      "          5.6129e-03,  9.1048e-02],\n",
      "        [ 7.7155e+00,  2.5024e+00, -1.8351e-01,  ...,  5.1845e-01,\n",
      "         -1.3046e-01, -7.4084e-03]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 82925\n",
      "skip_x[1] len is 24721\n",
      "skip_x[2] len is 6525\n",
      "skip_x[3] len is 1783\n",
      "e000-i0002 => L=13.740 acc=  1% / t(ms):  67.5 311.9 240.3)\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 86953\n",
      "len(batch.points[1]) 27131\n",
      "len(batch.points[2]) 7215\n",
      "len(batch.points[3]) 1754\n",
      "len(batch.points[4]) 457\n",
      "x.size() before for torch.Size([86953, 5])\n",
      "len(batch.features) 86953\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86953, 5])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([86953, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86953, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([86953, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([86953, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([86953, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27131, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27131, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27131, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27131, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27131, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27131, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([27131, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7215, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7215, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7215, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7215, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7215, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7215, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([7215, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1754, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1754, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1754, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1754, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1754, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1754, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1754, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([457, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([457, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-0.0289, -0.0339, -0.1963,  ...,  1.0772,  1.5530, -0.1402],\n",
      "        [ 0.0762, -0.0758,  0.3648,  ...,  0.2358,  0.7955, -0.1069],\n",
      "        [ 0.3974, -0.1098,  1.1753,  ...,  0.1842, -0.0821, -0.0644],\n",
      "        ...,\n",
      "        [-0.1099, -0.0055,  1.6190,  ...,  1.3600,  2.0730, -0.0529],\n",
      "        [-0.0984,  0.2881,  1.2064,  ...,  0.4679,  1.3673,  1.4495],\n",
      "        [-0.1521, -0.0273,  2.0004,  ...,  1.1404,  1.5647, -0.0560]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 0.5388,  0.0220,  0.2185,  ...,  0.5315,  0.3391,  0.3713],\n",
      "        [ 0.5320, -0.0181,  0.0907,  ...,  0.8947,  0.3998,  0.3035],\n",
      "        [ 0.1155,  0.2455,  0.1254,  ...,  0.8034,  1.2958, -0.0203],\n",
      "        ...,\n",
      "        [ 0.1451, -0.0345,  0.4126,  ...,  0.9929,  0.0678,  0.4641],\n",
      "        [-0.1079,  0.0827, -0.1475,  ..., -0.0818,  0.7401, -0.0279],\n",
      "        [ 0.0764, -0.0466,  0.2937,  ...,  0.9152, -0.0172,  0.8464]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[-0.0460,  1.6299, -0.0385,  ...,  0.4539,  0.7785,  0.5746],\n",
      "        [ 2.2677, -0.0387, -0.0252,  ...,  0.2927, -0.0061,  0.5554],\n",
      "        [ 0.1551,  1.3838,  0.1746,  ...,  0.9394,  0.1369, -0.0197],\n",
      "        ...,\n",
      "        [ 0.8134,  1.9372,  0.1688,  ...,  0.7405,  0.0832,  0.2633],\n",
      "        [-0.0614, -0.2152,  2.1753,  ..., -0.2505,  3.5863,  0.6896],\n",
      "        [ 0.6014,  0.0316,  1.0321,  ..., -0.1425,  1.1158,  1.0478]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.3093e+00,  1.9324e+00, -2.2850e-01,  ...,  1.7018e+00,\n",
      "         -1.0277e-01,  3.8878e+00],\n",
      "        [-6.2458e-02, -2.9920e-02,  2.5403e-01,  ..., -9.0704e-02,\n",
      "          1.4486e-01,  1.0789e+00],\n",
      "        [-5.1174e-02,  2.4902e-01,  1.7992e-01,  ...,  1.0354e-01,\n",
      "          7.2786e-01,  2.1385e+00],\n",
      "        ...,\n",
      "        [ 1.9960e+00, -8.3173e-03,  1.1219e+00,  ...,  6.0062e-01,\n",
      "         -3.4422e-03,  8.6146e-01],\n",
      "        [ 1.9402e+00,  3.7144e+00, -3.0164e-01,  ...,  1.1233e+00,\n",
      "         -5.4448e-01, -4.7813e-02],\n",
      "        [ 1.0858e+00,  1.7671e+00,  1.1224e+00,  ...,  7.9454e-01,\n",
      "         -2.6689e-02,  2.5383e-01]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 86953\n",
      "skip_x[1] len is 27131\n",
      "skip_x[2] len is 7215\n",
      "skip_x[3] len is 1754\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 84143\n",
      "len(input_list[1]) 25209\n",
      "len(input_list[2]) 6430\n",
      "len(input_list[3]) 1656\n",
      "len(input_list[4]) 423\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 80232\n",
      "len(input_list[1]) 22561\n",
      "len(input_list[2]) 6005\n",
      "len(input_list[3]) 1493\n",
      "len(input_list[4]) 391\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 92227\n",
      "len(input_list[1]) 27156\n",
      "len(input_list[2]) 7218\n",
      "len(input_list[3]) 1798\n",
      "len(input_list[4]) 454\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 91349\n",
      "len(input_list[1]) 26823\n",
      "len(input_list[2]) 6891\n",
      "len(input_list[3]) 1693\n",
      "len(input_list[4]) 416\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 94214\n",
      "len(batch.points[1]) 29341\n",
      "len(batch.points[2]) 7606\n",
      "len(batch.points[3]) 1993\n",
      "len(batch.points[4]) 538\n",
      "x.size() before for torch.Size([94214, 5])\n",
      "len(batch.features) 94214\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([94214, 5])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([94214, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([94214, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([94214, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([94214, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([94214, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([29341, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([29341, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([29341, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([29341, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([29341, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([29341, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([29341, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7606, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7606, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7606, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7606, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7606, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7606, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([7606, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1993, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1993, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1993, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1993, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1993, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1993, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1993, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([538, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([538, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([538, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([538, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([538, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-0.0465,  0.8268,  0.6259,  ...,  0.7695,  2.2336, -0.1179],\n",
      "        [ 0.2624,  0.1930,  0.4461,  ..., -0.0239,  2.0843, -0.0499],\n",
      "        [-0.0211,  0.0352,  0.5484,  ..., -0.0137,  1.4375, -0.0363],\n",
      "        ...,\n",
      "        [-0.0127,  0.1914,  1.5436,  ...,  1.9303,  1.7360, -0.0880],\n",
      "        [-0.0854,  0.3156,  2.5005,  ...,  2.4561,  1.3633, -0.0718],\n",
      "        [-0.0966,  0.3703,  2.1864,  ...,  2.5995,  0.8352, -0.0824]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[-2.5289e-02, -1.3539e-01,  6.6790e-02,  ...,  1.2384e+00,\n",
      "          8.6834e-01,  4.9524e-01],\n",
      "        [-6.6424e-02, -3.6878e-02,  1.1509e-01,  ...,  1.7941e+00,\n",
      "          1.3026e+00,  1.3807e-01],\n",
      "        [-5.2013e-02, -3.9659e-02,  8.0115e-02,  ...,  9.4232e-01,\n",
      "          2.4230e-01,  3.5846e-01],\n",
      "        ...,\n",
      "        [ 9.3844e-01,  2.4184e+00,  1.4723e-01,  ...,  3.3468e-01,\n",
      "          6.4274e-01,  8.6990e-02],\n",
      "        [ 8.9696e-01,  2.2742e+00,  5.5772e-01,  ...,  8.3133e-01,\n",
      "          8.8088e-02, -3.3987e-02],\n",
      "        [ 1.2905e+00,  2.5785e+00, -1.0601e-03,  ...,  1.4048e-01,\n",
      "          5.1127e-01, -9.3694e-02]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 2.0524e+00,  3.7989e+00,  6.7274e+00,  ..., -4.1372e-01,\n",
      "          8.1339e+00, -3.7221e-01],\n",
      "        [-1.9500e-02,  9.6595e-01,  7.6967e-01,  ...,  3.4390e-01,\n",
      "         -7.3223e-02, -3.5870e-02],\n",
      "        [-1.7666e-02,  8.4998e-01,  6.1329e-01,  ..., -7.8203e-05,\n",
      "         -6.9879e-02, -1.6323e-02],\n",
      "        ...,\n",
      "        [ 4.5050e-01,  1.7518e-01, -2.4569e-03,  ...,  4.8623e-01,\n",
      "         -4.2377e-02, -5.3618e-02],\n",
      "        [ 8.4021e-01, -2.3188e-01, -1.0840e-01,  ..., -3.6555e-02,\n",
      "          1.1451e+00, -1.0161e-01],\n",
      "        [ 3.8040e-01,  1.1339e-01, -3.9036e-02,  ...,  5.5617e-01,\n",
      "         -3.6756e-02, -6.3241e-02]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 1.4025,  0.0530,  3.1838,  ...,  0.4833,  0.0897,  0.2098],\n",
      "        [ 5.0147,  1.1707, -0.0229,  ...,  2.3989,  0.0928, -0.0480],\n",
      "        [ 0.0352,  0.9355,  1.0430,  ..., -0.0115,  0.7121,  1.2097],\n",
      "        ...,\n",
      "        [ 0.4789, -0.0275,  1.4748,  ..., -0.0221,  0.0989, -0.0094],\n",
      "        [ 1.7534,  0.8999, -0.1675,  ...,  1.8132,  0.2228,  3.0770],\n",
      "        [ 3.1686, -0.1126, -0.0574,  ...,  1.5253, -0.2170,  0.0413]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 94214\n",
      "skip_x[1] len is 29341\n",
      "skip_x[2] len is 7606\n",
      "skip_x[3] len is 1993\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 87716\n",
      "len(input_list[1]) 24969\n",
      "len(input_list[2]) 6383\n",
      "len(input_list[3]) 1613\n",
      "len(input_list[4]) 447\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 84273\n",
      "len(input_list[1]) 24829\n",
      "len(input_list[2]) 6421\n",
      "len(input_list[3]) 1627\n",
      "len(input_list[4]) 407\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 92755\n",
      "len(input_list[1]) 28884\n",
      "len(input_list[2]) 7337\n",
      "len(input_list[3]) 1875\n",
      "len(input_list[4]) 498\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 93570\n",
      "len(input_list[1]) 27317\n",
      "len(input_list[2]) 7335\n",
      "len(input_list[3]) 1794\n",
      "len(input_list[4]) 462\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 86688\n",
      "len(input_list[1]) 26468\n",
      "len(input_list[2]) 6869\n",
      "len(input_list[3]) 1662\n",
      "len(input_list[4]) 401\n",
      "e000-i0004 => L=13.540 acc=  4% / t(ms):  65.6 306.2 244.1)\n",
      "len(batch.points) 5\n",
      "len(batch.points[0]) 91725\n",
      "len(batch.points[1]) 27215\n",
      "len(batch.points[2]) 7165\n",
      "len(batch.points[3]) 1784\n",
      "len(batch.points[4]) 457\n",
      "x.size() before for torch.Size([91725, 5])\n",
      "len(batch.features) 91725\n",
      "len(batch.features[0]) 5\n",
      "len(batch.features[1]) 5\n",
      "len(batch.features[2]) 5\n",
      "len(batch.features[3]) 5\n",
      "len(batch.features[4]) 5\n",
      "len(batch.features[5]) 5\n",
      "\n",
      "In for before if: block_i 0 will apply block_op SimpleBlock(\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 5, out_feat: 64)\n",
      "  (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([91725, 5])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([91725, 64])\n",
      "\n",
      "In for before if: block_i 1 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([91725, 64])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([91725, 128])\n",
      "\n",
      "In for before if: block_i 2 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.07, in_feat: 32, out_feat: 32)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([91725, 128])\n",
      "In for before append: block_i 2 but we append x to skip_x, x.size() is torch.Size([91725, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27215, 128])\n",
      "\n",
      "In for before if: block_i 3 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27215, 128])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27215, 256])\n",
      "\n",
      "In for before if: block_i 4 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27215, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([27215, 256])\n",
      "\n",
      "In for before if: block_i 5 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.15, in_feat: 64, out_feat: 64)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([27215, 256])\n",
      "In for before append: block_i 5 but we append x to skip_x, x.size() is torch.Size([27215, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7165, 256])\n",
      "\n",
      "In for before if: block_i 6 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7165, 256])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7165, 512])\n",
      "\n",
      "In for before if: block_i 7 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7165, 512])\n",
      "len(input_list) 27\n",
      "len(input_list[0]) 93160\n",
      "len(input_list[1]) 30078\n",
      "len(input_list[2]) 7747\n",
      "len(input_list[3]) 2089\n",
      "len(input_list[4]) 508\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([7165, 512])\n",
      "\n",
      "In for before if: block_i 8 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.30, in_feat: 128, out_feat: 128)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([7165, 512])\n",
      "In for before append: block_i 8 but we append x to skip_x, x.size() is torch.Size([7165, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1784, 512])\n",
      "\n",
      "In for before if: block_i 9 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1784, 512])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1784, 1024])\n",
      "\n",
      "In for before if: block_i 10 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1784, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([1784, 1024])\n",
      "\n",
      "In for before if: block_i 11 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 0.60, in_feat: 256, out_feat: 256)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([1784, 1024])\n",
      "In for before append: block_i 11 but we append x to skip_x, x.size() is torch.Size([1784, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 1024])\n",
      "\n",
      "In for before if: block_i 12 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([457, 1024])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 2048])\n",
      "\n",
      "In for before if: block_i 13 will apply block_op ResnetBottleneckBlock(\n",
      "  (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "  (KPConv): KPConv(radius: 1.20, in_feat: 512, out_feat: 512)\n",
      "  (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "  (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "  (unary_shortcut): Identity()\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      ") to x, where x.size() is torch.Size([457, 2048])\n",
      "In for after if: now block is applied to x, x.size() is torch.Size([457, 2048])\n",
      "self.encoder_skips is [2, 5, 8, 11, 14]\n",
      "skip_x is [tensor([[-7.3456e-02,  8.4454e-01, -1.2469e-01,  ..., -6.8826e-02,\n",
      "          4.8417e-01, -7.5321e-02],\n",
      "        [-7.2382e-02,  9.0085e-01, -1.0702e-01,  ..., -3.5188e-02,\n",
      "          1.8200e-01, -7.8736e-02],\n",
      "        [-6.8374e-02,  4.6410e-01, -1.5108e-01,  ..., -3.6384e-02,\n",
      "          6.6266e-01, -9.2328e-02],\n",
      "        ...,\n",
      "        [-2.2946e-01,  1.8659e-01,  1.1885e+00,  ...,  3.5028e-01,\n",
      "         -3.5274e-04,  7.5294e-01],\n",
      "        [-2.6796e-01,  4.4878e-02,  1.6516e+00,  ..., -2.5803e-02,\n",
      "         -4.8209e-02,  9.0048e-01],\n",
      "        [-2.6389e-01,  1.0318e-02,  3.5969e-01,  ...,  2.9761e-01,\n",
      "         -3.7799e-03,  5.6791e-01]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>), tensor([[ 0.9860,  0.0360,  0.4050,  ...,  1.1528,  0.1663, -0.0086],\n",
      "        [ 1.0386, -0.0135,  0.7245,  ...,  0.9323,  0.4259, -0.0071],\n",
      "        [ 0.6005, -0.0140,  0.6966,  ...,  0.9163,  0.5129, -0.0080],\n",
      "        ...,\n",
      "        [ 0.1248,  5.0168,  2.8806,  ..., -0.1132, -0.2571,  2.5526],\n",
      "        [-0.0747,  0.0151, -0.0467,  ..., -0.1080,  1.3172,  0.5838],\n",
      "        [-0.0930,  0.3601, -0.0564,  ..., -0.1059,  1.7170,  0.2870]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 0.6852,  1.1546,  0.0816,  ...,  0.9527, -0.0366,  0.5864],\n",
      "        [-0.0268,  0.5673,  0.6653,  ..., -0.0252, -0.0872, -0.0104],\n",
      "        [-0.0087,  0.6442,  1.0881,  ..., -0.0484, -0.0833,  1.2371],\n",
      "        ...,\n",
      "        [-0.0069,  0.4964, -0.0801,  ...,  0.0195,  0.3467, -0.0365],\n",
      "        [-0.0106, -0.1911,  2.2421,  ..., -0.0944,  0.6306,  0.2855],\n",
      "        [-0.1684, -0.1860,  0.0666,  ...,  4.5634, -0.1199,  0.0315]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>), tensor([[ 1.4025, -0.0538,  1.9708,  ...,  0.4353,  1.3152,  0.7559],\n",
      "        [-0.0059,  0.7592,  1.7801,  ..., -0.0842,  0.0219,  0.5254],\n",
      "        [-0.0889,  0.2709,  0.3515,  ..., -0.0727,  0.8222,  1.2955],\n",
      "        ...,\n",
      "        [-0.0478,  0.8566,  0.0481,  ...,  1.4073, -0.0125, -0.0652],\n",
      "        [ 0.3534, -0.0231,  0.2441,  ..., -0.0967,  0.8995, -0.0165],\n",
      "        [ 3.2656,  0.9098,  0.6726,  ...,  1.0181,  0.5251,  1.6220]],\n",
      "       device='cuda:0', grad_fn=<LeakyReluBackward0>)]\n",
      "skip_x size is 4\n",
      "skip_x[0] len is 91725\n",
      "skip_x[1] len is 27215\n",
      "skip_x[2] len is 7165\n",
      "skip_x[3] len is 1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 294, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/reduction.py\", line 185, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.7/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-273df7d3ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net, training_loader, val_loader, config)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;31m#torch.nn.utils.clip_grad_norm_(net.parameters(), config.grad_clip_norm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('\\nStart training')\n",
    "print('**************')\n",
    "\n",
    "# Training\n",
    "trainer.train(net, training_loader, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Forcing exit now')\n",
    "#os.kill(os.getpid(), signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

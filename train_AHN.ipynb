{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      0=================================0\n",
    "#      |    Kernel Point Convolutions    |\n",
    "#      0=================================0\n",
    "#\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Callable script to start a training on AHN dataset\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Hugues THOMAS - 06/03/2020\n",
    "#\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Imports and global variables\n",
    "#       \\**********************************/\n",
    "#\n",
    "\n",
    "# Common libs\n",
    "import signal\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "from datasets.AHN import *\n",
    "#from datasets.S3DIS import *  # kuramin added\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.trainer import ModelTrainer\n",
    "from models.architectures import KPFCNN\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Config Class\n",
    "#       \\******************/\n",
    "#\n",
    "class AHNConfig(Config):\n",
    "    \"\"\"\n",
    "    Override the parameters you want to modify for this dataset\n",
    "    \"\"\"\n",
    "\n",
    "    ####################\n",
    "    # Dataset parameters\n",
    "    ####################\n",
    "\n",
    "    # Dataset name\n",
    "    dataset = 'AHN'\n",
    "\n",
    "    # Number of classes in the dataset (This value is overwritten by dataset class when Initializating dataset).\n",
    "    num_classes = None\n",
    "\n",
    "    # Type of task performed on this dataset (also overwritten)\n",
    "    dataset_task = ''\n",
    "\n",
    "    # Number of CPU threads for the input pipeline\n",
    "    input_threads = 0  # 10 kuramin changed\n",
    "\n",
    "    #########################\n",
    "    # Architecture definition\n",
    "    #########################\n",
    "\n",
    "    # Define layers\n",
    "    architecture = ['simple',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary']\n",
    "\n",
    "    ###################\n",
    "    # KPConv parameters\n",
    "    ###################\n",
    "\n",
    "    # Radius of the input sphere\n",
    "    in_radius = 15 #1.5 kuramin changed from s3dis to ahn\n",
    "\n",
    "    # Number of kernel points\n",
    "    num_kernel_points = 15  # kuramin changed back from 9\n",
    "\n",
    "    # Size of the first subsampling grid in meter\n",
    "    first_subsampling_dl = 0.3 # 0.5 # was 2.0 #0.03 kuramin changed from s3dis to ahn\n",
    "\n",
    "    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\n",
    "    conv_radius = 2.5\n",
    "\n",
    "    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\n",
    "    deform_radius = 6.0\n",
    "\n",
    "    # Radius of the area of influence of each kernel point in \"number grid cell\". (1.0 is the standard value)\n",
    "    KP_extent = 1.2\n",
    "\n",
    "    # Behavior of convolutions in ('constant', 'linear', 'gaussian')\n",
    "    KP_influence = 'linear'\n",
    "\n",
    "    # Aggregation function of KPConv in ('closest', 'sum')\n",
    "    aggregation_mode = 'sum'\n",
    "\n",
    "    # Choice of input features\n",
    "    first_features_dim = 128 # kuramin changed back from 8\n",
    "    in_features_dim = 5 # kuramin changed back from 4\n",
    "\n",
    "    # Can the network learn modulations\n",
    "    modulated = False\n",
    "\n",
    "    # Batch normalization parameters\n",
    "    use_batch_norm = True\n",
    "    batch_norm_momentum = 0.02\n",
    "\n",
    "    # Deformable offset loss\n",
    "    # 'point2point' fitting geometry by penalizing distance from deform point to input points\n",
    "    # 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\n",
    "    deform_fitting_mode = 'point2point'\n",
    "    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\n",
    "    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\n",
    "    repulse_extent = 1.2                    # Distance of repulsion for deformed kernel points\n",
    "\n",
    "    #####################\n",
    "    # Training parameters\n",
    "    #####################\n",
    "\n",
    "    # Maximal number of epochs\n",
    "    max_epoch = 10  # 500  kuramin changed\n",
    "\n",
    "    # Learning rate management\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.98\n",
    "    lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}\n",
    "    grad_clip_norm = 100.0\n",
    "\n",
    "    # Number of batch\n",
    "    batch_num = 6  # target_aver_batch_size will be set equal to it\n",
    "\n",
    "    # Number of steps per epoch (how many batches will be created from dataloader by enumerate(dataloader))\n",
    "    steps_per_epoch = 100 # 50  # kuramin changed back from 100\n",
    "\n",
    "    # Number of validation examples per epoch\n",
    "    validation_size = 100 # 50\n",
    "\n",
    "    # Number of epoch between each checkpoint\n",
    "    checkpoint_gap = 50\n",
    "\n",
    "    # Augmentations\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_symmetries = [True, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_scale_min = 0.8\n",
    "    augment_scale_max = 1.2\n",
    "    augment_noise = 0.001\n",
    "    augment_color = 0.8\n",
    "\n",
    "    # The way we balance segmentation loss\n",
    "    #   > 'none': Each point in the whole batch has the same contribution.\n",
    "    #   > 'class': Each class has the same contribution (points are weighted according to class balance)\n",
    "    #   > 'batch': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)\n",
    "    segloss_balance = 'none'\n",
    "\n",
    "    # Do we need to save convergence\n",
    "    saving = True\n",
    "    saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs is 4\n",
      "GPU_ID is 3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Main Call\n",
    "#       \\***************/\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "############################\n",
    "# Initialize the environment\n",
    "############################\n",
    "\n",
    "# Set which gpu is going to be used\n",
    "number_of_gpus = str(subprocess.check_output([\"nvidia-smi\", \"-L\"])).count('UUID')\n",
    "print('Number of GPUs is', number_of_gpus)\n",
    "\n",
    "if number_of_gpus == 1:\n",
    "    GPU_ID = '0'\n",
    "else:\n",
    "    GPU_ID = '3'\n",
    "print('GPU_ID is', GPU_ID)\n",
    "\n",
    "# Set GPU visible device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Previous chkp\n",
    "###############\n",
    "\n",
    "# Choose here if you want to start training from a previous snapshot (None for new training)\n",
    "# previous_training_path = 'Log_2020-03-19_19-53-27'\n",
    "previous_training_path = ''\n",
    "\n",
    "# Choose index of checkpoint to start from. If None, uses the latest chkp\n",
    "chkp_idx = None\n",
    "if previous_training_path:\n",
    "\n",
    "    # Find all snapshot in the chosen training folder\n",
    "    chkp_path = os.path.join('results', previous_training_path, 'checkpoints')\n",
    "    chkps = [f for f in os.listdir(chkp_path) if f[:4] == 'chkp']\n",
    "\n",
    "    # Find which snapshot to restore\n",
    "    if chkp_idx is None:\n",
    "        chosen_chkp = 'current_chkp.tar'\n",
    "    else:\n",
    "        chosen_chkp = np.sort(chkps)[chkp_idx]\n",
    "    chosen_chkp = os.path.join('results', previous_training_path, 'checkpoints', chosen_chkp)\n",
    "\n",
    "else:\n",
    "    chosen_chkp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preparation\n",
      "****************\n",
      "self.deform_layers set to [False, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Prepare Data (several cells)\n",
    "##############\n",
    "\n",
    "print()\n",
    "print('Data Preparation')\n",
    "print('****************')\n",
    "\n",
    "# Initialize configuration class\n",
    "config = AHNConfig()\n",
    "if previous_training_path:\n",
    "    config.load(os.path.join('results', previous_training_path))\n",
    "    config.saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.saving_path is None\n"
     ]
    }
   ],
   "source": [
    "# Get path from argument if given\n",
    "if len(sys.argv) > 1:\n",
    "    config.saving_path = None  #sys.argv[1]\n",
    "    print('config.saving_path is', config.saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.deform_layers set to []\n",
      "\n",
      "Preparing KDTree for cloud 1_rgb, subsampled at 0.300\n",
      "labels has size 4530785 hist is (array([1240229, 2729158,       0,       0,       0,  560786,       0,\n",
      "             0,     612]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]))\n",
      "field_list[0].shape[0] is 4152705\n",
      "116.3 MB loaded in 19.7s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 2.0s\n",
      "\n",
      "self.deform_layers set to []\n",
      "\n",
      "Preparing KDTree for cloud 2_rgb, subsampled at 0.300\n",
      "labels has size 5595788 hist is (array([ 939744, 4184252,       0,       0,       0,  471138,       0,\n",
      "             0,     632]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]))\n",
      "field_list[0].shape[0] is 4960123\n",
      "138.9 MB loaded in 23.0s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 2.5s\n",
      "\n",
      "Preparing reprojection indices for testing\n",
      "(5595788,)\n",
      "(5595788,)\n",
      "2_rgb done in 9.6s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets\n",
    "training_dataset = AHNDataset(config, set='training', use_potentials=True)  # kuramin commented\n",
    "test_dataset = AHNDataset(config, set='validation', use_potentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samplers\n",
    "training_sampler = AHNSampler(training_dataset)  # defines the strategy to draw samples from the dataset\n",
    "test_sampler = AHNSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataloader\n",
    "r\"\"\"\n",
    "    dataset (Dataset): dataset from which to load the data.\n",
    "    batch_size (int, optional): how many samples per batch to load\n",
    "        (default: ``1``).\n",
    "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "        at every epoch (default: ``False``).\n",
    "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
    "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
    "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
    "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
    "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
    "    num_workers (int, optional): how many subprocesses to use for data\n",
    "        loading. ``0`` means that the data will be loaded in the main process.\n",
    "        (default: ``0``)\n",
    "    collate_fn (callable, optional): merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset.\n",
    "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "        into CUDA pinned memory before returning them.  If your data elements\n",
    "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "        see the example below.\n",
    "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "        the size of dataset is not divisible by the batch size, then the last batch\n",
    "        will be smaller. (default: ``False``)\n",
    "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "        from workers. Should always be non-negative. (default: ``0``)\n",
    "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "        input, after seeding and before data loading. (default: ``None``)\n",
    "\"\"\"\n",
    "training_loader = DataLoader(training_dataset,\n",
    "                             batch_size=1,\n",
    "                             sampler=training_sampler,\n",
    "                             collate_fn=AHNCollate,\n",
    "                             num_workers=config.input_threads,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=AHNCollate,\n",
    "                         num_workers=config.input_threads,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[91m\"potentials_15.000_0.300_6\": ?\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[91m\"0.300_0.750\": ?\u001b[0m\n",
      "\u001b[91m\"0.600_1.500\": ?\u001b[0m\n",
      "\u001b[91m\"1.200_7.200\": ?\u001b[0m\n",
      "\u001b[91m\"2.400_14.400\": ?\u001b[0m\n",
      "\u001b[91m\"4.800_28.800\": ?\u001b[0m\n",
      "Step     1, estim_aver_bat_size = 0.10, b =  1, bat_lim =    501, error =  5, sm_append = 5.90000, lets_finer =5.90000, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     2, estim_aver_bat_size = 0.19, b =  1, bat_lim =   1001, error =  5, sm_append = 5.81000, lets_finer =5.81000, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     3, estim_aver_bat_size = 0.27, b =  1, bat_lim =   1501, error =  5, sm_append = 5.72900, lets_finer =5.72900, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     4, estim_aver_bat_size = 0.34, b =  1, bat_lim =   2001, error =  5, sm_append = 5.65610, lets_finer =5.65610, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     5, estim_aver_bat_size = 0.41, b =  1, bat_lim =   2501, error =  5, sm_append = 5.59049, lets_finer =5.59049, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     6, estim_aver_bat_size = 0.47, b =  1, bat_lim =   3001, error =  5, sm_append = 5.53144, lets_finer =5.53144, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     7, estim_aver_bat_size = 0.52, b =  1, bat_lim =   3501, error =  5, sm_append = 5.47830, lets_finer =5.47830, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     8, estim_aver_bat_size = 0.57, b =  1, bat_lim =   4001, error =  5, sm_append = 5.43047, lets_finer =5.43047, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step     9, estim_aver_bat_size = 0.61, b =  1, bat_lim =   4501, error =  5, sm_append = 5.38742, lets_finer =5.38742, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step    10, estim_aver_bat_size = 0.65, b =  1, bat_lim =   5001, error =  5, sm_append = 5.34868, lets_finer =5.34868, max_a_sm_err = 5.9000000, low_pass =  10, finer = 0\n",
      "Step    11, estim_aver_bat_size = 0.69, b =  1, bat_lim =   5501, error =  5, sm_append = 5.31381, lets_finer =5.31381, max_a_sm_err = 5.8100000, low_pass =  10, finer = 0\n",
      "Step    12, estim_aver_bat_size = 0.72, b =  1, bat_lim =   6001, error =  5, sm_append = 5.28243, lets_finer =5.28243, max_a_sm_err = 5.7290000, low_pass =  10, finer = 0\n",
      "Step    13, estim_aver_bat_size = 0.85, b =  2, bat_lim =   6401, error =  4, sm_append = 5.15419, lets_finer =5.15419, max_a_sm_err = 5.6561000, low_pass =  10, finer = 0\n",
      "Step    14, estim_aver_bat_size = 0.96, b =  2, bat_lim =   6801, error =  4, sm_append = 5.03877, lets_finer =5.03877, max_a_sm_err = 5.5904900, low_pass =  10, finer = 0\n",
      "Step    15, estim_aver_bat_size = 0.97, b =  1, bat_lim =   7301, error =  5, sm_append = 5.03489, lets_finer =5.03489, max_a_sm_err = 5.5314410, low_pass =  10, finer = 0\n",
      "Step    16, estim_aver_bat_size = 1.07, b =  2, bat_lim =   7701, error =  4, sm_append = 4.93140, lets_finer =4.93140, max_a_sm_err = 5.4782969, low_pass =  10, finer = 0\n",
      "Step    17, estim_aver_bat_size = 1.16, b =  2, bat_lim =   8101, error =  4, sm_append = 4.83826, lets_finer =4.83826, max_a_sm_err = 5.4304672, low_pass =  10, finer = 0\n",
      "Step    18, estim_aver_bat_size = 1.25, b =  2, bat_lim =   8501, error =  4, sm_append = 4.75444, lets_finer =4.75444, max_a_sm_err = 5.3874205, low_pass =  10, finer = 0\n",
      "Step    19, estim_aver_bat_size = 1.32, b =  2, bat_lim =   8901, error =  4, sm_append = 4.67899, lets_finer =4.67899, max_a_sm_err = 5.3486784, low_pass =  10, finer = 0\n",
      "Step    20, estim_aver_bat_size = 1.29, b =  1, bat_lim =   9401, error =  5, sm_append = 4.71109, lets_finer =4.71109, max_a_sm_err = 5.3138106, low_pass =  10, finer = 0\n",
      "Step    21, estim_aver_bat_size = 1.36, b =  2, bat_lim =   9801, error =  4, sm_append = 4.63998, lets_finer =4.63998, max_a_sm_err = 5.2824295, low_pass =  10, finer = 0\n",
      "Step    22, estim_aver_bat_size = 1.42, b =  2, bat_lim =  10201, error =  4, sm_append = 4.57599, lets_finer =4.57599, max_a_sm_err = 5.1541866, low_pass =  10, finer = 0\n",
      "Step    23, estim_aver_bat_size = 1.48, b =  2, bat_lim =  10601, error =  4, sm_append = 4.51839, lets_finer =4.51839, max_a_sm_err = 5.0387679, low_pass =  10, finer = 0\n",
      "Step    24, estim_aver_bat_size = 1.53, b =  2, bat_lim =  11001, error =  4, sm_append = 4.46655, lets_finer =4.46655, max_a_sm_err = 5.0348911, low_pass =  10, finer = 0\n",
      "Step    25, estim_aver_bat_size = 1.58, b =  2, bat_lim =  11401, error =  4, sm_append = 4.41989, lets_finer =4.41989, max_a_sm_err = 4.9314020, low_pass =  10, finer = 0\n",
      "Step    26, estim_aver_bat_size = 1.62, b =  2, bat_lim =  11801, error =  4, sm_append = 4.37790, lets_finer =4.37790, max_a_sm_err = 4.8382618, low_pass =  10, finer = 0\n",
      "Step    27, estim_aver_bat_size = 1.76, b =  3, bat_lim =  12101, error =  3, sm_append = 4.24011, lets_finer =4.24011, max_a_sm_err = 4.7544356, low_pass =  10, finer = 0\n",
      "Step    28, estim_aver_bat_size = 1.78, b =  2, bat_lim =  12501, error =  4, sm_append = 4.21610, lets_finer =4.21610, max_a_sm_err = 4.7110929, low_pass =  10, finer = 0\n",
      "Step    29, estim_aver_bat_size = 1.91, b =  3, bat_lim =  12801, error =  3, sm_append = 4.09449, lets_finer =4.09449, max_a_sm_err = 4.7110929, low_pass =  10, finer = 0\n",
      "Step    30, estim_aver_bat_size = 1.91, b =  2, bat_lim =  13201, error =  4, sm_append = 4.08504, lets_finer =4.08504, max_a_sm_err = 4.6399836, low_pass =  10, finer = 0\n",
      "Step    31, estim_aver_bat_size = 1.92, b =  2, bat_lim =  13601, error =  4, sm_append = 4.07654, lets_finer =4.07654, max_a_sm_err = 4.5759852, low_pass =  10, finer = 0\n",
      "Step    32, estim_aver_bat_size = 2.03, b =  3, bat_lim =  13901, error =  3, sm_append = 3.96888, lets_finer =3.96888, max_a_sm_err = 4.5183867, low_pass =  10, finer = 0\n",
      "Step    33, estim_aver_bat_size = 2.13, b =  3, bat_lim =  14201, error =  3, sm_append = 3.87200, lets_finer =3.87200, max_a_sm_err = 4.4665480, low_pass =  10, finer = 0\n",
      "Step    34, estim_aver_bat_size = 2.22, b =  3, bat_lim =  14501, error =  3, sm_append = 3.78480, lets_finer =3.78480, max_a_sm_err = 4.4198932, low_pass =  10, finer = 0\n",
      "Step    35, estim_aver_bat_size = 2.19, b =  2, bat_lim =  14901, error =  4, sm_append = 3.80632, lets_finer =3.80632, max_a_sm_err = 4.3779039, low_pass =  10, finer = 0\n",
      "Step    36, estim_aver_bat_size = 2.27, b =  3, bat_lim =  15201, error =  3, sm_append = 3.72569, lets_finer =3.72569, max_a_sm_err = 4.2401135, low_pass =  10, finer = 0\n",
      "Step    37, estim_aver_bat_size = 2.25, b =  2, bat_lim =  15601, error =  4, sm_append = 3.75312, lets_finer =3.75312, max_a_sm_err = 4.2161022, low_pass =  10, finer = 0\n",
      "Step    38, estim_aver_bat_size = 2.32, b =  3, bat_lim =  15901, error =  3, sm_append = 3.67781, lets_finer =3.67781, max_a_sm_err = 4.0944919, low_pass =  10, finer = 0\n",
      "Step    39, estim_aver_bat_size = 2.49, b =  4, bat_lim =  16101, error =  2, sm_append = 3.51002, lets_finer =3.51002, max_a_sm_err = 4.0850428, low_pass =  10, finer = 0\n",
      "Step    40, estim_aver_bat_size = 2.54, b =  3, bat_lim =  16401, error =  3, sm_append = 3.45902, lets_finer =3.45902, max_a_sm_err = 4.0765385, low_pass =  10, finer = 0\n",
      "Step    41, estim_aver_bat_size = 2.49, b =  2, bat_lim =  16801, error =  4, sm_append = 3.51312, lets_finer =3.51312, max_a_sm_err = 3.9688846, low_pass =  10, finer = 0\n",
      "Step    42, estim_aver_bat_size = 2.54, b =  3, bat_lim =  17101, error =  3, sm_append = 3.46181, lets_finer =3.46181, max_a_sm_err = 3.8719962, low_pass =  10, finer = 0\n",
      "Step    43, estim_aver_bat_size = 2.58, b =  3, bat_lim =  17401, error =  3, sm_append = 3.41563, lets_finer =3.41563, max_a_sm_err = 3.8063169, low_pass =  10, finer = 0\n",
      "Step    44, estim_aver_bat_size = 2.73, b =  4, bat_lim =  17601, error =  2, sm_append = 3.27406, lets_finer =3.27406, max_a_sm_err = 3.8063169, low_pass =  10, finer = 0\n",
      "Step    45, estim_aver_bat_size = 2.75, b =  3, bat_lim =  17901, error =  3, sm_append = 3.24666, lets_finer =3.24666, max_a_sm_err = 3.7531167, low_pass =  10, finer = 0\n",
      "Step    46, estim_aver_bat_size = 2.78, b =  3, bat_lim =  18201, error =  3, sm_append = 3.22199, lets_finer =3.22199, max_a_sm_err = 3.7531167, low_pass =  10, finer = 0\n",
      "Step    47, estim_aver_bat_size = 2.80, b =  3, bat_lim =  18501, error =  3, sm_append = 3.19979, lets_finer =3.19979, max_a_sm_err = 3.6778050, low_pass =  10, finer = 0\n",
      "Step    48, estim_aver_bat_size = 2.72, b =  2, bat_lim =  18901, error =  4, sm_append = 3.27981, lets_finer =3.27981, max_a_sm_err = 3.5131199, low_pass =  10, finer = 0\n",
      "Step    49, estim_aver_bat_size = 2.75, b =  3, bat_lim =  19201, error =  3, sm_append = 3.25183, lets_finer =3.25183, max_a_sm_err = 3.5131199, low_pass =  10, finer = 0\n",
      "Step    50, estim_aver_bat_size = 2.77, b =  3, bat_lim =  19501, error =  3, sm_append = 3.22665, lets_finer =3.22665, max_a_sm_err = 3.5131199, low_pass =  10, finer = 0\n",
      "Step    51, estim_aver_bat_size = 2.80, b =  3, bat_lim =  19801, error =  3, sm_append = 3.20398, lets_finer =3.20398, max_a_sm_err = 3.4618079, low_pass =  10, finer = 0\n",
      "Step    52, estim_aver_bat_size = 2.92, b =  4, bat_lim =  20001, error =  2, sm_append = 3.08359, lets_finer =3.08359, max_a_sm_err = 3.4156271, low_pass =  10, finer = 0\n",
      "Step    53, estim_aver_bat_size = 3.12, b =  5, bat_lim =  20101, error =  1, sm_append = 2.87523, lets_finer =2.87523, max_a_sm_err = 3.2798136, low_pass =  10, finer = 0\n",
      "Step    54, estim_aver_bat_size = 3.21, b =  4, bat_lim =  20301, error =  2, sm_append = 2.78770, lets_finer =2.78770, max_a_sm_err = 3.2798136, low_pass =  10, finer = 0\n",
      "Step    55, estim_aver_bat_size = 3.29, b =  4, bat_lim =  20501, error =  2, sm_append = 2.70893, lets_finer =2.70893, max_a_sm_err = 3.2798136, low_pass =  10, finer = 0\n",
      "Step    56, estim_aver_bat_size = 3.26, b =  3, bat_lim =  20801, error =  3, sm_append = 2.73804, lets_finer =2.73804, max_a_sm_err = 3.2798136, low_pass =  10, finer = 0\n",
      "Step    57, estim_aver_bat_size = 3.24, b =  3, bat_lim =  21101, error =  3, sm_append = 2.76424, lets_finer =2.76424, max_a_sm_err = 3.2798136, low_pass =  10, finer = 0\n",
      "Step    58, estim_aver_bat_size = 3.21, b =  3, bat_lim =  21401, error =  3, sm_append = 2.78781, lets_finer =2.78781, max_a_sm_err = 3.2518323, low_pass =  10, finer = 0\n",
      "Step    59, estim_aver_bat_size = 3.19, b =  3, bat_lim =  21701, error =  3, sm_append = 2.80903, lets_finer =2.80903, max_a_sm_err = 3.2266490, low_pass =  10, finer = 0\n",
      "Step    60, estim_aver_bat_size = 3.27, b =  4, bat_lim =  21901, error =  2, sm_append = 2.72813, lets_finer =2.72813, max_a_sm_err = 3.2039841, low_pass =  10, finer = 0\n",
      "Step    61, estim_aver_bat_size = 3.34, b =  4, bat_lim =  22101, error =  2, sm_append = 2.65532, lets_finer =2.65532, max_a_sm_err = 3.0835857, low_pass =  10, finer = 0\n",
      "Step    62, estim_aver_bat_size = 3.41, b =  4, bat_lim =  22301, error =  2, sm_append = 2.58978, lets_finer =2.58978, max_a_sm_err = 2.8752272, low_pass =  10, finer = 0\n",
      "Step    63, estim_aver_bat_size = 3.47, b =  4, bat_lim =  22501, error =  2, sm_append = 2.53081, lets_finer =2.53081, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    64, estim_aver_bat_size = 3.52, b =  4, bat_lim =  22701, error =  2, sm_append = 2.47773, lets_finer =2.47773, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    65, estim_aver_bat_size = 3.57, b =  4, bat_lim =  22901, error =  2, sm_append = 2.42995, lets_finer =2.42995, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    66, estim_aver_bat_size = 3.61, b =  4, bat_lim =  23101, error =  2, sm_append = 2.38696, lets_finer =2.38696, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    67, estim_aver_bat_size = 3.75, b =  5, bat_lim =  23201, error =  1, sm_append = 2.24826, lets_finer =2.24826, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    68, estim_aver_bat_size = 3.68, b =  3, bat_lim =  23501, error =  3, sm_append = 2.32344, lets_finer =2.32344, max_a_sm_err = 2.8090316, low_pass =  10, finer = 0\n",
      "Step    69, estim_aver_bat_size = 3.71, b =  4, bat_lim =  23701, error =  2, sm_append = 2.29109, lets_finer =2.29109, max_a_sm_err = 2.7281284, low_pass =  10, finer = 0\n",
      "Step    70, estim_aver_bat_size = 3.84, b =  5, bat_lim =  23801, error =  1, sm_append = 2.16198, lets_finer =2.16198, max_a_sm_err = 2.6553156, low_pass =  10, finer = 0\n",
      "Step    71, estim_aver_bat_size = 3.95, b =  5, bat_lim =  23901, error =  1, sm_append = 2.04578, lets_finer =2.04578, max_a_sm_err = 2.5897840, low_pass =  10, finer = 0\n",
      "Step    72, estim_aver_bat_size = 3.96, b =  4, bat_lim =  24101, error =  2, sm_append = 2.04121, lets_finer =2.04121, max_a_sm_err = 2.5308056, low_pass =  10, finer = 0\n",
      "Step    73, estim_aver_bat_size = 3.86, b =  3, bat_lim =  24401, error =  3, sm_append = 2.13709, lets_finer =2.13709, max_a_sm_err = 2.4777251, low_pass =  10, finer = 0\n",
      "Step    74, estim_aver_bat_size = 3.68, b =  2, bat_lim =  24801, error =  4, sm_append = 2.32338, lets_finer =2.32338, max_a_sm_err = 2.4299526, low_pass =  10, finer = 0\n",
      "Step    75, estim_aver_bat_size = 3.81, b =  5, bat_lim =  24901, error =  1, sm_append = 2.19104, lets_finer =2.19104, max_a_sm_err = 2.3869573, low_pass =  10, finer = 0\n",
      "Step    76, estim_aver_bat_size = 3.83, b =  4, bat_lim =  25101, error =  2, sm_append = 2.17194, lets_finer =2.17194, max_a_sm_err = 2.3234354, low_pass =  10, finer = 0\n",
      "Step    77, estim_aver_bat_size = 4.05, b =  6, bat_lim =  25101, error =  0, sm_append = 1.95474, lets_finer =1.95474, max_a_sm_err = 2.3234354, low_pass =  10, finer = 0\n",
      "Step    78, estim_aver_bat_size = 4.14, b =  5, bat_lim =  25201, error =  1, sm_append = 1.85927, lets_finer =1.85927, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    79, estim_aver_bat_size = 4.23, b =  5, bat_lim =  25301, error =  1, sm_append = 1.77334, lets_finer =1.77334, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    80, estim_aver_bat_size = 4.00, b =  2, bat_lim =  25701, error =  4, sm_append = 1.99601, lets_finer =1.99601, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    81, estim_aver_bat_size = 4.00, b =  4, bat_lim =  25901, error =  2, sm_append = 1.99641, lets_finer =1.99641, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    82, estim_aver_bat_size = 4.10, b =  5, bat_lim =  26001, error =  1, sm_append = 1.89677, lets_finer =1.89677, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    83, estim_aver_bat_size = 4.09, b =  4, bat_lim =  26201, error =  2, sm_append = 1.90709, lets_finer =1.90709, max_a_sm_err = 2.3233768, low_pass =  10, finer = 0\n",
      "Step    84, estim_aver_bat_size = 4.08, b =  4, bat_lim =  26401, error =  2, sm_append = 1.91638, lets_finer =1.91638, max_a_sm_err = 2.1910392, low_pass =  10, finer = 0\n",
      "Step    85, estim_aver_bat_size = 4.18, b =  5, bat_lim =  26501, error =  1, sm_append = 1.82474, lets_finer =1.82474, max_a_sm_err = 2.1719352, low_pass =  10, finer = 0\n",
      "Step    86, estim_aver_bat_size = 4.26, b =  5, bat_lim =  26601, error =  1, sm_append = 1.74227, lets_finer =1.74227, max_a_sm_err = 1.9964060, low_pass =  10, finer = 0\n",
      "Step    87, estim_aver_bat_size = 4.33, b =  5, bat_lim =  26701, error =  1, sm_append = 1.66804, lets_finer =1.66804, max_a_sm_err = 1.9964060, low_pass =  10, finer = 0\n",
      "Step    88, estim_aver_bat_size = 4.40, b =  5, bat_lim =  26801, error =  1, sm_append = 1.60124, lets_finer =1.60124, max_a_sm_err = 1.9964060, low_pass =  10, finer = 0\n",
      "Step    89, estim_aver_bat_size = 4.36, b =  4, bat_lim =  27001, error =  2, sm_append = 1.64111, lets_finer =1.64111, max_a_sm_err = 1.9964060, low_pass =  10, finer = 0\n",
      "Step    90, estim_aver_bat_size = 4.32, b =  4, bat_lim =  27201, error =  2, sm_append = 1.67700, lets_finer =1.67700, max_a_sm_err = 1.9964060, low_pass =  10, finer = 0\n",
      "Step    91, estim_aver_bat_size = 4.39, b =  5, bat_lim =  27301, error =  1, sm_append = 1.60930, lets_finer =1.60930, max_a_sm_err = 1.9163800, low_pass =  10, finer = 0\n",
      "Step    92, estim_aver_bat_size = 4.35, b =  4, bat_lim =  27501, error =  2, sm_append = 1.64837, lets_finer =1.64837, max_a_sm_err = 1.9163800, low_pass =  10, finer = 0\n",
      "Step    93, estim_aver_bat_size = 4.32, b =  4, bat_lim =  27701, error =  2, sm_append = 1.68353, lets_finer =1.68353, max_a_sm_err = 1.9163800, low_pass =  10, finer = 0\n",
      "Step    94, estim_aver_bat_size = 4.28, b =  4, bat_lim =  27901, error =  2, sm_append = 1.71518, lets_finer =1.71518, max_a_sm_err = 1.8247420, low_pass =  10, finer = 0\n",
      "Step    95, estim_aver_bat_size = 4.16, b =  3, bat_lim =  28201, error =  3, sm_append = 1.84366, lets_finer =1.84366, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step    96, estim_aver_bat_size = 4.24, b =  5, bat_lim =  28301, error =  1, sm_append = 1.75930, lets_finer =1.75930, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step    97, estim_aver_bat_size = 4.32, b =  5, bat_lim =  28401, error =  1, sm_append = 1.68337, lets_finer =1.68337, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step    98, estim_aver_bat_size = 4.18, b =  3, bat_lim =  28701, error =  3, sm_append = 1.81503, lets_finer =1.81503, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step    99, estim_aver_bat_size = 4.27, b =  5, bat_lim =  28801, error =  1, sm_append = 1.73353, lets_finer =1.73353, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   100, estim_aver_bat_size = 4.34, b =  5, bat_lim =  28901, error =  1, sm_append = 1.66017, lets_finer =1.66017, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   101, estim_aver_bat_size = 4.41, b =  5, bat_lim =  29001, error =  1, sm_append = 1.59416, lets_finer =1.59416, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   102, estim_aver_bat_size = 4.37, b =  4, bat_lim =  29201, error =  2, sm_append = 1.63474, lets_finer =1.63474, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   103, estim_aver_bat_size = 4.43, b =  5, bat_lim =  29301, error =  1, sm_append = 1.57127, lets_finer =1.57127, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   104, estim_aver_bat_size = 4.29, b =  3, bat_lim =  29601, error =  3, sm_append = 1.71414, lets_finer =1.71414, max_a_sm_err = 1.8436629, low_pass =  10, finer = 0\n",
      "Step   105, estim_aver_bat_size = 4.46, b =  6, bat_lim =  29601, error =  0, sm_append = 1.54273, lets_finer =1.54273, max_a_sm_err = 1.8150302, low_pass =  10, finer = 0\n",
      "Step   106, estim_aver_bat_size = 4.61, b =  6, bat_lim =  29601, error =  0, sm_append = 1.38845, lets_finer =1.38845, max_a_sm_err = 1.8150302, low_pass =  10, finer = 0\n",
      "Step   107, estim_aver_bat_size = 4.65, b =  5, bat_lim =  29701, error =  1, sm_append = 1.34961, lets_finer =1.34961, max_a_sm_err = 1.8150302, low_pass =  10, finer = 0\n",
      "Step   108, estim_aver_bat_size = 4.69, b =  5, bat_lim =  29801, error =  1, sm_append = 1.31465, lets_finer =1.31465, max_a_sm_err = 1.7335272, low_pass =  10, finer = 0\n",
      "Step   109, estim_aver_bat_size = 4.72, b =  5, bat_lim =  29901, error =  1, sm_append = 1.28318, lets_finer =1.28318, max_a_sm_err = 1.7141405, low_pass =  10, finer = 0\n",
      "Step   110, estim_aver_bat_size = 4.85, b =  6, bat_lim =  29901, error =  0, sm_append = 1.15486, lets_finer =1.15486, max_a_sm_err = 1.7141405, low_pass =  10, finer = 0\n",
      "Step   111, estim_aver_bat_size = 4.76, b =  4, bat_lim =  30101, error =  2, sm_append = 1.23938, lets_finer =1.23938, max_a_sm_err = 1.7141405, low_pass =  10, finer = 0\n",
      "Step   112, estim_aver_bat_size = 4.88, b =  6, bat_lim =  30101, error =  0, sm_append = 1.11544, lets_finer =1.11544, max_a_sm_err = 1.7141405, low_pass =  10, finer = 0\n",
      "Step   113, estim_aver_bat_size = 4.80, b =  4, bat_lim =  30301, error =  2, sm_append = 1.20390, lets_finer =1.20390, max_a_sm_err = 1.7141405, low_pass =  10, finer = 0\n",
      "Step   114, estim_aver_bat_size = 4.72, b =  4, bat_lim =  30501, error =  2, sm_append = 1.28351, lets_finer =1.28351, max_a_sm_err = 1.5427264, low_pass =  10, finer = 0\n",
      "Step   115, estim_aver_bat_size = 4.74, b =  5, bat_lim =  30601, error =  1, sm_append = 1.25516, lets_finer =1.25516, max_a_sm_err = 1.3884538, low_pass =  10, finer = 0\n",
      "Step   116, estim_aver_bat_size = 4.87, b =  6, bat_lim =  30601, error =  0, sm_append = 1.12964, lets_finer =1.12964, max_a_sm_err = 1.3496084, low_pass =  10, finer = 0\n",
      "Step   117, estim_aver_bat_size = 4.88, b =  5, bat_lim =  30701, error =  1, sm_append = 1.11668, lets_finer =1.11668, max_a_sm_err = 1.3146476, low_pass =  10, finer = 0\n",
      "Step   118, estim_aver_bat_size = 4.99, b =  6, bat_lim =  30701, error =  0, sm_append = 1.00501, lets_finer =1.00501, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   119, estim_aver_bat_size = 5.00, b =  5, bat_lim =  30801, error =  1, sm_append = 1.00451, lets_finer =1.00451, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   120, estim_aver_bat_size = 4.90, b =  4, bat_lim =  31001, error =  2, sm_append = 1.10406, lets_finer =1.10406, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   121, estim_aver_bat_size = 4.81, b =  4, bat_lim =  31201, error =  2, sm_append = 1.19365, lets_finer =1.19365, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   122, estim_aver_bat_size = 4.93, b =  6, bat_lim =  31201, error =  0, sm_append = 1.07429, lets_finer =1.07429, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   123, estim_aver_bat_size = 4.83, b =  4, bat_lim =  31401, error =  2, sm_append = 1.16686, lets_finer =1.16686, max_a_sm_err = 1.2835066, low_pass =  10, finer = 0\n",
      "Step   124, estim_aver_bat_size = 4.95, b =  6, bat_lim =  31401, error =  0, sm_append = 1.05017, lets_finer =1.05017, max_a_sm_err = 1.2551560, low_pass =  10, finer = 0\n",
      "Step   125, estim_aver_bat_size = 4.85, b =  4, bat_lim =  31601, error =  2, sm_append = 1.14515, lets_finer =1.14515, max_a_sm_err = 1.1936513, low_pass =  10, finer = 0\n",
      "Step   126, estim_aver_bat_size = 4.87, b =  5, bat_lim =  31701, error =  1, sm_append = 1.13064, lets_finer =1.13064, max_a_sm_err = 1.1936513, low_pass =  10, finer = 0\n",
      "Step   127, estim_aver_bat_size = 4.88, b =  5, bat_lim =  31801, error =  1, sm_append = 1.11758, lets_finer =1.11758, max_a_sm_err = 1.1936513, low_pass =  10, finer = 0\n",
      "Step   128, estim_aver_bat_size = 4.99, b =  6, bat_lim =  31801, error =  0, sm_append = 1.00582, lets_finer =1.00582, max_a_sm_err = 1.1936513, low_pass =  10, finer = 0\n",
      "Step   129, estim_aver_bat_size = 5.19, b =  7, bat_lim =  31701, error = -1, sm_append = 0.80524, lets_finer =0.80524, max_a_sm_err = 1.1936513, low_pass = 100, finer = 1\n",
      "Step   130, estim_aver_bat_size = 5.19, b =  5, bat_lim =  31801, error =  1, sm_append = 0.80718, lets_finer =0.80718, max_a_sm_err = 1.1936513, low_pass = 100, finer = 1\n",
      "Step   131, estim_aver_bat_size = 5.20, b =  6, bat_lim =  31801, error =  0, sm_append = 0.79911, lets_finer =0.79911, max_a_sm_err = 1.1668576, low_pass = 100, finer = 1\n",
      "Step   132, estim_aver_bat_size = 5.20, b =  5, bat_lim =  31901, error =  1, sm_append = 0.80112, lets_finer =0.80112, max_a_sm_err = 1.1668576, low_pass = 100, finer = 1\n",
      "Step   133, estim_aver_bat_size = 5.19, b =  4, bat_lim =  32101, error =  2, sm_append = 0.81311, lets_finer =0.81311, max_a_sm_err = 1.1451546, low_pass = 100, finer = 1\n",
      "Step   134, estim_aver_bat_size = 5.19, b =  5, bat_lim =  32201, error =  1, sm_append = 0.81498, lets_finer =0.81498, max_a_sm_err = 1.1451546, low_pass = 100, finer = 1\n",
      "Step   135, estim_aver_bat_size = 5.19, b =  6, bat_lim =  32201, error =  0, sm_append = 0.80683, lets_finer =0.80683, max_a_sm_err = 1.1306392, low_pass = 100, finer = 1\n",
      "Step   136, estim_aver_bat_size = 5.20, b =  6, bat_lim =  32201, error =  0, sm_append = 0.79876, lets_finer =0.79876, max_a_sm_err = 1.1175753, low_pass = 100, finer = 1\n",
      "Step   137, estim_aver_bat_size = 5.22, b =  7, bat_lim =  32101, error = -1, sm_append = 0.78077, lets_finer =0.78077, max_a_sm_err = 1.0058177, low_pass = 100, finer = 1\n",
      "Step   138, estim_aver_bat_size = 5.22, b =  5, bat_lim =  32201, error =  1, sm_append = 0.78296, lets_finer =0.78296, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   139, estim_aver_bat_size = 5.23, b =  7, bat_lim =  32101, error = -1, sm_append = 0.76514, lets_finer =0.76514, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   140, estim_aver_bat_size = 5.25, b =  7, bat_lim =  32001, error = -1, sm_append = 0.74748, lets_finer =0.74748, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   141, estim_aver_bat_size = 5.26, b =  6, bat_lim =  32001, error =  0, sm_append = 0.74001, lets_finer =0.74001, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   142, estim_aver_bat_size = 5.27, b =  6, bat_lim =  32001, error =  0, sm_append = 0.73261, lets_finer =0.73261, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   143, estim_aver_bat_size = 5.28, b =  7, bat_lim =  31901, error = -1, sm_append = 0.71528, lets_finer =0.71528, max_a_sm_err = 0.8149783, low_pass = 100, finer = 1\n",
      "Step   144, estim_aver_bat_size = 5.29, b =  6, bat_lim =  31901, error =  0, sm_append = 0.70813, lets_finer =0.70813, max_a_sm_err = 0.8068286, low_pass = 100, finer = 1\n",
      "Step   145, estim_aver_bat_size = 5.30, b =  6, bat_lim =  31901, error =  0, sm_append = 0.70105, lets_finer =0.70105, max_a_sm_err = 0.7987603, low_pass = 100, finer = 1\n",
      "Step   146, estim_aver_bat_size = 5.29, b =  4, bat_lim =  32101, error =  2, sm_append = 0.71404, lets_finer =0.71404, max_a_sm_err = 0.7829649, low_pass = 100, finer = 1\n",
      "Step   147, estim_aver_bat_size = 5.28, b =  5, bat_lim =  32201, error =  1, sm_append = 0.71690, lets_finer =0.71690, max_a_sm_err = 0.7829649, low_pass = 100, finer = 1\n",
      "Step   148, estim_aver_bat_size = 5.29, b =  6, bat_lim =  32201, error =  0, sm_append = 0.70973, lets_finer =0.70973, max_a_sm_err = 0.7651353, low_pass = 100, finer = 1\n",
      "Step   149, estim_aver_bat_size = 5.30, b =  6, bat_lim =  32201, error =  0, sm_append = 0.70263, lets_finer =0.70263, max_a_sm_err = 0.7474839, low_pass = 100, finer = 1\n",
      "Step   150, estim_aver_bat_size = 5.28, b =  4, bat_lim =  32401, error =  2, sm_append = 0.71561, lets_finer =0.71561, max_a_sm_err = 0.7400091, low_pass = 100, finer = 1\n",
      "Step   151, estim_aver_bat_size = 5.29, b =  6, bat_lim =  32401, error =  0, sm_append = 0.70845, lets_finer =0.70845, max_a_sm_err = 0.7326090, low_pass = 100, finer = 1\n",
      "Step   152, estim_aver_bat_size = 5.31, b =  7, bat_lim =  32301, error = -1, sm_append = 0.69136, lets_finer =0.69136, max_a_sm_err = 0.7168979, low_pass = 100, finer = 1\n",
      "Step   153, estim_aver_bat_size = 5.31, b =  5, bat_lim =  32401, error =  1, sm_append = 0.69445, lets_finer =0.69445, max_a_sm_err = 0.7168979, low_pass = 100, finer = 1\n",
      "Step   154, estim_aver_bat_size = 5.31, b =  6, bat_lim =  32401, error =  0, sm_append = 0.68751, lets_finer =0.68751, max_a_sm_err = 0.7168979, low_pass = 100, finer = 1\n",
      "Step   155, estim_aver_bat_size = 5.32, b =  6, bat_lim =  32401, error =  0, sm_append = 0.68063, lets_finer =0.68063, max_a_sm_err = 0.7168979, low_pass = 100, finer = 1\n",
      "Step   156, estim_aver_bat_size = 5.33, b =  6, bat_lim =  32401, error =  0, sm_append = 0.67383, lets_finer =0.67383, max_a_sm_err = 0.7168979, low_pass = 100, finer = 1\n",
      "Step   157, estim_aver_bat_size = 5.32, b =  5, bat_lim =  32501, error =  1, sm_append = 0.67709, lets_finer =0.67709, max_a_sm_err = 0.7156053, low_pass = 100, finer = 1\n",
      "Step   158, estim_aver_bat_size = 5.32, b =  5, bat_lim =  32601, error =  1, sm_append = 0.68032, lets_finer =0.68032, max_a_sm_err = 0.7156053, low_pass = 100, finer = 1\n",
      "Step   159, estim_aver_bat_size = 5.33, b =  6, bat_lim =  32601, error =  0, sm_append = 0.67351, lets_finer =0.67351, max_a_sm_err = 0.7156053, low_pass = 100, finer = 1\n",
      "Step   160, estim_aver_bat_size = 5.33, b =  6, bat_lim =  32601, error =  0, sm_append = 0.66678, lets_finer =0.66678, max_a_sm_err = 0.7084493, low_pass = 100, finer = 1\n",
      "Step   161, estim_aver_bat_size = 5.35, b =  7, bat_lim =  32501, error = -1, sm_append = 0.65011, lets_finer =0.65011, max_a_sm_err = 0.6944511, low_pass = 100, finer = 1\n",
      "Step   162, estim_aver_bat_size = 5.34, b =  4, bat_lim =  32701, error =  2, sm_append = 0.66361, lets_finer =0.66361, max_a_sm_err = 0.6944511, low_pass = 100, finer = 1\n",
      "Step   163, estim_aver_bat_size = 5.34, b =  6, bat_lim =  32701, error =  0, sm_append = 0.65697, lets_finer =0.65697, max_a_sm_err = 0.6875066, low_pass = 100, finer = 1\n",
      "Step   164, estim_aver_bat_size = 5.35, b =  6, bat_lim =  32701, error =  0, sm_append = 0.65040, lets_finer =0.65040, max_a_sm_err = 0.6806316, low_pass = 100, finer = 1\n",
      "Step   165, estim_aver_bat_size = 5.36, b =  6, bat_lim =  32701, error =  0, sm_append = 0.64390, lets_finer =0.64390, max_a_sm_err = 0.6803161, low_pass = 100, finer = 1\n",
      "Step   166, estim_aver_bat_size = 5.37, b =  7, bat_lim =  32601, error = -1, sm_append = 0.62746, lets_finer =0.62746, max_a_sm_err = 0.6803161, low_pass = 100, finer = 1\n",
      "Step   167, estim_aver_bat_size = 5.39, b =  7, bat_lim =  32501, error = -1, sm_append = 0.61119, lets_finer =0.61119, max_a_sm_err = 0.6803161, low_pass = 100, finer = 1\n",
      "Step   168, estim_aver_bat_size = 5.40, b =  7, bat_lim =  32401, error = -1, sm_append = 0.59507, lets_finer =0.59507, max_a_sm_err = 0.6735130, low_pass = 100, finer = 1\n",
      "Step   169, estim_aver_bat_size = 5.41, b =  6, bat_lim =  32401, error =  0, sm_append = 0.58912, lets_finer =0.58912, max_a_sm_err = 0.6667778, low_pass = 100, finer = 1\n",
      "Step   170, estim_aver_bat_size = 5.41, b =  5, bat_lim =  32501, error =  1, sm_append = 0.59323, lets_finer =0.59323, max_a_sm_err = 0.6636090, low_pass = 100, finer = 1\n",
      "Step   171, estim_aver_bat_size = 5.41, b =  6, bat_lim =  32501, error =  0, sm_append = 0.58730, lets_finer =0.58730, max_a_sm_err = 0.6636090, low_pass = 100, finer = 1\n",
      "Step   172, estim_aver_bat_size = 5.43, b =  7, bat_lim =  32401, error = -1, sm_append = 0.57143, lets_finer =0.57143, max_a_sm_err = 0.6569729, low_pass = 100, finer = 1\n",
      "Step   173, estim_aver_bat_size = 5.43, b =  6, bat_lim =  32401, error =  0, sm_append = 0.56571, lets_finer =0.56571, max_a_sm_err = 0.6504031, low_pass = 100, finer = 1\n",
      "Step   174, estim_aver_bat_size = 5.45, b =  7, bat_lim =  32301, error = -1, sm_append = 0.55005, lets_finer =0.55005, max_a_sm_err = 0.6438991, low_pass = 100, finer = 1\n",
      "Step   175, estim_aver_bat_size = 5.46, b =  6, bat_lim =  32301, error =  0, sm_append = 0.54455, lets_finer =0.54455, max_a_sm_err = 0.6274601, low_pass = 100, finer = 1\n",
      "Step   176, estim_aver_bat_size = 5.47, b =  7, bat_lim =  32201, error = -1, sm_append = 0.52911, lets_finer =0.52911, max_a_sm_err = 0.6111855, low_pass = 100, finer = 1\n",
      "Step   177, estim_aver_bat_size = 5.48, b =  6, bat_lim =  32201, error =  0, sm_append = 0.52382, lets_finer =0.52382, max_a_sm_err = 0.5950737, low_pass = 100, finer = 1\n",
      "Step   178, estim_aver_bat_size = 5.49, b =  7, bat_lim =  32101, error = -1, sm_append = 0.50858, lets_finer =0.50858, max_a_sm_err = 0.5932317, low_pass = 100, finer = 1\n",
      "Step   179, estim_aver_bat_size = 5.49, b =  5, bat_lim =  32201, error =  1, sm_append = 0.51349, lets_finer =0.51349, max_a_sm_err = 0.5932317, low_pass = 100, finer = 1\n",
      "Step   180, estim_aver_bat_size = 5.48, b =  5, bat_lim =  32301, error =  1, sm_append = 0.51836, lets_finer =0.51836, max_a_sm_err = 0.5872994, low_pass = 100, finer = 1\n",
      "Step   181, estim_aver_bat_size = 5.50, b =  7, bat_lim =  32201, error = -1, sm_append = 0.50318, lets_finer =0.50318, max_a_sm_err = 0.5714264, low_pass = 100, finer = 1\n",
      "Step   182, estim_aver_bat_size = 5.49, b =  5, bat_lim =  32301, error =  1, sm_append = 0.50814, lets_finer =0.50814, max_a_sm_err = 0.5657121, low_pass = 100, finer = 1\n",
      "Step   183, estim_aver_bat_size = 5.50, b =  6, bat_lim =  32301, error =  0, sm_append = 0.50306, lets_finer =0.50306, max_a_sm_err = 0.5500550, low_pass = 100, finer = 1\n",
      "Step   184, estim_aver_bat_size = 5.49, b =  5, bat_lim =  32401, error =  1, sm_append = 0.50803, lets_finer =0.50803, max_a_sm_err = 0.5445544, low_pass = 100, finer = 1\n",
      "Step   185, estim_aver_bat_size = 5.50, b =  6, bat_lim =  32401, error =  0, sm_append = 0.50295, lets_finer =0.50295, max_a_sm_err = 0.5291089, low_pass = 100, finer = 1\n",
      "Step   186, estim_aver_bat_size = 5.50, b =  6, bat_lim =  32401, error =  0, sm_append = 0.49792, lets_finer =0.49792, max_a_sm_err = 0.5238178, low_pass = 100, finer = 1\n",
      "Step   187, estim_aver_bat_size = 5.50, b =  5, bat_lim =  32501, error =  1, sm_append = 0.50294, lets_finer =0.50294, max_a_sm_err = 0.5183589, low_pass = 100, finer = 1\n",
      "Step   188, estim_aver_bat_size = 5.50, b =  6, bat_lim =  32501, error =  0, sm_append = 0.49791, lets_finer =0.49791, max_a_sm_err = 0.5183589, low_pass = 100, finer = 1\n",
      "Step   189, estim_aver_bat_size = 5.52, b =  7, bat_lim =  32401, error = -1, sm_append = 0.48293, lets_finer =0.48293, max_a_sm_err = 0.5183589, low_pass = 100, finer = 1\n",
      "Step   190, estim_aver_bat_size = 5.52, b =  6, bat_lim =  32401, error =  0, sm_append = 0.47810, lets_finer =0.47810, max_a_sm_err = 0.5081436, low_pass = 100, finer = 1\n",
      "Step   191, estim_aver_bat_size = 5.52, b =  5, bat_lim =  32501, error =  1, sm_append = 0.48332, lets_finer =0.48332, max_a_sm_err = 0.5081436, low_pass = 100, finer = 1\n",
      "Step   192, estim_aver_bat_size = 5.53, b =  7, bat_lim =  32401, error = -1, sm_append = 0.46849, lets_finer =0.46849, max_a_sm_err = 0.5080315, low_pass = 100, finer = 1\n",
      "Step   193, estim_aver_bat_size = 5.54, b =  6, bat_lim =  32401, error =  0, sm_append = 0.46381, lets_finer =0.46381, max_a_sm_err = 0.5080315, low_pass = 100, finer = 1\n",
      "Step   194, estim_aver_bat_size = 5.54, b =  6, bat_lim =  32401, error =  0, sm_append = 0.45917, lets_finer =0.45917, max_a_sm_err = 0.5029512, low_pass = 100, finer = 1\n",
      "Step   195, estim_aver_bat_size = 5.55, b =  6, bat_lim =  32401, error =  0, sm_append = 0.45458, lets_finer =0.45458, max_a_sm_err = 0.5029425, low_pass = 100, finer = 1\n",
      "Step   196, estim_aver_bat_size = 5.55, b =  6, bat_lim =  32401, error =  0, sm_append = 0.45003, lets_finer =0.45003, max_a_sm_err = 0.5029425, low_pass = 100, finer = 1\n",
      "Step   197, estim_aver_bat_size = 5.55, b =  6, bat_lim =  32401, error =  0, sm_append = 0.44553, lets_finer =0.44553, max_a_sm_err = 0.4979130, low_pass = 100, finer = 1\n",
      "Step   198, estim_aver_bat_size = 5.56, b =  6, bat_lim =  32401, error =  0, sm_append = 0.44107, lets_finer =0.44107, max_a_sm_err = 0.4833235, low_pass = 100, finer = 1\n",
      "Step   199, estim_aver_bat_size = 5.56, b =  6, bat_lim =  32401, error =  0, sm_append = 0.43666, lets_finer =0.43666, max_a_sm_err = 0.4833235, low_pass = 100, finer = 1\n",
      "Step   200, estim_aver_bat_size = 5.57, b =  6, bat_lim =  32401, error =  0, sm_append = 0.43230, lets_finer =0.43230, max_a_sm_err = 0.4833235, low_pass = 100, finer = 1\n",
      "Step   201, estim_aver_bat_size = 5.55, b =  4, bat_lim =  32601, error =  2, sm_append = 0.44797, lets_finer =0.44797, max_a_sm_err = 0.4684903, low_pass = 100, finer = 1\n",
      "Step   202, estim_aver_bat_size = 5.56, b =  6, bat_lim =  32601, error =  0, sm_append = 0.44349, lets_finer =0.44349, max_a_sm_err = 0.4638054, low_pass = 100, finer = 1\n",
      "Step   203, estim_aver_bat_size = 5.57, b =  7, bat_lim =  32501, error = -1, sm_append = 0.42906, lets_finer =0.42906, max_a_sm_err = 0.4591673, low_pass = 100, finer = 1\n",
      "Step   204, estim_aver_bat_size = 5.59, b =  7, bat_lim =  32401, error = -1, sm_append = 0.41477, lets_finer =0.41477, max_a_sm_err = 0.4545757, low_pass = 100, finer = 1\n",
      "Step   205, estim_aver_bat_size = 5.58, b =  5, bat_lim =  32501, error =  1, sm_append = 0.42062, lets_finer =0.42062, max_a_sm_err = 0.4500299, low_pass = 100, finer = 1\n",
      "Step   206, estim_aver_bat_size = 5.57, b =  5, bat_lim =  32601, error =  1, sm_append = 0.42641, lets_finer =0.42641, max_a_sm_err = 0.4479740, low_pass = 100, finer = 1\n",
      "Step   207, estim_aver_bat_size = 5.58, b =  6, bat_lim =  32601, error =  0, sm_append = 0.42215, lets_finer =0.42215, max_a_sm_err = 0.4479740, low_pass = 100, finer = 1\n",
      "Step   208, estim_aver_bat_size = 5.59, b =  7, bat_lim =  32501, error = -1, sm_append = 0.40793, lets_finer =0.40793, max_a_sm_err = 0.4479740, low_pass = 100, finer = 1\n",
      "Step   209, estim_aver_bat_size = 5.59, b =  5, bat_lim =  32601, error =  1, sm_append = 0.41385, lets_finer =0.41385, max_a_sm_err = 0.4479740, low_pass = 100, finer = 1\n",
      "Step   210, estim_aver_bat_size = 5.60, b =  7, bat_lim =  32501, error = -1, sm_append = 0.39971, lets_finer =0.39971, max_a_sm_err = 0.4479740, low_pass = 100, finer = 1\n",
      "Step   211, estim_aver_bat_size = 5.60, b =  6, bat_lim =  32501, error =  0, sm_append = 0.39571, lets_finer =0.39571, max_a_sm_err = 0.4434942, low_pass = 100, finer = 1\n",
      "Step   212, estim_aver_bat_size = 5.61, b =  6, bat_lim =  32501, error =  0, sm_append = 0.39176, lets_finer =0.39176, max_a_sm_err = 0.4290593, low_pass = 100, finer = 1\n",
      "Step   213, estim_aver_bat_size = 5.60, b =  5, bat_lim =  32601, error =  1, sm_append = 0.39784, lets_finer =0.39784, max_a_sm_err = 0.4264148, low_pass = 100, finer = 1\n",
      "Step   214, estim_aver_bat_size = 5.59, b =  4, bat_lim =  32801, error =  2, sm_append = 0.41386, lets_finer =0.41386, max_a_sm_err = 0.4264148, low_pass = 100, finer = 1\n",
      "Step   215, estim_aver_bat_size = 5.60, b =  7, bat_lim =  32701, error = -1, sm_append = 0.39972, lets_finer =0.39972, max_a_sm_err = 0.4264148, low_pass = 100, finer = 1\n",
      "Step   216, estim_aver_bat_size = 5.62, b =  8, bat_lim =  32501, error = -2, sm_append = 0.37573, lets_finer =0.37573, max_a_sm_err = 0.4221506, low_pass = 100, finer = 1\n",
      "Step   217, estim_aver_bat_size = 5.62, b =  5, bat_lim =  32601, error =  1, sm_append = 0.38197, lets_finer =0.38197, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   218, estim_aver_bat_size = 5.61, b =  5, bat_lim =  32701, error =  1, sm_append = 0.38815, lets_finer =0.38815, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   219, estim_aver_bat_size = 5.61, b =  5, bat_lim =  32801, error =  1, sm_append = 0.39427, lets_finer =0.39427, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   220, estim_aver_bat_size = 5.60, b =  5, bat_lim =  32901, error =  1, sm_append = 0.40032, lets_finer =0.40032, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   221, estim_aver_bat_size = 5.60, b =  6, bat_lim =  32901, error =  0, sm_append = 0.39632, lets_finer =0.39632, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   222, estim_aver_bat_size = 5.62, b =  7, bat_lim =  32801, error = -1, sm_append = 0.38236, lets_finer =0.38236, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   223, estim_aver_bat_size = 5.62, b =  6, bat_lim =  32801, error =  0, sm_append = 0.37853, lets_finer =0.37853, max_a_sm_err = 0.4138611, low_pass = 100, finer = 1\n",
      "Step   224, estim_aver_bat_size = 5.64, b =  7, bat_lim =  32701, error = -1, sm_append = 0.36475, lets_finer =0.36475, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   225, estim_aver_bat_size = 5.64, b =  6, bat_lim =  32701, error =  0, sm_append = 0.36110, lets_finer =0.36110, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   226, estim_aver_bat_size = 5.63, b =  5, bat_lim =  32801, error =  1, sm_append = 0.36749, lets_finer =0.36749, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   227, estim_aver_bat_size = 5.64, b =  6, bat_lim =  32801, error =  0, sm_append = 0.36382, lets_finer =0.36382, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   228, estim_aver_bat_size = 5.63, b =  5, bat_lim =  32901, error =  1, sm_append = 0.37018, lets_finer =0.37018, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   229, estim_aver_bat_size = 5.63, b =  6, bat_lim =  32901, error =  0, sm_append = 0.36648, lets_finer =0.36648, max_a_sm_err = 0.4003242, low_pass = 100, finer = 1\n",
      "Step   230, estim_aver_bat_size = 5.64, b =  6, bat_lim =  32901, error =  0, sm_append = 0.36281, lets_finer =0.36281, max_a_sm_err = 0.3963210, low_pass = 100, finer = 1\n",
      "Step   231, estim_aver_bat_size = 5.64, b =  6, bat_lim =  32901, error =  0, sm_append = 0.35918, lets_finer =0.35918, max_a_sm_err = 0.3823577, low_pass = 100, finer = 1\n",
      "Step   232, estim_aver_bat_size = 5.64, b =  6, bat_lim =  32901, error =  0, sm_append = 0.35559, lets_finer =0.35559, max_a_sm_err = 0.3785342, low_pass = 100, finer = 1\n",
      "Step   233, estim_aver_bat_size = 5.66, b =  7, bat_lim =  32801, error = -1, sm_append = 0.34203, lets_finer =0.34203, max_a_sm_err = 0.3701773, low_pass = 100, finer = 1\n",
      "Step   234, estim_aver_bat_size = 5.67, b =  7, bat_lim =  32701, error = -1, sm_append = 0.32861, lets_finer =0.32861, max_a_sm_err = 0.3701773, low_pass = 100, finer = 1\n",
      "Step   235, estim_aver_bat_size = 5.66, b =  5, bat_lim =  32801, error =  1, sm_append = 0.33533, lets_finer =0.33533, max_a_sm_err = 0.3701773, low_pass = 100, finer = 1\n",
      "Step   236, estim_aver_bat_size = 5.69, b =  8, bat_lim =  32601, error = -2, sm_append = 0.31198, lets_finer =0.31198, max_a_sm_err = 0.3701773, low_pass = 100, finer = 1\n",
      "Step   237, estim_aver_bat_size = 5.70, b =  7, bat_lim =  32501, error = -1, sm_append = 0.29886, lets_finer =0.29886, max_a_sm_err = 0.3701773, low_pass = 100, finer = 1\n",
      "Step   238, estim_aver_bat_size = 5.69, b =  5, bat_lim =  32601, error =  1, sm_append = 0.30587, lets_finer =0.30587, max_a_sm_err = 0.3664755, low_pass = 100, finer = 1\n",
      "Step   239, estim_aver_bat_size = 5.71, b =  7, bat_lim =  32501, error = -1, sm_append = 0.29281, lets_finer =0.29281, max_a_sm_err = 0.3628107, low_pass = 100, finer = 1\n",
      "Step   240, estim_aver_bat_size = 5.72, b =  7, bat_lim =  32401, error = -1, sm_append = 0.27988, lets_finer =0.27988, max_a_sm_err = 0.3591826, low_pass = 100, finer = 1\n",
      "Step   241, estim_aver_bat_size = 5.72, b =  6, bat_lim =  32401, error =  0, sm_append = 0.27708, lets_finer =0.27708, max_a_sm_err = 0.3555908, low_pass = 100, finer = 1\n",
      "Step   242, estim_aver_bat_size = 5.72, b =  5, bat_lim =  32501, error =  1, sm_append = 0.28431, lets_finer =0.28431, max_a_sm_err = 0.3420349, low_pass = 100, finer = 1\n",
      "Step   243, estim_aver_bat_size = 5.73, b =  7, bat_lim =  32401, error = -1, sm_append = 0.27147, lets_finer =0.27147, max_a_sm_err = 0.3353284, low_pass = 100, finer = 1\n",
      "Step   244, estim_aver_bat_size = 5.73, b =  6, bat_lim =  32401, error =  0, sm_append = 0.26875, lets_finer =0.26875, max_a_sm_err = 0.3353284, low_pass = 100, finer = 1\n",
      "Step   245, estim_aver_bat_size = 5.73, b =  6, bat_lim =  32401, error =  0, sm_append = 0.26607, lets_finer =0.26607, max_a_sm_err = 0.3119751, low_pass = 100, finer = 1\n",
      "Step   246, estim_aver_bat_size = 5.74, b =  6, bat_lim =  32401, error =  0, sm_append = 0.26340, lets_finer =0.26340, max_a_sm_err = 0.3058668, low_pass = 100, finer = 1\n",
      "Step   247, estim_aver_bat_size = 5.74, b =  6, bat_lim =  32401, error =  0, sm_append = 0.26077, lets_finer =0.26077, max_a_sm_err = 0.3058668, low_pass = 100, finer = 1\n",
      "Step   248, estim_aver_bat_size = 5.74, b =  6, bat_lim =  32401, error =  0, sm_append = 0.25816, lets_finer =0.25816, max_a_sm_err = 0.2928081, low_pass = 100, finer = 1\n",
      "Step   249, estim_aver_bat_size = 5.73, b =  5, bat_lim =  32501, error =  1, sm_append = 0.26558, lets_finer =0.26558, max_a_sm_err = 0.2843105, low_pass = 100, finer = 1\n",
      "Step   250, estim_aver_bat_size = 5.74, b =  6, bat_lim =  32501, error =  0, sm_append = 0.26293, lets_finer =0.26293, max_a_sm_err = 0.2843105, low_pass = 100, finer = 1\n",
      "Step   251, estim_aver_bat_size = 5.75, b =  7, bat_lim =  32401, error = -1, sm_append = 0.25030, lets_finer =0.25030, max_a_sm_err = 0.2843105, low_pass = 100, finer = 1\n",
      "Step   252, estim_aver_bat_size = 5.74, b =  5, bat_lim =  32501, error =  1, sm_append = 0.25779, lets_finer =0.25779, max_a_sm_err = 0.2714673, low_pass = 100, finer = 1\n",
      "Step   253, estim_aver_bat_size = 5.75, b =  7, bat_lim =  32401, error = -1, sm_append = 0.24522, lets_finer =0.24522, max_a_sm_err = 0.2687527, low_pass = 100, finer = 1\n",
      "Step   254, estim_aver_bat_size = 5.77, b =  7, bat_lim =  32301, error = -1, sm_append = 0.23276, lets_finer =0.23276, max_a_sm_err = 0.2660651, low_pass = 100, finer = 1\n",
      "Step   255, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32301, error =  0, sm_append = 0.23044, lets_finer =0.23044, max_a_sm_err = 0.2655811, low_pass = 100, finer = 1\n",
      "Step   256, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32301, error =  0, sm_append = 0.22813, lets_finer =0.22813, max_a_sm_err = 0.2655811, low_pass = 100, finer = 1\n",
      "Step   257, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32301, error =  0, sm_append = 0.22585, lets_finer =0.22585, max_a_sm_err = 0.2655811, low_pass = 100, finer = 1\n",
      "Step   258, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32301, error =  0, sm_append = 0.22359, lets_finer =0.22359, max_a_sm_err = 0.2655811, low_pass = 100, finer = 1\n",
      "Step   259, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32301, error =  0, sm_append = 0.22136, lets_finer =0.22136, max_a_sm_err = 0.2629253, low_pass = 100, finer = 1\n",
      "Step   260, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32301, error =  0, sm_append = 0.21914, lets_finer =0.21914, max_a_sm_err = 0.2577931, low_pass = 100, finer = 1\n",
      "Step   261, estim_aver_bat_size = 5.77, b =  5, bat_lim =  32401, error =  1, sm_append = 0.22695, lets_finer =0.22695, max_a_sm_err = 0.2577931, low_pass = 100, finer = 1\n",
      "Step   262, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32401, error =  0, sm_append = 0.22468, lets_finer =0.22468, max_a_sm_err = 0.2452152, low_pass = 100, finer = 1\n",
      "Step   263, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32401, error =  0, sm_append = 0.22243, lets_finer =0.22243, max_a_sm_err = 0.2327630, low_pass = 100, finer = 1\n",
      "Step   264, estim_aver_bat_size = 5.77, b =  5, bat_lim =  32501, error =  1, sm_append = 0.23021, lets_finer =0.23021, max_a_sm_err = 0.2304354, low_pass = 100, finer = 1\n",
      "Step   265, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32501, error =  0, sm_append = 0.22791, lets_finer =0.22791, max_a_sm_err = 0.2302097, low_pass = 100, finer = 1\n",
      "Step   266, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32501, error =  0, sm_append = 0.22563, lets_finer =0.22563, max_a_sm_err = 0.2302097, low_pass = 100, finer = 1\n",
      "Step   267, estim_aver_bat_size = 5.77, b =  5, bat_lim =  32601, error =  1, sm_append = 0.23337, lets_finer =0.23337, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   268, estim_aver_bat_size = 5.78, b =  7, bat_lim =  32501, error = -1, sm_append = 0.22104, lets_finer =0.22104, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   269, estim_aver_bat_size = 5.77, b =  5, bat_lim =  32601, error =  1, sm_append = 0.22883, lets_finer =0.22883, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   270, estim_aver_bat_size = 5.77, b =  6, bat_lim =  32601, error =  0, sm_append = 0.22654, lets_finer =0.22654, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   271, estim_aver_bat_size = 5.80, b =  8, bat_lim =  32401, error = -2, sm_append = 0.20427, lets_finer =0.20427, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   272, estim_aver_bat_size = 5.79, b =  5, bat_lim =  32501, error =  1, sm_append = 0.21223, lets_finer =0.21223, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   273, estim_aver_bat_size = 5.78, b =  5, bat_lim =  32601, error =  1, sm_append = 0.22011, lets_finer =0.22011, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   274, estim_aver_bat_size = 5.77, b =  5, bat_lim =  32701, error =  1, sm_append = 0.22791, lets_finer =0.22791, max_a_sm_err = 0.2333722, low_pass = 100, finer = 1\n",
      "Step   275, estim_aver_bat_size = 5.76, b =  5, bat_lim =  32801, error =  1, sm_append = 0.23563, lets_finer =0.23563, max_a_sm_err = 0.2356292, low_pass = 100, finer = 1\n",
      "Step   276, estim_aver_bat_size = 5.76, b =  5, bat_lim =  32901, error =  1, sm_append = 0.24327, lets_finer =0.24327, max_a_sm_err = 0.2432729, low_pass = 100, finer = 1\n",
      "Step   277, estim_aver_bat_size = 5.75, b =  5, bat_lim =  33001, error =  1, sm_append = 0.25084, lets_finer =0.25084, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   278, estim_aver_bat_size = 5.75, b =  6, bat_lim =  33001, error =  0, sm_append = 0.24833, lets_finer =0.24833, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   279, estim_aver_bat_size = 5.76, b =  7, bat_lim =  32901, error = -1, sm_append = 0.23585, lets_finer =0.23585, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   280, estim_aver_bat_size = 5.78, b =  7, bat_lim =  32801, error = -1, sm_append = 0.22349, lets_finer =0.22349, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   281, estim_aver_bat_size = 5.78, b =  6, bat_lim =  32801, error =  0, sm_append = 0.22126, lets_finer =0.22126, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   282, estim_aver_bat_size = 5.79, b =  7, bat_lim =  32701, error = -1, sm_append = 0.20904, lets_finer =0.20904, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   283, estim_aver_bat_size = 5.79, b =  6, bat_lim =  32701, error =  0, sm_append = 0.20695, lets_finer =0.20695, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   284, estim_aver_bat_size = 5.81, b =  7, bat_lim =  32601, error = -1, sm_append = 0.19488, lets_finer =0.19488, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   285, estim_aver_bat_size = 5.82, b =  7, bat_lim =  32501, error = -1, sm_append = 0.18293, lets_finer =0.18293, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   286, estim_aver_bat_size = 5.83, b =  7, bat_lim =  32401, error = -1, sm_append = 0.17110, lets_finer =0.17110, max_a_sm_err = 0.2508402, low_pass = 100, finer = 1\n",
      "Step   287, estim_aver_bat_size = 5.82, b =  5, bat_lim =  32501, error =  1, sm_append = 0.17939, lets_finer =0.17939, max_a_sm_err = 0.2483318, low_pass = 100, finer = 1\n",
      "Step   288, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32501, error =  0, sm_append = 0.17760, lets_finer =0.17760, max_a_sm_err = 0.2358485, low_pass = 100, finer = 1\n",
      "Step   289, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32501, error =  0, sm_append = 0.17582, lets_finer =0.17582, max_a_sm_err = 0.2234900, low_pass = 100, finer = 1\n",
      "Step   290, estim_aver_bat_size = 5.83, b =  6, bat_lim =  32501, error =  0, sm_append = 0.17407, lets_finer =0.17407, max_a_sm_err = 0.2212551, low_pass = 100, finer = 1\n",
      "Step   291, estim_aver_bat_size = 5.83, b =  6, bat_lim =  32501, error =  0, sm_append = 0.17232, lets_finer =0.17232, max_a_sm_err = 0.2090425, low_pass = 100, finer = 1\n",
      "Step   292, estim_aver_bat_size = 5.82, b =  5, bat_lim =  32601, error =  1, sm_append = 0.18060, lets_finer =0.18060, max_a_sm_err = 0.2069521, low_pass = 100, finer = 1\n",
      "Step   293, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32601, error =  0, sm_append = 0.17880, lets_finer =0.17880, max_a_sm_err = 0.1948826, low_pass = 100, finer = 1\n",
      "Step   294, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32601, error =  0, sm_append = 0.17701, lets_finer =0.17701, max_a_sm_err = 0.1829338, low_pass = 100, finer = 1\n",
      "Step   295, estim_aver_bat_size = 5.81, b =  5, bat_lim =  32701, error =  1, sm_append = 0.18524, lets_finer =0.18524, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   296, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32701, error =  0, sm_append = 0.18338, lets_finer =0.18338, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   297, estim_aver_bat_size = 5.84, b =  8, bat_lim =  32501, error = -2, sm_append = 0.16155, lets_finer =0.16155, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   298, estim_aver_bat_size = 5.84, b =  6, bat_lim =  32501, error =  0, sm_append = 0.15994, lets_finer =0.15994, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   299, estim_aver_bat_size = 5.85, b =  7, bat_lim =  32401, error = -1, sm_append = 0.14834, lets_finer =0.14834, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   300, estim_aver_bat_size = 5.84, b =  5, bat_lim =  32501, error =  1, sm_append = 0.15685, lets_finer =0.15685, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   301, estim_aver_bat_size = 5.84, b =  6, bat_lim =  32501, error =  0, sm_append = 0.15528, lets_finer =0.15528, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   302, estim_aver_bat_size = 5.85, b =  6, bat_lim =  32501, error =  0, sm_append = 0.15373, lets_finer =0.15373, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   303, estim_aver_bat_size = 5.84, b =  5, bat_lim =  32601, error =  1, sm_append = 0.16219, lets_finer =0.16219, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   304, estim_aver_bat_size = 5.84, b =  6, bat_lim =  32601, error =  0, sm_append = 0.16057, lets_finer =0.16057, max_a_sm_err = 0.1852373, low_pass = 100, finer = 1\n",
      "Step   305, estim_aver_bat_size = 5.83, b =  5, bat_lim =  32701, error =  1, sm_append = 0.16897, lets_finer =0.16897, max_a_sm_err = 0.1833849, low_pass = 100, finer = 1\n",
      "Step   306, estim_aver_bat_size = 5.83, b =  6, bat_lim =  32701, error =  0, sm_append = 0.16728, lets_finer =0.16728, max_a_sm_err = 0.1689665, low_pass = 100, finer = 1\n",
      "Step   307, estim_aver_bat_size = 5.82, b =  5, bat_lim =  32801, error =  1, sm_append = 0.17560, lets_finer =0.17560, max_a_sm_err = 0.1756040, low_pass = 100, finer = 1\n",
      "Step   308, estim_aver_bat_size = 5.82, b =  5, bat_lim =  32901, error =  1, sm_append = 0.18385, lets_finer =0.18385, max_a_sm_err = 0.1838480, low_pass = 100, finer = 1\n",
      "Step   309, estim_aver_bat_size = 5.82, b =  6, bat_lim =  32901, error =  0, sm_append = 0.18201, lets_finer =0.18201, max_a_sm_err = 0.1838480, low_pass = 100, finer = 1\n",
      "Step   310, estim_aver_bat_size = 5.81, b =  5, bat_lim =  33001, error =  1, sm_append = 0.19019, lets_finer =0.19019, max_a_sm_err = 0.1901894, low_pass = 100, finer = 1\n",
      "Step   311, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33001, error =  0, sm_append = 0.18829, lets_finer =0.18829, max_a_sm_err = 0.1901894, low_pass = 100, finer = 1\n",
      "Step   312, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33001, error =  0, sm_append = 0.18640, lets_finer =0.18640, max_a_sm_err = 0.1901894, low_pass = 100, finer = 1\n",
      "Step   313, estim_aver_bat_size = 5.80, b =  4, bat_lim =  33201, error =  2, sm_append = 0.20454, lets_finer =0.20454, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   314, estim_aver_bat_size = 5.80, b =  6, bat_lim =  33201, error =  0, sm_append = 0.20250, lets_finer =0.20250, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   315, estim_aver_bat_size = 5.81, b =  7, bat_lim =  33101, error = -1, sm_append = 0.19047, lets_finer =0.19047, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   316, estim_aver_bat_size = 5.80, b =  5, bat_lim =  33201, error =  1, sm_append = 0.19857, lets_finer =0.19857, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   317, estim_aver_bat_size = 5.81, b =  7, bat_lim =  33101, error = -1, sm_append = 0.18658, lets_finer =0.18658, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   318, estim_aver_bat_size = 5.81, b =  5, bat_lim =  33201, error =  1, sm_append = 0.19471, lets_finer =0.19471, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   319, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33201, error =  0, sm_append = 0.19277, lets_finer =0.19277, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   320, estim_aver_bat_size = 5.82, b =  7, bat_lim =  33101, error = -1, sm_append = 0.18084, lets_finer =0.18084, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   321, estim_aver_bat_size = 5.83, b =  7, bat_lim =  33001, error = -1, sm_append = 0.16903, lets_finer =0.16903, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   322, estim_aver_bat_size = 5.83, b =  6, bat_lim =  33001, error =  0, sm_append = 0.16734, lets_finer =0.16734, max_a_sm_err = 0.2045406, low_pass = 100, finer = 1\n",
      "Step   323, estim_aver_bat_size = 5.83, b =  6, bat_lim =  33001, error =  0, sm_append = 0.16567, lets_finer =0.16567, max_a_sm_err = 0.2024952, low_pass = 100, finer = 1\n",
      "Step   324, estim_aver_bat_size = 5.84, b =  6, bat_lim =  33001, error =  0, sm_append = 0.16401, lets_finer =0.16401, max_a_sm_err = 0.1985655, low_pass = 100, finer = 1\n",
      "Step   325, estim_aver_bat_size = 5.84, b =  6, bat_lim =  33001, error =  0, sm_append = 0.16237, lets_finer =0.16237, max_a_sm_err = 0.1985655, low_pass = 100, finer = 1\n",
      "Step   326, estim_aver_bat_size = 5.85, b =  7, bat_lim =  32901, error = -1, sm_append = 0.15075, lets_finer =0.15075, max_a_sm_err = 0.1947141, low_pass = 100, finer = 1\n",
      "Step   327, estim_aver_bat_size = 5.85, b =  6, bat_lim =  32901, error =  0, sm_append = 0.14924, lets_finer =0.14924, max_a_sm_err = 0.1947141, low_pass = 100, finer = 1\n",
      "Step   328, estim_aver_bat_size = 5.86, b =  7, bat_lim =  32801, error = -1, sm_append = 0.13775, lets_finer =0.13775, max_a_sm_err = 0.1927670, low_pass = 100, finer = 1\n",
      "Step   329, estim_aver_bat_size = 5.84, b =  4, bat_lim =  33001, error =  2, sm_append = 0.15637, lets_finer =0.15637, max_a_sm_err = 0.1808393, low_pass = 100, finer = 1\n",
      "Step   330, estim_aver_bat_size = 5.84, b =  5, bat_lim =  33101, error =  1, sm_append = 0.16481, lets_finer =0.16481, max_a_sm_err = 0.1690309, low_pass = 100, finer = 1\n",
      "Step   331, estim_aver_bat_size = 5.85, b =  7, bat_lim =  33001, error = -1, sm_append = 0.15316, lets_finer =0.15316, max_a_sm_err = 0.1673406, low_pass = 100, finer = 1\n",
      "Step   332, estim_aver_bat_size = 5.84, b =  5, bat_lim =  33101, error =  1, sm_append = 0.16163, lets_finer =0.16163, max_a_sm_err = 0.1656672, low_pass = 100, finer = 1\n",
      "Step   333, estim_aver_bat_size = 5.85, b =  7, bat_lim =  33001, error = -1, sm_append = 0.15001, lets_finer =0.15001, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   334, estim_aver_bat_size = 5.86, b =  7, bat_lim =  32901, error = -1, sm_append = 0.13851, lets_finer =0.13851, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   335, estim_aver_bat_size = 5.87, b =  7, bat_lim =  32801, error = -1, sm_append = 0.12712, lets_finer =0.12712, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   336, estim_aver_bat_size = 5.85, b =  4, bat_lim =  33001, error =  2, sm_append = 0.14585, lets_finer =0.14585, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   337, estim_aver_bat_size = 5.85, b =  5, bat_lim =  33101, error =  1, sm_append = 0.15439, lets_finer =0.15439, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   338, estim_aver_bat_size = 5.85, b =  6, bat_lim =  33101, error =  0, sm_append = 0.15285, lets_finer =0.15285, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   339, estim_aver_bat_size = 5.84, b =  5, bat_lim =  33201, error =  1, sm_append = 0.16132, lets_finer =0.16132, max_a_sm_err = 0.1648057, low_pass = 100, finer = 1\n",
      "Step   340, estim_aver_bat_size = 5.83, b =  5, bat_lim =  33301, error =  1, sm_append = 0.16971, lets_finer =0.16971, max_a_sm_err = 0.1697091, low_pass = 100, finer = 1\n",
      "Step   341, estim_aver_bat_size = 5.82, b =  5, bat_lim =  33401, error =  1, sm_append = 0.17801, lets_finer =0.17801, max_a_sm_err = 0.1780120, low_pass = 100, finer = 1\n",
      "Step   342, estim_aver_bat_size = 5.83, b =  7, bat_lim =  33301, error = -1, sm_append = 0.16623, lets_finer =0.16623, max_a_sm_err = 0.1780120, low_pass = 100, finer = 1\n",
      "Step   343, estim_aver_bat_size = 5.83, b =  5, bat_lim =  33401, error =  1, sm_append = 0.17457, lets_finer =0.17457, max_a_sm_err = 0.1780120, low_pass = 100, finer = 1\n",
      "Step   344, estim_aver_bat_size = 5.82, b =  5, bat_lim =  33501, error =  1, sm_append = 0.18282, lets_finer =0.18282, max_a_sm_err = 0.1828239, low_pass = 100, finer = 1\n",
      "Step   345, estim_aver_bat_size = 5.81, b =  5, bat_lim =  33601, error =  1, sm_append = 0.19100, lets_finer =0.19100, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   346, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33601, error =  0, sm_append = 0.18909, lets_finer =0.18909, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   347, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33601, error =  0, sm_append = 0.18719, lets_finer =0.18719, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   348, estim_aver_bat_size = 5.81, b =  6, bat_lim =  33601, error =  0, sm_append = 0.18532, lets_finer =0.18532, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   349, estim_aver_bat_size = 5.83, b =  7, bat_lim =  33501, error = -1, sm_append = 0.17347, lets_finer =0.17347, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   350, estim_aver_bat_size = 5.82, b =  5, bat_lim =  33601, error =  1, sm_append = 0.18173, lets_finer =0.18173, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   351, estim_aver_bat_size = 5.82, b =  6, bat_lim =  33601, error =  0, sm_append = 0.17992, lets_finer =0.17992, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   352, estim_aver_bat_size = 5.81, b =  5, bat_lim =  33701, error =  1, sm_append = 0.18812, lets_finer =0.18812, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   353, estim_aver_bat_size = 5.82, b =  7, bat_lim =  33601, error = -1, sm_append = 0.17624, lets_finer =0.17624, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   354, estim_aver_bat_size = 5.84, b =  7, bat_lim =  33501, error = -1, sm_append = 0.16447, lets_finer =0.16447, max_a_sm_err = 0.1909957, low_pass = 100, finer = 1\n",
      "Step   355, estim_aver_bat_size = 5.85, b =  7, bat_lim =  33401, error = -1, sm_append = 0.15283, lets_finer =0.15283, max_a_sm_err = 0.1890857, low_pass = 100, finer = 1\n",
      "Step   356, estim_aver_bat_size = 5.86, b =  7, bat_lim =  33301, error = -1, sm_append = 0.14130, lets_finer =0.14130, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   357, estim_aver_bat_size = 5.87, b =  7, bat_lim =  33201, error = -1, sm_append = 0.12989, lets_finer =0.12989, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   358, estim_aver_bat_size = 5.88, b =  7, bat_lim =  33101, error = -1, sm_append = 0.11859, lets_finer =0.11859, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   359, estim_aver_bat_size = 5.86, b =  4, bat_lim =  33301, error =  2, sm_append = 0.13740, lets_finer =0.13740, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   360, estim_aver_bat_size = 5.86, b =  6, bat_lim =  33301, error =  0, sm_append = 0.13603, lets_finer =0.13603, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   361, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33301, error =  0, sm_append = 0.13467, lets_finer =0.13467, max_a_sm_err = 0.1881184, low_pass = 100, finer = 1\n",
      "Step   362, estim_aver_bat_size = 5.88, b =  7, bat_lim =  33201, error = -1, sm_append = 0.12332, lets_finer =0.12332, max_a_sm_err = 0.1762373, low_pass = 100, finer = 1\n",
      "Step   363, estim_aver_bat_size = 5.88, b =  6, bat_lim =  33201, error =  0, sm_append = 0.12209, lets_finer =0.12209, max_a_sm_err = 0.1644749, low_pass = 100, finer = 1\n",
      "Step   364, estim_aver_bat_size = 5.87, b =  5, bat_lim =  33301, error =  1, sm_append = 0.13087, lets_finer =0.13087, max_a_sm_err = 0.1528301, low_pass = 100, finer = 1\n",
      "Step   365, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33301, error =  0, sm_append = 0.12956, lets_finer =0.12956, max_a_sm_err = 0.1413018, low_pass = 100, finer = 1\n",
      "Step   366, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33301, error =  0, sm_append = 0.12826, lets_finer =0.12826, max_a_sm_err = 0.1374040, low_pass = 100, finer = 1\n",
      "Step   367, estim_aver_bat_size = 5.88, b =  7, bat_lim =  33201, error = -1, sm_append = 0.11698, lets_finer =0.11698, max_a_sm_err = 0.1374040, low_pass = 100, finer = 1\n",
      "Step   368, estim_aver_bat_size = 5.87, b =  5, bat_lim =  33301, error =  1, sm_append = 0.12581, lets_finer =0.12581, max_a_sm_err = 0.1374040, low_pass = 100, finer = 1\n",
      "Step   369, estim_aver_bat_size = 5.88, b =  6, bat_lim =  33301, error =  0, sm_append = 0.12455, lets_finer =0.12455, max_a_sm_err = 0.1360300, low_pass = 100, finer = 1\n",
      "Step   370, estim_aver_bat_size = 5.87, b =  5, bat_lim =  33401, error =  1, sm_append = 0.13331, lets_finer =0.13331, max_a_sm_err = 0.1346697, low_pass = 100, finer = 1\n",
      "Step   371, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33401, error =  0, sm_append = 0.13198, lets_finer =0.13198, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   372, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33401, error =  0, sm_append = 0.13066, lets_finer =0.13066, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   373, estim_aver_bat_size = 5.88, b =  7, bat_lim =  33301, error = -1, sm_append = 0.11935, lets_finer =0.11935, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   374, estim_aver_bat_size = 5.91, b =  9, bat_lim =  33001, error = -3, sm_append = 0.08816, lets_finer =0.08816, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   375, estim_aver_bat_size = 5.91, b =  6, bat_lim =  33001, error =  0, sm_append = 0.08727, lets_finer =0.08727, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   376, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33101, error =  1, sm_append = 0.09640, lets_finer =0.09640, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   377, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33101, error =  0, sm_append = 0.09544, lets_finer =0.09544, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   378, estim_aver_bat_size = 5.91, b =  6, bat_lim =  33101, error =  0, sm_append = 0.09448, lets_finer =0.09448, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   379, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33201, error =  1, sm_append = 0.10354, lets_finer =0.10354, max_a_sm_err = 0.1333085, low_pass = 100, finer = 1\n",
      "Step   380, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33201, error =  0, sm_append = 0.10250, lets_finer =0.10250, max_a_sm_err = 0.1319754, low_pass = 100, finer = 1\n",
      "Step   381, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33201, error =  0, sm_append = 0.10148, lets_finer =0.10148, max_a_sm_err = 0.1306556, low_pass = 100, finer = 1\n",
      "Step   382, estim_aver_bat_size = 5.91, b =  7, bat_lim =  33101, error = -1, sm_append = 0.09046, lets_finer =0.09046, max_a_sm_err = 0.1193491, low_pass = 100, finer = 1\n",
      "Step   383, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33201, error =  1, sm_append = 0.09956, lets_finer =0.09956, max_a_sm_err = 0.1035381, low_pass = 100, finer = 1\n",
      "Step   384, estim_aver_bat_size = 5.91, b =  7, bat_lim =  33101, error = -1, sm_append = 0.08856, lets_finer =0.08856, max_a_sm_err = 0.1035381, low_pass = 100, finer = 1\n",
      "Step   385, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33201, error =  1, sm_append = 0.09768, lets_finer =0.09768, max_a_sm_err = 0.1035381, low_pass = 100, finer = 1\n",
      "Step   386, estim_aver_bat_size = 5.89, b =  5, bat_lim =  33301, error =  1, sm_append = 0.10670, lets_finer =0.10670, max_a_sm_err = 0.1067003, low_pass = 100, finer = 1\n",
      "Step   387, estim_aver_bat_size = 5.88, b =  5, bat_lim =  33401, error =  1, sm_append = 0.11563, lets_finer =0.11563, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   388, estim_aver_bat_size = 5.89, b =  6, bat_lim =  33401, error =  0, sm_append = 0.11448, lets_finer =0.11448, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   389, estim_aver_bat_size = 5.90, b =  7, bat_lim =  33301, error = -1, sm_append = 0.10333, lets_finer =0.10333, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   390, estim_aver_bat_size = 5.89, b =  5, bat_lim =  33401, error =  1, sm_append = 0.11230, lets_finer =0.11230, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   391, estim_aver_bat_size = 5.89, b =  6, bat_lim =  33401, error =  0, sm_append = 0.11118, lets_finer =0.11118, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   392, estim_aver_bat_size = 5.91, b =  8, bat_lim =  33201, error = -2, sm_append = 0.09006, lets_finer =0.09006, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   393, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33301, error =  1, sm_append = 0.09916, lets_finer =0.09916, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   394, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33301, error =  0, sm_append = 0.09817, lets_finer =0.09817, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   395, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33301, error =  0, sm_append = 0.09719, lets_finer =0.09719, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   396, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33301, error =  0, sm_append = 0.09622, lets_finer =0.09622, max_a_sm_err = 0.1156333, low_pass = 100, finer = 1\n",
      "Step   397, estim_aver_bat_size = 5.90, b =  6, bat_lim =  33301, error =  0, sm_append = 0.09526, lets_finer =0.09526, max_a_sm_err = 0.1144769, low_pass = 100, finer = 1\n",
      "Step   398, estim_aver_bat_size = 5.90, b =  5, bat_lim =  33401, error =  1, sm_append = 0.10430, lets_finer =0.10430, max_a_sm_err = 0.1122988, low_pass = 100, finer = 1\n",
      "Step   399, estim_aver_bat_size = 5.89, b =  5, bat_lim =  33501, error =  1, sm_append = 0.11326, lets_finer =0.11326, max_a_sm_err = 0.1132604, low_pass = 100, finer = 1\n",
      "Step   400, estim_aver_bat_size = 5.89, b =  6, bat_lim =  33501, error =  0, sm_append = 0.11213, lets_finer =0.11213, max_a_sm_err = 0.1132604, low_pass = 100, finer = 1\n",
      "Step   401, estim_aver_bat_size = 5.86, b =  3, bat_lim =  33801, error =  3, sm_append = 0.14101, lets_finer =0.14101, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   402, estim_aver_bat_size = 5.87, b =  7, bat_lim =  33701, error = -1, sm_append = 0.12960, lets_finer =0.12960, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   403, estim_aver_bat_size = 5.86, b =  5, bat_lim =  33801, error =  1, sm_append = 0.13830, lets_finer =0.13830, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   404, estim_aver_bat_size = 5.86, b =  6, bat_lim =  33801, error =  0, sm_append = 0.13692, lets_finer =0.13692, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   405, estim_aver_bat_size = 5.86, b =  6, bat_lim =  33801, error =  0, sm_append = 0.13555, lets_finer =0.13555, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   406, estim_aver_bat_size = 5.87, b =  6, bat_lim =  33801, error =  0, sm_append = 0.13419, lets_finer =0.13419, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   407, estim_aver_bat_size = 5.88, b =  7, bat_lim =  33701, error = -1, sm_append = 0.12285, lets_finer =0.12285, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   408, estim_aver_bat_size = 5.87, b =  5, bat_lim =  33801, error =  1, sm_append = 0.13162, lets_finer =0.13162, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   409, estim_aver_bat_size = 5.86, b =  5, bat_lim =  33901, error =  1, sm_append = 0.14031, lets_finer =0.14031, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   410, estim_aver_bat_size = 5.86, b =  6, bat_lim =  33901, error =  0, sm_append = 0.13890, lets_finer =0.13890, max_a_sm_err = 0.1410065, low_pass = 100, finer = 1\n",
      "Step   411, estim_aver_bat_size = 5.85, b =  5, bat_lim =  34001, error =  1, sm_append = 0.14751, lets_finer =0.14751, max_a_sm_err = 0.1475141, low_pass = 100, finer = 1\n",
      "Step   412, estim_aver_bat_size = 5.86, b =  7, bat_lim =  33901, error = -1, sm_append = 0.13604, lets_finer =0.13604, max_a_sm_err = 0.1475141, low_pass = 100, finer = 1\n",
      "Step   413, estim_aver_bat_size = 5.86, b =  5, bat_lim =  34001, error =  1, sm_append = 0.14468, lets_finer =0.14468, max_a_sm_err = 0.1475141, low_pass = 100, finer = 1\n",
      "Step   414, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34001, error =  0, sm_append = 0.14323, lets_finer =0.14323, max_a_sm_err = 0.1475141, low_pass = 100, finer = 1\n",
      "Step   415, estim_aver_bat_size = 5.85, b =  5, bat_lim =  34101, error =  1, sm_append = 0.15180, lets_finer =0.15180, max_a_sm_err = 0.1517995, low_pass = 100, finer = 1\n",
      "Step   416, estim_aver_bat_size = 5.87, b =  8, bat_lim =  33901, error = -2, sm_append = 0.13028, lets_finer =0.13028, max_a_sm_err = 0.1517995, low_pass = 100, finer = 1\n",
      "Step   417, estim_aver_bat_size = 5.86, b =  5, bat_lim =  34001, error =  1, sm_append = 0.13898, lets_finer =0.13898, max_a_sm_err = 0.1517995, low_pass = 100, finer = 1\n",
      "Step   418, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34001, error =  0, sm_append = 0.13759, lets_finer =0.13759, max_a_sm_err = 0.1517995, low_pass = 100, finer = 1\n",
      "Step   419, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34001, error =  0, sm_append = 0.13621, lets_finer =0.13621, max_a_sm_err = 0.1517995, low_pass = 100, finer = 1\n",
      "Step   420, estim_aver_bat_size = 5.85, b =  4, bat_lim =  34201, error =  2, sm_append = 0.15485, lets_finer =0.15485, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   421, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34201, error =  0, sm_append = 0.15330, lets_finer =0.15330, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   422, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34201, error =  0, sm_append = 0.15177, lets_finer =0.15177, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   423, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34201, error =  0, sm_append = 0.15025, lets_finer =0.15025, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   424, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34201, error =  0, sm_append = 0.14875, lets_finer =0.14875, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   425, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34201, error =  0, sm_append = 0.14726, lets_finer =0.14726, max_a_sm_err = 0.1548508, low_pass = 100, finer = 1\n",
      "Step   426, estim_aver_bat_size = 5.84, b =  5, bat_lim =  34301, error =  1, sm_append = 0.15579, lets_finer =0.15579, max_a_sm_err = 0.1557890, low_pass = 100, finer = 1\n",
      "Step   427, estim_aver_bat_size = 5.84, b =  5, bat_lim =  34401, error =  1, sm_append = 0.16423, lets_finer =0.16423, max_a_sm_err = 0.1642311, low_pass = 100, finer = 1\n",
      "Step   428, estim_aver_bat_size = 5.84, b =  6, bat_lim =  34401, error =  0, sm_append = 0.16259, lets_finer =0.16259, max_a_sm_err = 0.1642311, low_pass = 100, finer = 1\n",
      "Step   429, estim_aver_bat_size = 5.83, b =  5, bat_lim =  34501, error =  1, sm_append = 0.17096, lets_finer =0.17096, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   430, estim_aver_bat_size = 5.83, b =  6, bat_lim =  34501, error =  0, sm_append = 0.16925, lets_finer =0.16925, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   431, estim_aver_bat_size = 5.85, b =  8, bat_lim =  34301, error = -2, sm_append = 0.14756, lets_finer =0.14756, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   432, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34301, error =  0, sm_append = 0.14609, lets_finer =0.14609, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   433, estim_aver_bat_size = 5.85, b =  5, bat_lim =  34401, error =  1, sm_append = 0.15462, lets_finer =0.15462, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   434, estim_aver_bat_size = 5.85, b =  6, bat_lim =  34401, error =  0, sm_append = 0.15308, lets_finer =0.15308, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   435, estim_aver_bat_size = 5.86, b =  7, bat_lim =  34301, error = -1, sm_append = 0.14155, lets_finer =0.14155, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   436, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34301, error =  0, sm_append = 0.14013, lets_finer =0.14013, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   437, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34301, error =  0, sm_append = 0.13873, lets_finer =0.13873, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   438, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34301, error =  0, sm_append = 0.13734, lets_finer =0.13734, max_a_sm_err = 0.1709629, low_pass = 100, finer = 1\n",
      "Step   439, estim_aver_bat_size = 5.86, b =  6, bat_lim =  34301, error =  0, sm_append = 0.13597, lets_finer =0.13597, max_a_sm_err = 0.1692533, low_pass = 100, finer = 1\n",
      "Step   440, estim_aver_bat_size = 5.88, b =  7, bat_lim =  34201, error = -1, sm_append = 0.12461, lets_finer =0.12461, max_a_sm_err = 0.1546243, low_pass = 100, finer = 1\n",
      "Step   441, estim_aver_bat_size = 5.88, b =  6, bat_lim =  34201, error =  0, sm_append = 0.12336, lets_finer =0.12336, max_a_sm_err = 0.1546243, low_pass = 100, finer = 1\n",
      "Step   442, estim_aver_bat_size = 5.88, b =  6, bat_lim =  34201, error =  0, sm_append = 0.12213, lets_finer =0.12213, max_a_sm_err = 0.1546243, low_pass = 100, finer = 1\n",
      "Step   443, estim_aver_bat_size = 5.87, b =  5, bat_lim =  34301, error =  1, sm_append = 0.13091, lets_finer =0.13091, max_a_sm_err = 0.1530780, low_pass = 100, finer = 1\n",
      "Step   444, estim_aver_bat_size = 5.88, b =  7, bat_lim =  34201, error = -1, sm_append = 0.11960, lets_finer =0.11960, max_a_sm_err = 0.1415473, low_pass = 100, finer = 1\n",
      "Step   445, estim_aver_bat_size = 5.89, b =  7, bat_lim =  34101, error = -1, sm_append = 0.10840, lets_finer =0.10840, max_a_sm_err = 0.1401318, low_pass = 100, finer = 1\n",
      "Step   446, estim_aver_bat_size = 5.90, b =  7, bat_lim =  34001, error = -1, sm_append = 0.09732, lets_finer =0.09732, max_a_sm_err = 0.1387305, low_pass = 100, finer = 1\n",
      "Step   447, estim_aver_bat_size = 5.91, b =  7, bat_lim =  33901, error = -1, sm_append = 0.08635, lets_finer =0.08635, max_a_sm_err = 0.1373432, low_pass = 100, finer = 1\n",
      "Step   448, estim_aver_bat_size = 5.92, b =  7, bat_lim =  33801, error = -1, sm_append = 0.07548, lets_finer =0.07548, max_a_sm_err = 0.1359697, low_pass = 100, finer = 1\n",
      "Step   449, estim_aver_bat_size = 5.92, b =  5, bat_lim =  33901, error =  1, sm_append = 0.08473, lets_finer =0.08473, max_a_sm_err = 0.1309090, low_pass = 100, finer = 1\n",
      "Step   450, estim_aver_bat_size = 5.92, b =  6, bat_lim =  33901, error =  0, sm_append = 0.08388, lets_finer =0.08388, max_a_sm_err = 0.1309090, low_pass = 100, finer = 1\n",
      "Step   451, estim_aver_bat_size = 5.92, b =  6, bat_lim =  33901, error =  0, sm_append = 0.08304, lets_finer =0.08304, max_a_sm_err = 0.1309090, low_pass = 100, finer = 1\n",
      "Step   452, estim_aver_bat_size = 5.91, b =  5, bat_lim =  34001, error =  1, sm_append = 0.09221, lets_finer =0.09221, max_a_sm_err = 0.1309090, low_pass = 100, finer = 1\n",
      "Step   453, estim_aver_bat_size = 5.91, b =  6, bat_lim =  34001, error =  0, sm_append = 0.09129, lets_finer =0.09129, max_a_sm_err = 0.1195999, low_pass = 100, finer = 1\n",
      "Step   454, estim_aver_bat_size = 5.92, b =  7, bat_lim =  33901, error = -1, sm_append = 0.08038, lets_finer =0.08038, max_a_sm_err = 0.1084039, low_pass = 100, finer = 1\n",
      "\n",
      "**************************************************\n",
      "\n",
      "neighbors_num |  layer  0  |  layer  1  |  layer  2  |  layer  3  |  layer  4  \n",
      "        0     |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        1     |\u001b[92m     33571\u001b[0m  |\u001b[92m       839\u001b[0m  |\u001b[92m         3\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        2     |\u001b[92m    108215\u001b[0m  |\u001b[92m      2200\u001b[0m  |\u001b[92m         6\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        3     |\u001b[92m    224847\u001b[0m  |\u001b[92m      4430\u001b[0m  |\u001b[92m        12\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        4     |\u001b[92m    319298\u001b[0m  |\u001b[92m      7083\u001b[0m  |\u001b[92m        25\u001b[0m  |\u001b[92m         1\u001b[0m  |\u001b[92m         4\u001b[0m  \n",
      "        5     |\u001b[92m    434151\u001b[0m  |\u001b[92m     10343\u001b[0m  |\u001b[92m        39\u001b[0m  |\u001b[92m         0\u001b[0m  |\u001b[92m         5\u001b[0m  \n",
      "        6     |\u001b[92m    553810\u001b[0m  |\u001b[92m     14133\u001b[0m  |\u001b[92m        37\u001b[0m  |\u001b[92m         1\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        7     |\u001b[92m    691277\u001b[0m  |\u001b[92m     18804\u001b[0m  |\u001b[92m        42\u001b[0m  |\u001b[92m         1\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "        8     |\u001b[92m    811932\u001b[0m  |\u001b[92m     24745\u001b[0m  |\u001b[92m        25\u001b[0m  |\u001b[92m         2\u001b[0m  |\u001b[92m         8\u001b[0m  \n",
      "        9     |\u001b[92m   1085890\u001b[0m  |\u001b[92m     32545\u001b[0m  |\u001b[92m        35\u001b[0m  |\u001b[92m         4\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       10     |\u001b[92m   1165505\u001b[0m  |\u001b[92m     44337\u001b[0m  |\u001b[92m        41\u001b[0m  |\u001b[92m        15\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       11     |\u001b[92m   1646993\u001b[0m  |\u001b[92m     64492\u001b[0m  |\u001b[92m        37\u001b[0m  |\u001b[92m        13\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       12     |\u001b[92m   1620482\u001b[0m  |\u001b[92m     92948\u001b[0m  |\u001b[92m        84\u001b[0m  |\u001b[92m         6\u001b[0m  |\u001b[92m         4\u001b[0m  \n",
      "       13     |\u001b[92m   1870005\u001b[0m  |\u001b[92m    123780\u001b[0m  |\u001b[92m        55\u001b[0m  |\u001b[92m         4\u001b[0m  |\u001b[92m        13\u001b[0m  \n",
      "       14     |\u001b[92m   1101301\u001b[0m  |\u001b[92m    148660\u001b[0m  |\u001b[92m        53\u001b[0m  |\u001b[92m         3\u001b[0m  |\u001b[92m        10\u001b[0m  \n",
      "       15     |\u001b[92m    887175\u001b[0m  |\u001b[92m    167373\u001b[0m  |\u001b[92m        75\u001b[0m  |\u001b[92m         1\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       16     |\u001b[92m    667824\u001b[0m  |\u001b[92m    180531\u001b[0m  |\u001b[92m        85\u001b[0m  |\u001b[92m         4\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       17     |\u001b[92m    537116\u001b[0m  |\u001b[92m    194543\u001b[0m  |\u001b[92m        83\u001b[0m  |\u001b[92m         5\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       18     |\u001b[91m    384525\u001b[0m  |\u001b[92m    222770\u001b[0m  |\u001b[92m        84\u001b[0m  |\u001b[92m         4\u001b[0m  |\u001b[92m         0\u001b[0m  \n",
      "       19     |\u001b[91m    282857\u001b[0m  |\u001b[92m    281657\u001b[0m  |\u001b[92m        86\u001b[0m  |\u001b[92m         4\u001b[0m  |\u001b[92m        20\u001b[0m  \n",
      "       20     |\u001b[91m    203113\u001b[0m  |\u001b[92m    402774\u001b[0m  |\u001b[92m       107\u001b[0m  |\u001b[92m         2\u001b[0m  |\u001b[92m        62\u001b[0m  \n",
      "       21     |\u001b[91m    139635\u001b[0m  |\u001b[92m    603870\u001b[0m  |\u001b[92m       138\u001b[0m  |\u001b[92m        11\u001b[0m  |\u001b[92m        18\u001b[0m  \n",
      "       22     |\u001b[91m     88554\u001b[0m  |\u001b[92m    643175\u001b[0m  |\u001b[92m        91\u001b[0m  |\u001b[92m         2\u001b[0m  |\u001b[92m        22\u001b[0m  \n",
      "       23     |\u001b[91m     57039\u001b[0m  |\u001b[92m    698770\u001b[0m  |\u001b[92m       123\u001b[0m  |\u001b[92m         7\u001b[0m  |\u001b[92m        47\u001b[0m  \n",
      "       24     |\u001b[91m     35933\u001b[0m  |\u001b[92m    657383\u001b[0m  |\u001b[92m        83\u001b[0m  |\u001b[92m         8\u001b[0m  |\u001b[92m        53\u001b[0m  \n",
      "       25     |\u001b[91m     21951\u001b[0m  |\u001b[92m    533431\u001b[0m  |\u001b[92m       112\u001b[0m  |\u001b[92m        10\u001b[0m  |\u001b[92m       135\u001b[0m  \n",
      "       26     |\u001b[91m     13238\u001b[0m  |\u001b[92m    393577\u001b[0m  |\u001b[92m        86\u001b[0m  |\u001b[92m         8\u001b[0m  |\u001b[92m       106\u001b[0m  \n",
      "       27     |\u001b[91m      7680\u001b[0m  |\u001b[92m    279150\u001b[0m  |\u001b[92m       116\u001b[0m  |\u001b[92m         7\u001b[0m  |\u001b[92m       226\u001b[0m  \n",
      "       28     |\u001b[91m      4472\u001b[0m  |\u001b[92m    203151\u001b[0m  |\u001b[92m       162\u001b[0m  |\u001b[92m        11\u001b[0m  |\u001b[92m       285\u001b[0m  \n",
      "       29     |\u001b[91m      2499\u001b[0m  |\u001b[92m    157583\u001b[0m  |\u001b[92m       130\u001b[0m  |\u001b[92m        13\u001b[0m  |\u001b[92m       258\u001b[0m  \n",
      "       30     |\u001b[91m      1477\u001b[0m  |\u001b[92m    131476\u001b[0m  |\u001b[92m       115\u001b[0m  |\u001b[92m        18\u001b[0m  |\u001b[92m       321\u001b[0m  \n",
      "       31     |\u001b[91m       877\u001b[0m  |\u001b[92m    115693\u001b[0m  |\u001b[92m       190\u001b[0m  |\u001b[92m         7\u001b[0m  |\u001b[92m       354\u001b[0m  \n",
      "       32     |\u001b[91m       509\u001b[0m  |\u001b[92m    104715\u001b[0m  |\u001b[92m       199\u001b[0m  |\u001b[92m         9\u001b[0m  |\u001b[92m       720\u001b[0m  \n",
      "       33     |\u001b[91m       329\u001b[0m  |\u001b[92m     96646\u001b[0m  |\u001b[92m       228\u001b[0m  |\u001b[92m        19\u001b[0m  |\u001b[92m       401\u001b[0m  \n",
      "       34     |\u001b[91m       191\u001b[0m  |\u001b[92m     89507\u001b[0m  |\u001b[92m       206\u001b[0m  |\u001b[92m        25\u001b[0m  |\u001b[92m       501\u001b[0m  \n",
      "       35     |\u001b[91m       111\u001b[0m  |\u001b[92m     82352\u001b[0m  |\u001b[92m       182\u001b[0m  |\u001b[92m        26\u001b[0m  |\u001b[92m       818\u001b[0m  \n",
      "       36     |\u001b[91m        68\u001b[0m  |\u001b[92m     76691\u001b[0m  |\u001b[92m       190\u001b[0m  |\u001b[92m        32\u001b[0m  |\u001b[92m      1438\u001b[0m  \n",
      "       37     |\u001b[91m        39\u001b[0m  |\u001b[91m     70553\u001b[0m  |\u001b[92m       222\u001b[0m  |\u001b[92m        39\u001b[0m  |\u001b[92m      1257\u001b[0m  \n",
      "       38     |\u001b[91m        24\u001b[0m  |\u001b[91m     65405\u001b[0m  |\u001b[92m       242\u001b[0m  |\u001b[92m        42\u001b[0m  |\u001b[92m      1656\u001b[0m  \n",
      "       39     |\u001b[91m         9\u001b[0m  |\u001b[91m     60005\u001b[0m  |\u001b[92m       242\u001b[0m  |\u001b[92m        52\u001b[0m  |\u001b[92m      1293\u001b[0m  \n",
      "       40     |\u001b[91m         6\u001b[0m  |\u001b[91m     55808\u001b[0m  |\u001b[92m       256\u001b[0m  |\u001b[92m        51\u001b[0m  |\u001b[92m      2114\u001b[0m  \n",
      "       41     |\u001b[91m         2\u001b[0m  |\u001b[91m     51407\u001b[0m  |\u001b[92m       289\u001b[0m  |\u001b[92m        73\u001b[0m  |\u001b[92m      2197\u001b[0m  \n",
      "       42     |\u001b[91m         1\u001b[0m  |\u001b[91m     46644\u001b[0m  |\u001b[92m       315\u001b[0m  |\u001b[92m        89\u001b[0m  |\u001b[92m      2251\u001b[0m  \n",
      "       43     |\u001b[91m         0\u001b[0m  |\u001b[91m     42595\u001b[0m  |\u001b[92m       295\u001b[0m  |\u001b[92m        90\u001b[0m  |\u001b[92m      2393\u001b[0m  \n",
      "       44     |\u001b[91m         0\u001b[0m  |\u001b[91m     38688\u001b[0m  |\u001b[92m       377\u001b[0m  |\u001b[92m        82\u001b[0m  |\u001b[92m      2304\u001b[0m  \n",
      "       45     |\u001b[91m         0\u001b[0m  |\u001b[91m     35103\u001b[0m  |\u001b[92m       363\u001b[0m  |\u001b[92m       102\u001b[0m  |\u001b[92m      2322\u001b[0m  \n",
      "       46     |\u001b[91m         0\u001b[0m  |\u001b[91m     31596\u001b[0m  |\u001b[92m       377\u001b[0m  |\u001b[92m       112\u001b[0m  |\u001b[92m      3619\u001b[0m  \n",
      "       47     |\u001b[91m         0\u001b[0m  |\u001b[91m     28205\u001b[0m  |\u001b[92m       397\u001b[0m  |\u001b[92m       146\u001b[0m  |\u001b[92m      2531\u001b[0m  \n",
      "       48     |\u001b[91m         0\u001b[0m  |\u001b[91m     24974\u001b[0m  |\u001b[92m       436\u001b[0m  |\u001b[92m       166\u001b[0m  |\u001b[92m      2730\u001b[0m  \n",
      "       49     |\u001b[91m         0\u001b[0m  |\u001b[91m     22059\u001b[0m  |\u001b[92m       507\u001b[0m  |\u001b[92m       235\u001b[0m  |\u001b[92m      2918\u001b[0m  \n",
      "       50     |\u001b[91m         0\u001b[0m  |\u001b[91m     19733\u001b[0m  |\u001b[92m       517\u001b[0m  |\u001b[92m       295\u001b[0m  |\u001b[92m      3189\u001b[0m  \n",
      "       51     |\u001b[91m         0\u001b[0m  |\u001b[91m     17214\u001b[0m  |\u001b[92m       602\u001b[0m  |\u001b[92m       407\u001b[0m  |\u001b[92m      3244\u001b[0m  \n",
      "       52     |\u001b[91m         0\u001b[0m  |\u001b[91m     14921\u001b[0m  |\u001b[92m       649\u001b[0m  |\u001b[92m       572\u001b[0m  |\u001b[92m      3465\u001b[0m  \n",
      "       53     |\u001b[91m         0\u001b[0m  |\u001b[91m     12968\u001b[0m  |\u001b[92m       699\u001b[0m  |\u001b[92m       700\u001b[0m  |\u001b[92m      2837\u001b[0m  \n",
      "       54     |\u001b[91m         0\u001b[0m  |\u001b[91m     11032\u001b[0m  |\u001b[92m       732\u001b[0m  |\u001b[92m       855\u001b[0m  |\u001b[92m      3730\u001b[0m  \n",
      "       55     |\u001b[91m         0\u001b[0m  |\u001b[91m      9284\u001b[0m  |\u001b[92m       874\u001b[0m  |\u001b[92m      1038\u001b[0m  |\u001b[92m      2408\u001b[0m  \n",
      "       56     |\u001b[91m         0\u001b[0m  |\u001b[91m      7814\u001b[0m  |\u001b[92m      1022\u001b[0m  |\u001b[92m      1134\u001b[0m  |\u001b[92m      2225\u001b[0m  \n",
      "       57     |\u001b[91m         0\u001b[0m  |\u001b[91m      6461\u001b[0m  |\u001b[92m      1321\u001b[0m  |\u001b[92m      1320\u001b[0m  |\u001b[92m      3451\u001b[0m  \n",
      "       58     |\u001b[91m         0\u001b[0m  |\u001b[91m      5157\u001b[0m  |\u001b[92m      1557\u001b[0m  |\u001b[92m      1626\u001b[0m  |\u001b[92m      3500\u001b[0m  \n",
      "       59     |\u001b[91m         0\u001b[0m  |\u001b[91m      4141\u001b[0m  |\u001b[92m      1848\u001b[0m  |\u001b[92m      1990\u001b[0m  |\u001b[92m      3029\u001b[0m  \n",
      "       60     |\u001b[91m         0\u001b[0m  |\u001b[91m      3229\u001b[0m  |\u001b[92m      2179\u001b[0m  |\u001b[92m      2216\u001b[0m  |\u001b[92m      2889\u001b[0m  \n",
      "       61     |\u001b[91m         0\u001b[0m  |\u001b[91m      2604\u001b[0m  |\u001b[92m      2324\u001b[0m  |\u001b[92m      2581\u001b[0m  |\u001b[92m      3073\u001b[0m  \n",
      "       62     |\u001b[91m         0\u001b[0m  |\u001b[91m      1948\u001b[0m  |\u001b[92m      2741\u001b[0m  |\u001b[92m      2790\u001b[0m  |\u001b[92m      2646\u001b[0m  \n",
      "       63     |\u001b[91m         0\u001b[0m  |\u001b[91m      1449\u001b[0m  |\u001b[92m      3289\u001b[0m  |\u001b[92m      3104\u001b[0m  |\u001b[92m      2725\u001b[0m  \n",
      "       64     |\u001b[91m         0\u001b[0m  |\u001b[91m      1077\u001b[0m  |\u001b[92m      3763\u001b[0m  |\u001b[92m      3293\u001b[0m  |\u001b[92m      3041\u001b[0m  \n",
      "       65     |\u001b[91m         0\u001b[0m  |\u001b[91m       769\u001b[0m  |\u001b[92m      4270\u001b[0m  |\u001b[92m      3670\u001b[0m  |\u001b[92m      2992\u001b[0m  \n",
      "       66     |\u001b[91m         0\u001b[0m  |\u001b[91m       534\u001b[0m  |\u001b[92m      4832\u001b[0m  |\u001b[92m      3731\u001b[0m  |\u001b[92m      2851\u001b[0m  \n",
      "       67     |\u001b[91m         0\u001b[0m  |\u001b[91m       324\u001b[0m  |\u001b[92m      5914\u001b[0m  |\u001b[92m      3801\u001b[0m  |\u001b[92m      3897\u001b[0m  \n",
      "       68     |\u001b[91m         0\u001b[0m  |\u001b[91m       217\u001b[0m  |\u001b[92m      6306\u001b[0m  |\u001b[92m      3790\u001b[0m  |\u001b[92m      2443\u001b[0m  \n",
      "       69     |\u001b[91m         0\u001b[0m  |\u001b[91m       131\u001b[0m  |\u001b[92m      7442\u001b[0m  |\u001b[92m      3884\u001b[0m  |\u001b[92m      2163\u001b[0m  \n",
      "       70     |\u001b[91m         0\u001b[0m  |\u001b[91m        78\u001b[0m  |\u001b[92m      8135\u001b[0m  |\u001b[92m      3774\u001b[0m  |\u001b[92m      2142\u001b[0m  \n",
      "       71     |\u001b[91m         0\u001b[0m  |\u001b[91m        40\u001b[0m  |\u001b[92m      8841\u001b[0m  |\u001b[92m      3765\u001b[0m  |\u001b[92m      2322\u001b[0m  \n",
      "       72     |\u001b[91m         0\u001b[0m  |\u001b[91m        24\u001b[0m  |\u001b[92m      9469\u001b[0m  |\u001b[92m      3662\u001b[0m  |\u001b[92m      2487\u001b[0m  \n",
      "       73     |\u001b[91m         0\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[92m      9808\u001b[0m  |\u001b[92m      3593\u001b[0m  |\u001b[92m      2168\u001b[0m  \n",
      "       74     |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[92m      9881\u001b[0m  |\u001b[92m      3637\u001b[0m  |\u001b[92m      1846\u001b[0m  \n",
      "       75     |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[92m     10168\u001b[0m  |\u001b[92m      3560\u001b[0m  |\u001b[92m      1649\u001b[0m  \n",
      "       76     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9917\u001b[0m  |\u001b[92m      3633\u001b[0m  |\u001b[92m      1647\u001b[0m  \n",
      "       77     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9913\u001b[0m  |\u001b[92m      3652\u001b[0m  |\u001b[92m      1912\u001b[0m  \n",
      "       78     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9738\u001b[0m  |\u001b[92m      3598\u001b[0m  |\u001b[92m      2089\u001b[0m  \n",
      "       79     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9996\u001b[0m  |\u001b[92m      3650\u001b[0m  |\u001b[92m      1627\u001b[0m  \n",
      "       80     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9877\u001b[0m  |\u001b[92m      3723\u001b[0m  |\u001b[92m      1616\u001b[0m  \n",
      "       81     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9678\u001b[0m  |\u001b[92m      3758\u001b[0m  |\u001b[92m      1074\u001b[0m  \n",
      "       82     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9877\u001b[0m  |\u001b[92m      3909\u001b[0m  |\u001b[92m      1188\u001b[0m  \n",
      "       83     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9804\u001b[0m  |\u001b[92m      3864\u001b[0m  |\u001b[92m      1483\u001b[0m  \n",
      "       84     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10067\u001b[0m  |\u001b[92m      3949\u001b[0m  |\u001b[92m      1065\u001b[0m  \n",
      "       85     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9882\u001b[0m  |\u001b[92m      3953\u001b[0m  |\u001b[92m      1580\u001b[0m  \n",
      "       86     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10181\u001b[0m  |\u001b[92m      3939\u001b[0m  |\u001b[92m       824\u001b[0m  \n",
      "       87     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10283\u001b[0m  |\u001b[92m      3953\u001b[0m  |\u001b[92m       949\u001b[0m  \n",
      "       88     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10377\u001b[0m  |\u001b[92m      3936\u001b[0m  |\u001b[92m       965\u001b[0m  \n",
      "       89     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10291\u001b[0m  |\u001b[92m      4052\u001b[0m  |\u001b[92m      1340\u001b[0m  \n",
      "       90     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10795\u001b[0m  |\u001b[92m      4143\u001b[0m  |\u001b[91m       708\u001b[0m  \n",
      "       91     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10633\u001b[0m  |\u001b[92m      4091\u001b[0m  |\u001b[91m      1049\u001b[0m  \n",
      "       92     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10664\u001b[0m  |\u001b[92m      4320\u001b[0m  |\u001b[91m       803\u001b[0m  \n",
      "       93     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10661\u001b[0m  |\u001b[92m      4198\u001b[0m  |\u001b[91m       644\u001b[0m  \n",
      "       94     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10704\u001b[0m  |\u001b[92m      4260\u001b[0m  |\u001b[91m       695\u001b[0m  \n",
      "       95     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10739\u001b[0m  |\u001b[92m      4392\u001b[0m  |\u001b[91m       797\u001b[0m  \n",
      "       96     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10884\u001b[0m  |\u001b[92m      4195\u001b[0m  |\u001b[91m       721\u001b[0m  \n",
      "       97     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10873\u001b[0m  |\u001b[92m      4313\u001b[0m  |\u001b[91m       642\u001b[0m  \n",
      "       98     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11068\u001b[0m  |\u001b[92m      4298\u001b[0m  |\u001b[91m       516\u001b[0m  \n",
      "       99     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11066\u001b[0m  |\u001b[92m      4266\u001b[0m  |\u001b[91m       837\u001b[0m  \n",
      "      100     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11060\u001b[0m  |\u001b[92m      4421\u001b[0m  |\u001b[91m       702\u001b[0m  \n",
      "      101     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11214\u001b[0m  |\u001b[92m      4347\u001b[0m  |\u001b[91m       272\u001b[0m  \n",
      "      102     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11259\u001b[0m  |\u001b[92m      4354\u001b[0m  |\u001b[91m       224\u001b[0m  \n",
      "      103     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11249\u001b[0m  |\u001b[92m      4438\u001b[0m  |\u001b[91m       590\u001b[0m  \n",
      "      104     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11203\u001b[0m  |\u001b[92m      4357\u001b[0m  |\u001b[91m       311\u001b[0m  \n",
      "      105     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11280\u001b[0m  |\u001b[92m      4194\u001b[0m  |\u001b[91m       474\u001b[0m  \n",
      "      106     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11261\u001b[0m  |\u001b[92m      4200\u001b[0m  |\u001b[91m       554\u001b[0m  \n",
      "      107     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11478\u001b[0m  |\u001b[92m      4324\u001b[0m  |\u001b[91m        79\u001b[0m  \n",
      "      108     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11413\u001b[0m  |\u001b[92m      4306\u001b[0m  |\u001b[91m       229\u001b[0m  \n",
      "      109     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11656\u001b[0m  |\u001b[92m      4325\u001b[0m  |\u001b[91m       131\u001b[0m  \n",
      "      110     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11642\u001b[0m  |\u001b[92m      4208\u001b[0m  |\u001b[91m       457\u001b[0m  \n",
      "      111     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12366\u001b[0m  |\u001b[92m      4158\u001b[0m  |\u001b[91m        54\u001b[0m  \n",
      "      112     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12506\u001b[0m  |\u001b[92m      4182\u001b[0m  |\u001b[91m        47\u001b[0m  \n",
      "      113     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12609\u001b[0m  |\u001b[92m      4220\u001b[0m  |\u001b[91m       132\u001b[0m  \n",
      "      114     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12484\u001b[0m  |\u001b[92m      3996\u001b[0m  |\u001b[91m       184\u001b[0m  \n",
      "      115     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12197\u001b[0m  |\u001b[92m      4090\u001b[0m  |\u001b[91m       167\u001b[0m  \n",
      "      116     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12152\u001b[0m  |\u001b[92m      4018\u001b[0m  |\u001b[91m        72\u001b[0m  \n",
      "      117     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12486\u001b[0m  |\u001b[92m      4034\u001b[0m  |\u001b[91m        82\u001b[0m  \n",
      "      118     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12545\u001b[0m  |\u001b[92m      4021\u001b[0m  |\u001b[91m        43\u001b[0m  \n",
      "      119     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12816\u001b[0m  |\u001b[92m      3850\u001b[0m  |\u001b[91m       217\u001b[0m  \n",
      "      120     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12985\u001b[0m  |\u001b[92m      3868\u001b[0m  |\u001b[91m       205\u001b[0m  \n",
      "      121     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13344\u001b[0m  |\u001b[92m      3943\u001b[0m  |\u001b[91m        18\u001b[0m  \n",
      "      122     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13715\u001b[0m  |\u001b[92m      3898\u001b[0m  |\u001b[91m        44\u001b[0m  \n",
      "      123     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13666\u001b[0m  |\u001b[92m      3853\u001b[0m  |\u001b[91m        19\u001b[0m  \n",
      "      124     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13702\u001b[0m  |\u001b[92m      3761\u001b[0m  |\u001b[91m        91\u001b[0m  \n",
      "      125     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13843\u001b[0m  |\u001b[92m      3767\u001b[0m  |\u001b[91m        12\u001b[0m  \n",
      "      126     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13836\u001b[0m  |\u001b[92m      3787\u001b[0m  |\u001b[91m        12\u001b[0m  \n",
      "      127     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14055\u001b[0m  |\u001b[92m      3690\u001b[0m  |\u001b[91m         5\u001b[0m  \n",
      "      128     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13835\u001b[0m  |\u001b[92m      3639\u001b[0m  |\u001b[91m        71\u001b[0m  \n",
      "      129     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14033\u001b[0m  |\u001b[92m      3502\u001b[0m  |\u001b[91m         4\u001b[0m  \n",
      "      130     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13977\u001b[0m  |\u001b[92m      3614\u001b[0m  |\u001b[91m        13\u001b[0m  \n",
      "      131     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14037\u001b[0m  |\u001b[92m      3702\u001b[0m  |\u001b[91m         5\u001b[0m  \n",
      "      132     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14471\u001b[0m  |\u001b[92m      3549\u001b[0m  |\u001b[91m         9\u001b[0m  \n",
      "      133     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14390\u001b[0m  |\u001b[92m      3385\u001b[0m  |\u001b[91m         3\u001b[0m  \n",
      "      134     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     15004\u001b[0m  |\u001b[92m      3388\u001b[0m  |\u001b[91m        12\u001b[0m  \n",
      "      135     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14941\u001b[0m  |\u001b[92m      3356\u001b[0m  |\u001b[91m         7\u001b[0m  \n",
      "      136     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     15507\u001b[0m  |\u001b[92m      3320\u001b[0m  |\u001b[91m         6\u001b[0m  \n",
      "      137     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     15943\u001b[0m  |\u001b[92m      3325\u001b[0m  |\u001b[91m         6\u001b[0m  \n",
      "      138     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     16180\u001b[0m  |\u001b[92m      3337\u001b[0m  |\u001b[91m        63\u001b[0m  \n",
      "      139     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     16624\u001b[0m  |\u001b[92m      3417\u001b[0m  |\u001b[91m        66\u001b[0m  \n",
      "      140     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     17066\u001b[0m  |\u001b[92m      3352\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      141     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     17269\u001b[0m  |\u001b[92m      3329\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      142     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     17378\u001b[0m  |\u001b[92m      3144\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      143     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     17537\u001b[0m  |\u001b[92m      3079\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      144     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     16955\u001b[0m  |\u001b[92m      3167\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      145     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     16711\u001b[0m  |\u001b[92m      2941\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      146     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     15918\u001b[0m  |\u001b[92m      2908\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      147     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     15512\u001b[0m  |\u001b[92m      2978\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      148     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14741\u001b[0m  |\u001b[92m      2887\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      149     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     14053\u001b[0m  |\u001b[92m      2872\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      150     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     13521\u001b[0m  |\u001b[92m      2949\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      151     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     12907\u001b[0m  |\u001b[92m      2842\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      152     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11995\u001b[0m  |\u001b[92m      2901\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      153     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11595\u001b[0m  |\u001b[92m      2750\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      154     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     11113\u001b[0m  |\u001b[92m      2735\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      155     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10734\u001b[0m  |\u001b[92m      2708\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      156     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m     10135\u001b[0m  |\u001b[92m      2706\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      157     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9984\u001b[0m  |\u001b[92m      2600\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      158     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9897\u001b[0m  |\u001b[92m      2608\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      159     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9405\u001b[0m  |\u001b[92m      2604\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      160     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9429\u001b[0m  |\u001b[92m      2535\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      161     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9385\u001b[0m  |\u001b[92m      2596\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      162     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9412\u001b[0m  |\u001b[92m      2507\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      163     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9240\u001b[0m  |\u001b[92m      2472\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      164     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9111\u001b[0m  |\u001b[92m      2428\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      165     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9282\u001b[0m  |\u001b[92m      2336\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      166     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      9121\u001b[0m  |\u001b[92m      2376\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      167     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8967\u001b[0m  |\u001b[92m      2326\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      168     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8880\u001b[0m  |\u001b[92m      2382\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      169     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8885\u001b[0m  |\u001b[92m      2314\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      170     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8755\u001b[0m  |\u001b[92m      2306\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      171     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8753\u001b[0m  |\u001b[92m      2292\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      172     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8524\u001b[0m  |\u001b[92m      2313\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      173     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8485\u001b[0m  |\u001b[92m      2185\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      174     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8517\u001b[0m  |\u001b[92m      2231\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      175     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8419\u001b[0m  |\u001b[92m      2201\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      176     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8248\u001b[0m  |\u001b[92m      2227\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      177     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8075\u001b[0m  |\u001b[92m      2125\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      178     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      8189\u001b[0m  |\u001b[92m      2022\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      179     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7982\u001b[0m  |\u001b[92m      2124\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      180     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7886\u001b[0m  |\u001b[92m      2090\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      181     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7807\u001b[0m  |\u001b[92m      1970\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      182     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7843\u001b[0m  |\u001b[92m      2023\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      183     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7660\u001b[0m  |\u001b[92m      2035\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      184     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7686\u001b[0m  |\u001b[92m      1956\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      185     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7597\u001b[0m  |\u001b[92m      1922\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      186     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7485\u001b[0m  |\u001b[92m      1964\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      187     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7521\u001b[0m  |\u001b[92m      1867\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      188     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7401\u001b[0m  |\u001b[92m      1873\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      189     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7528\u001b[0m  |\u001b[92m      1781\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      190     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7280\u001b[0m  |\u001b[92m      1772\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      191     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7242\u001b[0m  |\u001b[92m      1866\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      192     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7162\u001b[0m  |\u001b[92m      1777\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      193     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      7123\u001b[0m  |\u001b[92m      1725\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      194     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6935\u001b[0m  |\u001b[92m      1692\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      195     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6955\u001b[0m  |\u001b[92m      1675\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      196     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6908\u001b[0m  |\u001b[92m      1760\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      197     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6802\u001b[0m  |\u001b[92m      1664\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      198     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6799\u001b[0m  |\u001b[92m      1594\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      199     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6583\u001b[0m  |\u001b[92m      1548\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      200     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6498\u001b[0m  |\u001b[92m      1570\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      201     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6550\u001b[0m  |\u001b[92m      1593\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      202     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6500\u001b[0m  |\u001b[92m      1510\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      203     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6449\u001b[0m  |\u001b[92m      1506\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      204     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6299\u001b[0m  |\u001b[92m      1540\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      205     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6253\u001b[0m  |\u001b[92m      1492\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      206     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6241\u001b[0m  |\u001b[92m      1430\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      207     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6095\u001b[0m  |\u001b[92m      1346\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      208     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      6054\u001b[0m  |\u001b[92m      1413\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      209     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5924\u001b[0m  |\u001b[92m      1383\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      210     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5854\u001b[0m  |\u001b[92m      1279\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      211     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5911\u001b[0m  |\u001b[92m      1256\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      212     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5848\u001b[0m  |\u001b[92m      1305\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      213     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5630\u001b[0m  |\u001b[92m      1303\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      214     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5740\u001b[0m  |\u001b[92m      1261\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      215     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5658\u001b[0m  |\u001b[92m      1266\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      216     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5621\u001b[0m  |\u001b[92m      1235\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      217     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5484\u001b[0m  |\u001b[92m      1149\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      218     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5444\u001b[0m  |\u001b[92m      1227\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      219     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5447\u001b[0m  |\u001b[92m      1150\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      220     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5390\u001b[0m  |\u001b[92m      1199\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      221     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5291\u001b[0m  |\u001b[92m      1176\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      222     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5272\u001b[0m  |\u001b[92m      1150\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      223     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5029\u001b[0m  |\u001b[92m      1080\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      224     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5184\u001b[0m  |\u001b[92m      1068\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      225     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      5027\u001b[0m  |\u001b[92m      1049\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      226     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4911\u001b[0m  |\u001b[92m       976\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      227     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4953\u001b[0m  |\u001b[92m       991\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      228     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4855\u001b[0m  |\u001b[92m      1028\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      229     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4761\u001b[0m  |\u001b[91m      1032\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      230     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4931\u001b[0m  |\u001b[91m       951\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      231     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4710\u001b[0m  |\u001b[91m       982\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      232     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4851\u001b[0m  |\u001b[91m       970\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      233     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4716\u001b[0m  |\u001b[91m       922\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      234     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4764\u001b[0m  |\u001b[91m       912\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      235     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4589\u001b[0m  |\u001b[91m       872\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      236     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4584\u001b[0m  |\u001b[91m       847\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      237     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4514\u001b[0m  |\u001b[91m       852\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      238     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4398\u001b[0m  |\u001b[91m       821\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      239     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4467\u001b[0m  |\u001b[91m       838\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      240     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4375\u001b[0m  |\u001b[91m       824\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      241     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4387\u001b[0m  |\u001b[91m       814\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      242     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4281\u001b[0m  |\u001b[91m       839\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      243     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4165\u001b[0m  |\u001b[91m       777\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      244     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4257\u001b[0m  |\u001b[91m       790\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      245     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4162\u001b[0m  |\u001b[91m       755\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      246     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4167\u001b[0m  |\u001b[91m       714\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      247     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4170\u001b[0m  |\u001b[91m       734\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      248     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4039\u001b[0m  |\u001b[91m       661\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      249     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      4104\u001b[0m  |\u001b[91m       737\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      250     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3930\u001b[0m  |\u001b[91m       674\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      251     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3983\u001b[0m  |\u001b[91m       683\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      252     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3900\u001b[0m  |\u001b[91m       644\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      253     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3941\u001b[0m  |\u001b[91m       631\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      254     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3870\u001b[0m  |\u001b[91m       708\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      255     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3768\u001b[0m  |\u001b[91m       618\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      256     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3766\u001b[0m  |\u001b[91m       591\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      257     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3825\u001b[0m  |\u001b[91m       659\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      258     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3713\u001b[0m  |\u001b[91m       627\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      259     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3705\u001b[0m  |\u001b[91m       602\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      260     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3593\u001b[0m  |\u001b[91m       579\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      261     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3680\u001b[0m  |\u001b[91m       565\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      262     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3668\u001b[0m  |\u001b[91m       604\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      263     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3594\u001b[0m  |\u001b[91m       547\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      264     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3525\u001b[0m  |\u001b[91m       599\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      265     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3531\u001b[0m  |\u001b[91m       539\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      266     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3497\u001b[0m  |\u001b[91m       501\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      267     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3521\u001b[0m  |\u001b[91m       545\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      268     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3403\u001b[0m  |\u001b[91m       502\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      269     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3345\u001b[0m  |\u001b[91m       503\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      270     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3342\u001b[0m  |\u001b[91m       503\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      271     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3361\u001b[0m  |\u001b[91m       447\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      272     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3353\u001b[0m  |\u001b[91m       440\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      273     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3201\u001b[0m  |\u001b[91m       501\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      274     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3127\u001b[0m  |\u001b[91m       464\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      275     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3225\u001b[0m  |\u001b[91m       428\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      276     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3334\u001b[0m  |\u001b[91m       465\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      277     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3250\u001b[0m  |\u001b[91m       447\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      278     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3141\u001b[0m  |\u001b[91m       423\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      279     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3106\u001b[0m  |\u001b[91m       418\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      280     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3091\u001b[0m  |\u001b[91m       375\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      281     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3111\u001b[0m  |\u001b[91m       433\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      282     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3055\u001b[0m  |\u001b[91m       395\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      283     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3152\u001b[0m  |\u001b[91m       360\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      284     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3030\u001b[0m  |\u001b[91m       422\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      285     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2985\u001b[0m  |\u001b[91m       400\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      286     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3003\u001b[0m  |\u001b[91m       373\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      287     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2985\u001b[0m  |\u001b[91m       351\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      288     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2960\u001b[0m  |\u001b[91m       373\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      289     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2887\u001b[0m  |\u001b[91m       354\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      290     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2871\u001b[0m  |\u001b[91m       312\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      291     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2826\u001b[0m  |\u001b[91m       334\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      292     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2911\u001b[0m  |\u001b[91m       332\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      293     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2891\u001b[0m  |\u001b[91m       315\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      294     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      3028\u001b[0m  |\u001b[91m       353\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      295     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2722\u001b[0m  |\u001b[91m       322\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      296     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2841\u001b[0m  |\u001b[91m       304\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      297     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2699\u001b[0m  |\u001b[91m       292\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      298     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2764\u001b[0m  |\u001b[91m       353\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      299     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2696\u001b[0m  |\u001b[91m       277\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      300     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2681\u001b[0m  |\u001b[91m       293\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      301     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2666\u001b[0m  |\u001b[91m       253\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      302     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2667\u001b[0m  |\u001b[91m       251\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      303     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2576\u001b[0m  |\u001b[91m       247\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      304     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2613\u001b[0m  |\u001b[91m       295\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      305     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2660\u001b[0m  |\u001b[91m       236\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      306     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2667\u001b[0m  |\u001b[91m       234\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      307     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2592\u001b[0m  |\u001b[91m       204\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      308     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2524\u001b[0m  |\u001b[91m       242\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      309     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2595\u001b[0m  |\u001b[91m       250\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      310     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2671\u001b[0m  |\u001b[91m       246\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      311     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2571\u001b[0m  |\u001b[91m       239\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      312     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2487\u001b[0m  |\u001b[91m       243\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      313     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2407\u001b[0m  |\u001b[91m       214\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      314     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2454\u001b[0m  |\u001b[91m       235\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      315     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2433\u001b[0m  |\u001b[91m       216\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      316     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2393\u001b[0m  |\u001b[91m       202\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      317     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2493\u001b[0m  |\u001b[91m       193\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      318     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2478\u001b[0m  |\u001b[91m       208\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      319     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2382\u001b[0m  |\u001b[91m       201\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      320     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2414\u001b[0m  |\u001b[91m       177\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      321     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2316\u001b[0m  |\u001b[91m       177\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      322     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2393\u001b[0m  |\u001b[91m       175\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      323     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2346\u001b[0m  |\u001b[91m       186\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      324     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2282\u001b[0m  |\u001b[91m       175\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      325     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2281\u001b[0m  |\u001b[91m       209\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      326     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2250\u001b[0m  |\u001b[91m       169\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      327     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2269\u001b[0m  |\u001b[91m       179\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      328     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2217\u001b[0m  |\u001b[91m       152\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      329     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2181\u001b[0m  |\u001b[91m       150\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      330     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2161\u001b[0m  |\u001b[91m       141\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      331     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2206\u001b[0m  |\u001b[91m       146\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      332     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2106\u001b[0m  |\u001b[91m       142\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      333     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[92m      2116\u001b[0m  |\u001b[91m       158\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      334     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2120\u001b[0m  |\u001b[91m       146\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      335     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2167\u001b[0m  |\u001b[91m       170\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      336     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2150\u001b[0m  |\u001b[91m       122\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      337     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2048\u001b[0m  |\u001b[91m       149\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      338     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2023\u001b[0m  |\u001b[91m       135\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      339     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2142\u001b[0m  |\u001b[91m       120\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      340     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2058\u001b[0m  |\u001b[91m       122\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      341     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1959\u001b[0m  |\u001b[91m       133\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      342     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1945\u001b[0m  |\u001b[91m       125\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      343     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      2003\u001b[0m  |\u001b[91m       132\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      344     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1977\u001b[0m  |\u001b[91m       109\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      345     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1938\u001b[0m  |\u001b[91m       134\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      346     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1973\u001b[0m  |\u001b[91m       130\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      347     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1894\u001b[0m  |\u001b[91m       110\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      348     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1913\u001b[0m  |\u001b[91m       115\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      349     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1860\u001b[0m  |\u001b[91m        99\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      350     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1866\u001b[0m  |\u001b[91m       115\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      351     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1842\u001b[0m  |\u001b[91m        96\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      352     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1852\u001b[0m  |\u001b[91m       105\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      353     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1816\u001b[0m  |\u001b[91m       120\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      354     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1838\u001b[0m  |\u001b[91m       108\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      355     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1759\u001b[0m  |\u001b[91m        94\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      356     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1800\u001b[0m  |\u001b[91m        89\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      357     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1762\u001b[0m  |\u001b[91m       104\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      358     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1825\u001b[0m  |\u001b[91m        80\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      359     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1785\u001b[0m  |\u001b[91m       103\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      360     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1717\u001b[0m  |\u001b[91m        88\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      361     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1740\u001b[0m  |\u001b[91m        88\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      362     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1686\u001b[0m  |\u001b[91m       100\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      363     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1655\u001b[0m  |\u001b[91m        88\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      364     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1729\u001b[0m  |\u001b[91m        85\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      365     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1620\u001b[0m  |\u001b[91m        67\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      366     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1620\u001b[0m  |\u001b[91m        74\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      367     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1714\u001b[0m  |\u001b[91m        66\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      368     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1672\u001b[0m  |\u001b[91m        78\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      369     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1645\u001b[0m  |\u001b[91m        78\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      370     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1561\u001b[0m  |\u001b[91m        64\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      371     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1633\u001b[0m  |\u001b[91m        58\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      372     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1577\u001b[0m  |\u001b[91m        63\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      373     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1573\u001b[0m  |\u001b[91m        68\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      374     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1586\u001b[0m  |\u001b[91m        62\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      375     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1528\u001b[0m  |\u001b[91m        55\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      376     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1555\u001b[0m  |\u001b[91m        77\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      377     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1564\u001b[0m  |\u001b[91m        63\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      378     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1469\u001b[0m  |\u001b[91m        63\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      379     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1491\u001b[0m  |\u001b[91m        67\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      380     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1546\u001b[0m  |\u001b[91m        59\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      381     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1512\u001b[0m  |\u001b[91m        52\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      382     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1416\u001b[0m  |\u001b[91m        44\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      383     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1491\u001b[0m  |\u001b[91m        43\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      384     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1376\u001b[0m  |\u001b[91m        52\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      385     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1474\u001b[0m  |\u001b[91m        45\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      386     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1417\u001b[0m  |\u001b[91m        55\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      387     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1312\u001b[0m  |\u001b[91m        47\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      388     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1380\u001b[0m  |\u001b[91m        61\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      389     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1353\u001b[0m  |\u001b[91m        39\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      390     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1328\u001b[0m  |\u001b[91m        37\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      391     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1383\u001b[0m  |\u001b[91m        41\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      392     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1309\u001b[0m  |\u001b[91m        39\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      393     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1364\u001b[0m  |\u001b[91m        42\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      394     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1337\u001b[0m  |\u001b[91m        34\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      395     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1340\u001b[0m  |\u001b[91m        45\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      396     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1308\u001b[0m  |\u001b[91m        31\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      397     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1285\u001b[0m  |\u001b[91m        32\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      398     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1325\u001b[0m  |\u001b[91m        38\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      399     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1298\u001b[0m  |\u001b[91m        43\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      400     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1283\u001b[0m  |\u001b[91m        33\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      401     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1270\u001b[0m  |\u001b[91m        32\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      402     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1222\u001b[0m  |\u001b[91m        26\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      403     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1277\u001b[0m  |\u001b[91m        18\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      404     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1189\u001b[0m  |\u001b[91m        31\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      405     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1233\u001b[0m  |\u001b[91m        34\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      406     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1199\u001b[0m  |\u001b[91m        27\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      407     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1146\u001b[0m  |\u001b[91m        32\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      408     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1172\u001b[0m  |\u001b[91m        33\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      409     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1149\u001b[0m  |\u001b[91m        26\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      410     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1167\u001b[0m  |\u001b[91m        29\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      411     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1134\u001b[0m  |\u001b[91m        24\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      412     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1138\u001b[0m  |\u001b[91m        27\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      413     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1133\u001b[0m  |\u001b[91m        28\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      414     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1112\u001b[0m  |\u001b[91m        33\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      415     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1125\u001b[0m  |\u001b[91m        21\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      416     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1051\u001b[0m  |\u001b[91m        19\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      417     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1073\u001b[0m  |\u001b[91m        20\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      418     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1039\u001b[0m  |\u001b[91m        28\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      419     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1059\u001b[0m  |\u001b[91m        18\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      420     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1050\u001b[0m  |\u001b[91m        16\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      421     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1072\u001b[0m  |\u001b[91m        13\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      422     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1000\u001b[0m  |\u001b[91m        20\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      423     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       992\u001b[0m  |\u001b[91m        16\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      424     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m      1006\u001b[0m  |\u001b[91m        22\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      425     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       989\u001b[0m  |\u001b[91m        16\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      426     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       998\u001b[0m  |\u001b[91m        20\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      427     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       954\u001b[0m  |\u001b[91m        14\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      428     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       990\u001b[0m  |\u001b[91m        20\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      429     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       989\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      430     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       959\u001b[0m  |\u001b[91m        14\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      431     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       947\u001b[0m  |\u001b[91m        22\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      432     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       917\u001b[0m  |\u001b[91m        17\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      433     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       936\u001b[0m  |\u001b[91m        20\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      434     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       916\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      435     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       910\u001b[0m  |\u001b[91m        13\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      436     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       848\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      437     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       804\u001b[0m  |\u001b[91m        18\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      438     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       946\u001b[0m  |\u001b[91m        16\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      439     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       862\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      440     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       848\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      441     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       845\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      442     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       878\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      443     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       836\u001b[0m  |\u001b[91m         6\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      444     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       846\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      445     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       839\u001b[0m  |\u001b[91m        11\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      446     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       779\u001b[0m  |\u001b[91m        15\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      447     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       767\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      448     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       776\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      449     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       776\u001b[0m  |\u001b[91m         7\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      450     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       809\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      451     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       778\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      452     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       733\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      453     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       800\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      454     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       769\u001b[0m  |\u001b[91m         6\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      455     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       778\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      456     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       767\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      457     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       702\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      458     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       708\u001b[0m  |\u001b[91m         7\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      459     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       715\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      460     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       722\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      461     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       742\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      462     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       661\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      463     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       674\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      464     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       736\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      465     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       686\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      466     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       672\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      467     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       631\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      468     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       647\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      469     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       677\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      470     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       632\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      471     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       626\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      472     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       606\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      473     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       610\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      474     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       594\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      475     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       644\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      476     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       583\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      477     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       604\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      478     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       572\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      479     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       553\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      480     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       597\u001b[0m  |\u001b[91m         6\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      481     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       584\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      482     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       532\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      483     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       541\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      484     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       508\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      485     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       522\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      486     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       511\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      487     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       535\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      488     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       551\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      489     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       530\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      490     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       511\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      491     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       480\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      492     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       541\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      493     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       493\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      494     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       482\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      495     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       497\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      496     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       494\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      497     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       468\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      498     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       494\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      499     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       455\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      500     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       439\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      501     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       451\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      502     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       450\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      503     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       420\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      504     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       486\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      505     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       431\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      506     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       395\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      507     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       448\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      508     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       401\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      509     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       394\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      510     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       412\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      511     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       399\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      512     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       378\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      513     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       426\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      514     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       425\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      515     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       386\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      516     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       376\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      517     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       384\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      518     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       394\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      519     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       367\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      520     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       349\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      521     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       334\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      522     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       358\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      523     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       314\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      524     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       339\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      525     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       338\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      526     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       308\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      527     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       352\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      528     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       317\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      529     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       304\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      530     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       291\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      531     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       316\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      532     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       348\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      533     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       332\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      534     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       277\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      535     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       326\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      536     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       299\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      537     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       273\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      538     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       313\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      539     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       297\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      540     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       275\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      541     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       252\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      542     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       267\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      543     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       268\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      544     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       264\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      545     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       245\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      546     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       242\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      547     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       233\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      548     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       267\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      549     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       252\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      550     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       198\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      551     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       255\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      552     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       281\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      553     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       211\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      554     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       209\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      555     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       210\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      556     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       201\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      557     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       231\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      558     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       231\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      559     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       224\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      560     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       186\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      561     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       203\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      562     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       201\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      563     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       185\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      564     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       216\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      565     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       231\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      566     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       199\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      567     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       175\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      568     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       201\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      569     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       163\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      570     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       172\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      571     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       185\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      572     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       180\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      573     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       170\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      574     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       168\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      575     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       140\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      576     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       164\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      577     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       146\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      578     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       130\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      579     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       158\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      580     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       152\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      581     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       141\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      582     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       142\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      583     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       132\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      584     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       131\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      585     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       123\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      586     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       139\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      587     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       115\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      588     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       142\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      589     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       142\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      590     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       134\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      591     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       122\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      592     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       133\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      593     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       107\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      594     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       131\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      595     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       107\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      596     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       129\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      597     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       100\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      598     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       111\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      599     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        97\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      600     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       112\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      601     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        99\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      602     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       114\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      603     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       114\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      604     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        99\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      605     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        90\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      606     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        95\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      607     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       104\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      608     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       102\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      609     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        97\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      610     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       100\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      611     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        82\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      612     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m       106\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      613     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        76\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      614     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        79\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      615     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        88\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      616     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        91\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      617     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        74\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      618     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        64\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      619     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        74\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      620     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        81\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      621     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        70\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      622     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        76\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      623     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        69\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      624     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        83\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      625     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        53\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      626     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        53\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      627     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        48\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      628     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        57\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      629     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        67\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      630     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        50\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      631     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        53\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      632     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        56\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      633     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        43\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      634     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        46\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      635     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        53\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      636     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        47\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      637     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        48\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      638     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        47\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      639     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        42\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      640     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        41\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      641     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        39\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      642     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        34\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      643     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        37\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      644     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        35\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      645     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        37\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      646     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        34\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      647     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        41\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      648     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        26\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      649     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        28\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      650     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        36\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      651     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        32\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      652     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        28\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      653     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        32\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      654     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        31\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      655     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        19\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      656     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        27\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      657     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        22\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      658     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        35\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      659     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        35\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      660     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        24\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      661     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        23\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      662     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        22\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      663     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        23\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      664     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        15\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      665     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        19\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      666     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        17\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      667     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        14\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      668     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        23\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      669     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        26\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      670     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        11\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      671     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        15\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      672     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        23\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      673     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        13\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      674     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      675     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      676     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        14\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      677     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      678     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        10\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      679     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        13\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      680     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        11\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      681     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         6\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      682     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        12\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      683     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        11\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      684     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      685     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         8\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      686     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m        11\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      687     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      688     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      689     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         7\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      690     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      691     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         7\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      692     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      693     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         9\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      694     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      695     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      696     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      697     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      698     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      699     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      700     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      701     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      702     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      703     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      704     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         6\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      705     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      706     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      707     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      708     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      709     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         4\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      710     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      711     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         5\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      712     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      713     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      714     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      715     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      716     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      717     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      718     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      719     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      720     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      721     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      722     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      723     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      724     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      725     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      726     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      727     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      728     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         2\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      729     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      730     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      731     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      732     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         3\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      733     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      734     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      735     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      736     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      737     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      738     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      739     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      740     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      741     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      742     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      743     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      744     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      745     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      746     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      747     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      748     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      749     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      750     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      751     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      752     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      753     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      754     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      755     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      756     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      757     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      758     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      759     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      760     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      761     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "      762     |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         1\u001b[0m  |\u001b[91m         0\u001b[0m  |\u001b[91m         0\u001b[0m  \n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n",
      "chosen neighbors limits:  [ 17  36 333 228  89]\n",
      "\n",
      "Calibration done in 694.9s\n",
      "\n",
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_15.000_0.300_6\": 33801\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"0.300_0.750\": 17\u001b[0m\n",
      "\u001b[92m\"0.600_1.500\": 36\u001b[0m\n",
      "\u001b[92m\"1.200_7.200\": 333\u001b[0m\n",
      "\u001b[92m\"2.400_14.400\": 228\u001b[0m\n",
      "\u001b[92m\"4.800_28.800\": 89\u001b[0m\n",
      "Calibration done in 0.0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibrate samplers\n",
    "training_sampler.calibration(training_loader, verbose=True)\n",
    "test_sampler.calibration(test_loader, verbose=True)\n",
    "\n",
    "# Optional debug functions\n",
    "# debug_timing(training_dataset, training_loader)\n",
    "# debug_timing(test_dataset, test_loader)\n",
    "# debug_upsampling(training_dataset, training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Preparation\n",
      "*****************\n",
      "encoder_blocks is calculated as ModuleList(\n",
      "  (0): SimpleBlock(\n",
      "    (KPConv): KPConv(radius: 0.75, in_feat: 5, out_feat: 64)\n",
      "    (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (1): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.75, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (2): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 0.75, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (3): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (4): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (5): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (6): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (7): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (8): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (9): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (10): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (11): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (12): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 12.00, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (13): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 12.00, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      ")\n",
      "layer after encoder is 4\n",
      "r after encoder is 12.0\n",
      "out_dim after encoder is 2048\n",
      "decoder.blocks is ModuleList(\n",
      "  (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "  (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "  (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "  (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "  (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "  (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "  (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "  (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      ")\n",
      "layer after decoder is 0\n",
      "r after decoder is 0.75\n",
      "out_dim after decoder is 128\n",
      "Initialized the following KPFCNN architecture KPFCNN(\n",
      "  (encoder_blocks): ModuleList(\n",
      "    (0): SimpleBlock(\n",
      "      (KPConv): KPConv(radius: 0.75, in_feat: 5, out_feat: 64)\n",
      "      (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (1): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.75, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (2): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 0.75, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (3): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (4): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (5): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 1.50, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (6): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (7): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (8): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 3.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (9): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (10): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (11): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 6.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (12): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 12.00, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (13): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 12.00, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "  )\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "    (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "    (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "    (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "    (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "    (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "    (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "    (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      "  )\n",
      "  (head_mlp): UnaryBlock(in_feat: 128, out_feat: 128, BN: False, ReLU: True)\n",
      "  (head_softmax): UnaryBlock(in_feat: 128, out_feat: 9, BN: False, ReLU: True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (l1): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('\\nModel Preparation')\n",
    "print('*****************')\n",
    "\n",
    "# Define network model\n",
    "t1 = time.time()\n",
    "net = KPFCNN(config, training_dataset.label_values, training_dataset.ignored_labels)\n",
    "\n",
    "# debug = False\n",
    "# if debug:\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(net)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     for param in net.parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(param.shape)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(\"Model size %i\" % sum(param.numel() for param in net.parameters() if param.requires_grad))\n",
    "#     print('\\n*************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.2s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a trainer class\n",
    "trainer = ModelTrainer(net, config, chkp_path=chosen_chkp)\n",
    "print('Done in {:.1f}s\\n'.format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "**************\n",
      "self.output_loss= tensor(2.1990, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(12.5024, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0000 => L=14.701 acc=  3% / t(ms): 1650.6 155.6 2459.0)\n",
      "self.output_loss= tensor(2.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(11.8490, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0001 => L=14.017 acc=  9% / t(ms): 1553.5 140.7 2308.1)\n",
      "self.output_loss= tensor(2.1046, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.4353, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0002 => L=12.540 acc= 47% / t(ms): 1552.6 139.7 2270.1)\n",
      "self.output_loss= tensor(2.0542, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.8336, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0003 => L=12.888 acc= 43% / t(ms): 1580.4 141.2 2270.2)\n",
      "self.output_loss= tensor(1.9181, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.2192, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0004 => L=12.137 acc= 60% / t(ms): 1604.6 142.0 2238.2)\n",
      "self.output_loss= tensor(1.8252, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.2822, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0005 => L=12.107 acc= 59% / t(ms): 1620.2 142.5 2245.8)\n",
      "self.output_loss= tensor(1.6679, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.5816, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0006 => L=11.249 acc= 71% / t(ms): 1621.3 142.2 2205.8)\n",
      "self.output_loss= tensor(1.5241, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.7567, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0007 => L=11.281 acc= 49% / t(ms): 1694.7 145.5 2241.0)\n",
      "self.output_loss= tensor(1.3942, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.4812, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0008 => L=10.875 acc= 71% / t(ms): 1689.8 144.9 2228.6)\n",
      "self.output_loss= tensor(1.2298, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(12.4741, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0009 => L=13.704 acc= 49% / t(ms): 1716.6 146.7 2238.3)\n",
      "self.output_loss= tensor(0.9498, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.4208, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0010 => L=11.371 acc= 55% / t(ms): 1748.2 148.6 2256.0)\n",
      "self.output_loss= tensor(0.6799, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.0906, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0011 => L=10.771 acc= 67% / t(ms): 1747.3 148.6 2268.1)\n",
      "self.output_loss= tensor(0.4968, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.3325, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0012 => L=10.829 acc= 58% / t(ms): 1800.3 151.2 2288.6)\n",
      "self.output_loss= tensor(0.4004, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(10.1356, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0013 => L=10.536 acc= 53% / t(ms): 1821.0 151.5 2254.1)\n",
      "self.output_loss= tensor(0.2542, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.2727, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0014 => L=9.527 acc= 59% / t(ms): 1817.5 151.6 2229.7)\n",
      "self.output_loss= tensor(0.1560, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.3095, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0015 => L=9.466 acc= 73% / t(ms): 1814.5 151.6 2248.2)\n",
      "self.output_loss= tensor(0.4825, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(9.4940, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0016 => L=9.977 acc= 61% / t(ms): 1812.3 151.4 2263.4)\n",
      "self.output_loss= tensor(0.1622, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(8.4032, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0017 => L=8.565 acc= 59% / t(ms): 1863.2 153.1 2258.1)\n",
      "self.output_loss= tensor(0.2695, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(8.1522, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0018 => L=8.422 acc= 58% / t(ms): 1886.4 154.7 2280.5)\n",
      "self.output_loss= tensor(0.4503, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(8.7533, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0019 => L=9.204 acc= 56% / t(ms): 1913.9 156.3 2307.3)\n",
      "self.output_loss= tensor(0.7600, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(8.6730, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0020 => L=9.433 acc= 46% / t(ms): 1922.1 157.4 2316.7)\n",
      "self.output_loss= tensor(0.4202, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(8.4612, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0021 => L=8.881 acc= 55% / t(ms): 1900.2 155.8 2301.3)\n",
      "self.output_loss= tensor(0.1158, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.7009, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0022 => L=7.817 acc= 61% / t(ms): 1885.4 154.1 2262.1)\n",
      "self.output_loss= tensor(0.0412, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.3269, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0023 => L=7.368 acc= 71% / t(ms): 1864.7 152.5 2249.8)\n",
      "self.output_loss= tensor(0.0192, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.3627, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0024 => L=7.382 acc= 63% / t(ms): 1855.2 150.8 2212.4)\n",
      "self.output_loss= tensor(0.1035, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7095, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0025 => L=6.813 acc= 65% / t(ms): 1876.8 152.2 2225.2)\n",
      "self.output_loss= tensor(0.0775, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.8759, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0026 => L=7.953 acc= 77% / t(ms): 1844.1 151.2 2229.7)\n",
      "self.output_loss= tensor(0.1077, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.4777, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0027 => L=7.585 acc= 73% / t(ms): 1817.8 150.2 2233.3)\n",
      "self.output_loss= tensor(0.0601, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.7395, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0028 => L=7.800 acc= 69% / t(ms): 1802.9 149.4 2232.0)\n",
      "self.output_loss= tensor(0.3518, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.6272, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0029 => L=6.979 acc= 72% / t(ms): 1766.8 147.3 2206.6)\n",
      "self.output_loss= tensor(0.0374, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.5413, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0030 => L=7.579 acc= 64% / t(ms): 1780.6 149.0 2225.6)\n",
      "self.output_loss= tensor(0.0907, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.0491, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0031 => L=7.140 acc= 70% / t(ms): 1823.2 151.1 2253.5)\n",
      "self.output_loss= tensor(0.0430, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.0415, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0032 => L=7.085 acc= 64% / t(ms): 1856.6 153.3 2291.9)\n",
      "self.output_loss= tensor(0.0736, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.0487, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0033 => L=7.122 acc= 58% / t(ms): 1815.6 150.9 2253.6)\n",
      "self.output_loss= tensor(0.4563, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.3429, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0034 => L=7.799 acc= 45% / t(ms): 1911.6 154.9 2274.7)\n",
      "self.output_loss= tensor(0.3508, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.1221, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0035 => L=7.473 acc= 72% / t(ms): 1879.0 153.5 2276.7)\n",
      "self.output_loss= tensor(0.6483, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.4854, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0036 => L=8.134 acc= 62% / t(ms): 1854.4 152.1 2255.4)\n",
      "self.output_loss= tensor(0.3531, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1952, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0037 => L=6.548 acc= 65% / t(ms): 1846.3 152.3 2275.3)\n",
      "self.output_loss= tensor(0.0772, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.2301, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0038 => L=7.307 acc= 64% / t(ms): 1854.0 153.0 2259.8)\n",
      "self.output_loss= tensor(0.4677, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.3902, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0039 => L=6.858 acc= 63% / t(ms): 1839.4 152.4 2262.3)\n",
      "self.output_loss= tensor(0.1759, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.8541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0040 => L=8.030 acc= 76% / t(ms): 1812.5 150.9 2240.3)\n",
      "self.output_loss= tensor(0.0403, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8703, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0041 => L=6.911 acc= 64% / t(ms): 1818.7 152.0 2241.9)\n",
      "self.output_loss= tensor(0.0831, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8116, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0042 => L=6.895 acc= 61% / t(ms): 1835.8 151.9 2196.7)\n",
      "self.output_loss= tensor(0.4698, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.5117, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0043 => L=7.981 acc= 40% / t(ms): 1911.8 155.0 2211.7)\n",
      "self.output_loss= tensor(0.0637, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.2218, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0044 => L=7.286 acc= 55% / t(ms): 1927.2 156.7 2246.4)\n",
      "self.output_loss= tensor(0.0254, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5472, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0045 => L=6.573 acc= 69% / t(ms): 1930.1 157.3 2245.6)\n",
      "self.output_loss= tensor(0.1906, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5457, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0046 => L=6.736 acc= 57% / t(ms): 1947.2 157.5 2216.9)\n",
      "self.output_loss= tensor(0.0862, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5776, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0047 => L=6.664 acc= 82% / t(ms): 1904.2 156.0 2223.7)\n",
      "self.output_loss= tensor(0.3053, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7219, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0048 => L=7.027 acc= 56% / t(ms): 1879.8 154.0 2194.8)\n",
      "self.output_loss= tensor(0.1026, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.2297, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0049 => L=6.332 acc= 65% / t(ms): 1853.3 151.6 2153.6)\n",
      "self.output_loss= tensor(0.0453, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8626, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0050 => L=6.908 acc= 56% / t(ms): 1874.3 151.8 2117.7)\n",
      "self.output_loss= tensor(0.0577, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7779, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0051 => L=5.836 acc= 64% / t(ms): 1888.9 153.3 2141.7)\n",
      "self.output_loss= tensor(0.0434, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0052 => L=6.555 acc= 62% / t(ms): 1877.8 152.6 2147.5)\n",
      "self.output_loss= tensor(0.0890, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7596, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0053 => L=6.849 acc= 64% / t(ms): 1868.7 153.5 2175.3)\n",
      "self.output_loss= tensor(0.0886, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.2512, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0054 => L=6.340 acc= 71% / t(ms): 1866.6 153.6 2168.5)\n",
      "self.output_loss= tensor(1.3385, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.1954, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0055 => L=8.534 acc= 42% / t(ms): 1936.0 156.0 2180.7)\n",
      "self.output_loss= tensor(0.1153, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.9075, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0056 => L=7.023 acc= 54% / t(ms): 1987.6 158.9 2232.9)\n",
      "self.output_loss= tensor(0.2095, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.9541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0057 => L=7.164 acc= 63% / t(ms): 1965.8 157.3 2219.1)\n",
      "self.output_loss= tensor(0.1184, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8283, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0058 => L=6.947 acc= 60% / t(ms): 1941.5 155.4 2187.2)\n",
      "self.output_loss= tensor(0.5063, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7775, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0059 => L=6.284 acc= 55% / t(ms): 1897.3 154.6 2161.1)\n",
      "self.output_loss= tensor(0.4335, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9928, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0060 => L=6.426 acc= 50% / t(ms): 1899.3 154.8 2144.4)\n",
      "self.output_loss= tensor(0.3352, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.0439, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0061 => L=6.379 acc= 67% / t(ms): 1868.2 154.4 2195.1)\n",
      "self.output_loss= tensor(0.2075, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.3560, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0062 => L=7.563 acc= 56% / t(ms): 1887.1 155.6 2204.2)\n",
      "self.output_loss= tensor(0.0715, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.3969, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0063 => L=6.468 acc= 55% / t(ms): 1913.7 155.9 2180.0)\n",
      "self.output_loss= tensor(0.0802, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5716, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0064 => L=6.652 acc= 79% / t(ms): 1898.4 156.0 2241.5)\n",
      "self.output_loss= tensor(0.2074, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7786, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0065 => L=6.986 acc= 65% / t(ms): 1874.3 153.7 2204.5)\n",
      "self.output_loss= tensor(0.1880, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.6870, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0066 => L=5.875 acc= 75% / t(ms): 1838.0 152.2 2199.6)\n",
      "self.output_loss= tensor(0.2189, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.0253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0067 => L=6.244 acc= 84% / t(ms): 1781.5 149.0 2142.1)\n",
      "self.output_loss= tensor(0.1117, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.0749, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0068 => L=6.187 acc= 91% / t(ms): 1723.3 146.6 2125.2)\n",
      "self.output_loss= tensor(0.1459, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7933, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0069 => L=6.939 acc= 75% / t(ms): 1704.9 146.7 2165.7)\n",
      "self.output_loss= tensor(0.0903, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.0042, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0070 => L=7.094 acc= 59% / t(ms): 1691.1 145.2 2130.0)\n",
      "self.output_loss= tensor(0.0663, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1667, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0071 => L=6.233 acc= 71% / t(ms): 1690.6 144.8 2123.6)\n",
      "self.output_loss= tensor(0.0928, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8187, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0072 => L=6.912 acc= 83% / t(ms): 1665.6 144.0 2123.5)\n",
      "self.output_loss= tensor(0.1972, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.3977, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0073 => L=7.595 acc= 38% / t(ms): 1683.7 144.6 2094.1)\n",
      "self.output_loss= tensor(0.3668, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.5879, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0074 => L=7.955 acc= 41% / t(ms): 1753.5 147.9 2128.4)\n",
      "self.output_loss= tensor(0.2667, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.5976, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0075 => L=7.864 acc= 37% / t(ms): 1813.8 150.1 2138.0)\n",
      "self.output_loss= tensor(0.0538, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9537, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0076 => L=6.008 acc= 70% / t(ms): 1831.3 150.8 2131.2)\n",
      "self.output_loss= tensor(0.2112, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(7.1353, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0077 => L=7.347 acc= 56% / t(ms): 1877.9 153.5 2167.4)\n",
      "self.output_loss= tensor(0.0985, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1513, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0078 => L=6.250 acc= 55% / t(ms): 1872.0 152.4 2134.8)\n",
      "self.output_loss= tensor(0.2329, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.4219, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0079 => L=6.655 acc= 48% / t(ms): 1909.1 154.2 2153.0)\n",
      "self.output_loss= tensor(0.2240, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.4827, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0080 => L=6.707 acc= 56% / t(ms): 1901.8 154.4 2152.8)\n",
      "self.output_loss= tensor(0.0604, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.4788, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0081 => L=6.539 acc= 74% / t(ms): 1888.4 154.4 2183.9)\n",
      "self.output_loss= tensor(0.0385, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5676, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0082 => L=6.606 acc= 73% / t(ms): 1867.4 154.0 2203.2)\n",
      "self.output_loss= tensor(0.1813, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.9850, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0083 => L=7.166 acc= 46% / t(ms): 1885.0 154.5 2186.0)\n",
      "self.output_loss= tensor(0.0871, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.7588, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0084 => L=6.846 acc= 72% / t(ms): 1875.4 154.7 2231.4)\n",
      "self.output_loss= tensor(0.0339, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5922, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0085 => L=6.626 acc= 60% / t(ms): 1884.7 155.1 2211.9)\n",
      "self.output_loss= tensor(0.0501, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5230, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0086 => L=6.573 acc= 67% / t(ms): 1875.6 154.9 2243.6)\n",
      "self.output_loss= tensor(0.0245, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.4213, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0087 => L=6.446 acc= 48% / t(ms): 1906.2 155.5 2217.6)\n",
      "self.output_loss= tensor(0.2223, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5653, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0088 => L=6.788 acc= 55% / t(ms): 1930.5 156.2 2215.4)\n",
      "self.output_loss= tensor(0.1193, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.8272, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0089 => L=6.946 acc= 64% / t(ms): 1939.1 157.3 2247.7)\n",
      "self.output_loss= tensor(0.0351, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.6657, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0090 => L=5.701 acc= 67% / t(ms): 1917.8 155.9 2230.3)\n",
      "self.output_loss= tensor(0.0749, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.6728, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0091 => L=6.748 acc= 70% / t(ms): 1880.4 153.8 2210.4)\n",
      "self.output_loss= tensor(0.1759, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.6881, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0092 => L=6.864 acc= 67% / t(ms): 1843.7 152.3 2211.3)\n",
      "self.output_loss= tensor(0.1258, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1986, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0093 => L=6.324 acc= 63% / t(ms): 1829.7 151.6 2220.7)\n",
      "self.output_loss= tensor(0.4566, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7159, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0094 => L=6.173 acc= 59% / t(ms): 1827.0 151.8 2203.0)\n",
      "self.output_loss= tensor(0.1750, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7944, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0095 => L=5.969 acc= 57% / t(ms): 1811.3 149.6 2136.8)\n",
      "self.output_loss= tensor(0.1512, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1796, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0096 => L=6.331 acc= 65% / t(ms): 1810.0 150.0 2176.8)\n",
      "self.output_loss= tensor(0.1604, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.2910, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0097 => L=6.451 acc= 63% / t(ms): 1817.3 151.7 2199.3)\n",
      "self.output_loss= tensor(0.0646, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.0138, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0098 => L=6.078 acc= 67% / t(ms): 1809.2 150.8 2189.6)\n",
      "self.output_loss= tensor(0.3777, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7027, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0099 => L=6.080 acc= 67% / t(ms): 1791.3 150.0 2178.0)\n",
      "Validation : 0.0% (timings : 75.42 4.46)\n",
      "Validation : 1.0% (timings : 141.09 9.15)\n",
      "Validation : 2.0% (timings : 224.77 14.89)\n",
      "Validation : 3.0% (timings : 296.07 19.24)\n",
      "Validation : 4.0% (timings : 359.27 22.99)\n",
      "Validation : 5.0% (timings : 423.68 26.61)\n",
      "Validation : 6.0% (timings : 496.87 31.39)\n",
      "Validation : 7.0% (timings : 570.55 35.51)\n",
      "Validation : 8.0% (timings : 643.91 38.97)\n",
      "Validation : 9.0% (timings : 714.84 42.79)\n",
      "Validation : 10.0% (timings : 757.24 46.03)\n",
      "Validation : 11.0% (timings : 819.51 50.09)\n",
      "Validation : 12.0% (timings : 879.45 53.55)\n",
      "Validation : 13.0% (timings : 923.28 56.39)\n",
      "Validation : 14.0% (timings : 953.40 59.22)\n",
      "Validation : 15.0% (timings : 1000.13 61.78)\n",
      "Validation : 16.0% (timings : 1028.53 63.67)\n",
      "Validation : 17.0% (timings : 1047.02 64.91)\n",
      "Validation : 18.0% (timings : 1095.40 66.84)\n",
      "Validation : 19.0% (timings : 1155.87 69.86)\n",
      "Validation : 20.0% (timings : 1187.62 71.92)\n",
      "Validation : 21.0% (timings : 1207.64 73.78)\n",
      "Validation : 22.0% (timings : 1247.02 75.86)\n",
      "Validation : 23.0% (timings : 1275.22 77.58)\n",
      "Validation : 24.0% (timings : 1295.55 78.21)\n",
      "Validation : 25.0% (timings : 1300.32 79.06)\n",
      "Validation : 26.0% (timings : 1300.39 79.78)\n",
      "Validation : 27.0% (timings : 1309.43 80.92)\n",
      "Validation : 28.0% (timings : 1318.06 81.30)\n",
      "Validation : 29.0% (timings : 1327.75 82.01)\n",
      "Validation : 30.0% (timings : 1332.07 82.43)\n",
      "Validation : 31.0% (timings : 1339.35 83.60)\n",
      "Validation : 32.0% (timings : 1350.02 84.52)\n",
      "Validation : 33.0% (timings : 1358.80 85.58)\n",
      "Validation : 34.0% (timings : 1364.43 86.21)\n",
      "Validation : 35.0% (timings : 1373.68 87.38)\n",
      "Validation : 36.0% (timings : 1385.39 87.81)\n",
      "Validation : 37.0% (timings : 1407.48 88.95)\n",
      "Validation : 38.0% (timings : 1409.34 88.73)\n",
      "Validation : 39.0% (timings : 1421.07 89.82)\n",
      "Validation : 40.0% (timings : 1445.95 91.60)\n",
      "Validation : 41.0% (timings : 1445.16 92.20)\n",
      "Validation : 42.0% (timings : 1437.28 91.93)\n",
      "Validation : 43.0% (timings : 1499.96 94.67)\n",
      "Validation : 44.0% (timings : 1496.83 93.97)\n",
      "Validation : 45.0% (timings : 1489.56 94.29)\n",
      "Validation : 46.0% (timings : 1495.66 94.18)\n",
      "Validation : 47.0% (timings : 1488.49 94.31)\n",
      "Validation : 48.0% (timings : 1484.21 94.59)\n",
      "Validation : 49.0% (timings : 1474.98 93.97)\n",
      "Validation : 50.0% (timings : 1477.13 94.20)\n",
      "Validation : 51.0% (timings : 1483.15 94.38)\n",
      "Validation : 52.0% (timings : 1483.18 94.61)\n",
      "Validation : 53.0% (timings : 1492.19 95.40)\n",
      "Validation : 54.0% (timings : 1500.75 95.90)\n",
      "Validation : 55.0% (timings : 1495.78 95.60)\n",
      "Validation : 56.0% (timings : 1497.06 95.89)\n",
      "Validation : 57.0% (timings : 1493.57 96.03)\n",
      "Validation : 58.0% (timings : 1494.20 96.53)\n",
      "Validation : 59.0% (timings : 1482.14 95.95)\n",
      "Validation : 60.0% (timings : 1497.11 97.11)\n",
      "Validation : 61.0% (timings : 1489.34 96.31)\n",
      "Validation : 62.0% (timings : 1504.90 96.86)\n",
      "Validation : 63.0% (timings : 1493.89 96.20)\n",
      "Validation : 64.0% (timings : 1495.30 96.86)\n",
      "Validation : 65.0% (timings : 1501.28 96.41)\n",
      "Validation : 66.0% (timings : 1489.33 95.60)\n",
      "Validation : 67.0% (timings : 1497.25 96.11)\n",
      "Validation : 68.0% (timings : 1495.47 96.17)\n",
      "Validation : 69.0% (timings : 1495.92 96.17)\n",
      "Validation : 70.0% (timings : 1504.70 97.17)\n",
      "Validation : 71.0% (timings : 1507.43 98.22)\n",
      "Validation : 72.0% (timings : 1502.18 97.68)\n",
      "Validation : 73.0% (timings : 1505.89 97.94)\n",
      "Validation : 74.0% (timings : 1521.80 98.13)\n",
      "Validation : 75.0% (timings : 1549.06 98.76)\n",
      "Validation : 76.0% (timings : 1547.52 98.42)\n",
      "Validation : 77.0% (timings : 1562.97 99.20)\n",
      "Validation : 78.0% (timings : 1547.80 98.20)\n",
      "Validation : 79.0% (timings : 1552.62 98.40)\n",
      "Validation : 80.0% (timings : 1557.32 98.05)\n",
      "Validation : 81.0% (timings : 1550.55 97.64)\n",
      "Validation : 82.0% (timings : 1559.01 98.48)\n",
      "Validation : 83.0% (timings : 1557.44 98.96)\n",
      "Validation : 84.0% (timings : 1548.42 98.83)\n",
      "Validation : 85.0% (timings : 1560.91 99.43)\n",
      "Validation : 86.0% (timings : 1546.12 99.05)\n",
      "Validation : 87.0% (timings : 1543.25 99.22)\n",
      "Validation : 88.0% (timings : 1536.71 98.50)\n",
      "Validation : 89.0% (timings : 1582.71 100.15)\n",
      "Validation : 90.0% (timings : 1568.49 99.60)\n",
      "Validation : 91.0% (timings : 1557.82 99.11)\n",
      "Validation : 92.0% (timings : 1540.47 98.32)\n",
      "Validation : 93.0% (timings : 1575.55 100.03)\n",
      "Validation : 94.0% (timings : 1576.64 100.14)\n",
      "Validation : 95.0% (timings : 1570.03 100.11)\n",
      "Validation : 96.0% (timings : 1548.05 98.78)\n",
      "Validation : 97.0% (timings : 1544.07 98.81)\n",
      "Validation : 98.0% (timings : 1528.70 97.91)\n",
      "Validation : 99.0% (timings : 1509.96 96.71)\n",
      "field_list[0].shape[0] is 382692\n",
      "AHN mean IoU = 69.7%\n",
      "self.output_loss= tensor(0.0781, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7293, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0000 => L=5.807 acc= 76% / t(ms): 174416.9 163.0 2717.2)\n",
      "self.output_loss= tensor(0.0699, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9734, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0001 => L=6.043 acc= 69% / t(ms): 1458.7 129.9 1886.4)\n",
      "self.output_loss= tensor(0.1678, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.3826, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0002 => L=6.550 acc= 60% / t(ms): 1489.9 131.0 1898.0)\n",
      "self.output_loss= tensor(0.0336, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.0742, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0003 => L=6.108 acc= 69% / t(ms): 1518.1 133.1 1953.1)\n",
      "self.output_loss= tensor(0.4788, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.6478, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0004 => L=7.127 acc= 49% / t(ms): 1565.2 135.3 1971.7)\n",
      "self.output_loss= tensor(0.0338, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8012, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0005 => L=5.835 acc= 85% / t(ms): 1560.9 135.7 2008.3)\n",
      "self.output_loss= tensor(0.0574, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8099, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0006 => L=5.867 acc= 83% / t(ms): 1566.9 137.0 2047.0)\n",
      "self.output_loss= tensor(0.4148, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.4122, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0007 => L=6.827 acc= 24% / t(ms): 1632.5 138.7 1995.4)\n",
      "self.output_loss= tensor(0.0601, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1718, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0008 => L=6.232 acc= 74% / t(ms): 1631.7 138.9 2013.6)\n",
      "self.output_loss= tensor(0.2008, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.5040, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0009 => L=6.705 acc= 53% / t(ms): 1673.7 141.9 2051.6)\n",
      "self.output_loss= tensor(0.3278, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7797, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0010 => L=6.108 acc= 63% / t(ms): 1675.4 142.1 2066.8)\n",
      "self.output_loss= tensor(0.0585, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9605, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0011 => L=6.019 acc= 86% / t(ms): 1646.5 140.7 2037.1)\n",
      "self.output_loss= tensor(0.0650, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8038, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0012 => L=5.869 acc= 69% / t(ms): 1641.9 140.2 2027.8)\n",
      "self.output_loss= tensor(0.0658, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8734, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0013 => L=5.939 acc= 62% / t(ms): 1693.3 142.9 2049.8)\n",
      "self.output_loss= tensor(0.3809, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1145, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0014 => L=6.495 acc= 50% / t(ms): 1735.4 146.1 2109.6)\n",
      "self.output_loss= tensor(0.0462, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7080, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0015 => L=5.754 acc= 68% / t(ms): 1753.5 147.5 2119.5)\n",
      "self.output_loss= tensor(0.0448, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8619, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0016 => L=5.907 acc= 67% / t(ms): 1742.3 146.1 2084.6)\n",
      "self.output_loss= tensor(0.0203, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7232, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0017 => L=5.744 acc= 57% / t(ms): 1762.8 147.9 2108.7)\n",
      "self.output_loss= tensor(0.2616, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4138, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0018 => L=5.675 acc= 44% / t(ms): 1825.6 149.7 2098.4)\n",
      "self.output_loss= tensor(0.1765, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7793, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0019 => L=5.956 acc= 67% / t(ms): 1817.1 149.3 2112.8)\n",
      "self.output_loss= tensor(0.1490, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0020 => L=5.625 acc= 84% / t(ms): 1782.5 147.8 2114.7)\n",
      "self.output_loss= tensor(0.1030, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(6.1355, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0021 => L=6.238 acc= 65% / t(ms): 1783.0 148.4 2153.1)\n",
      "self.output_loss= tensor(0.0918, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1641, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0022 => L=5.256 acc= 80% / t(ms): 1747.5 146.3 2103.7)\n",
      "self.output_loss= tensor(0.0515, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1890, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0023 => L=5.241 acc= 67% / t(ms): 1741.1 145.4 2084.7)\n",
      "self.output_loss= tensor(0.0403, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2392, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0024 => L=5.279 acc= 72% / t(ms): 1717.7 143.7 2060.6)\n",
      "self.output_loss= tensor(0.0395, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8915, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0025 => L=5.931 acc= 63% / t(ms): 1728.9 145.0 2076.7)\n",
      "self.output_loss= tensor(0.0630, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.5469, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0026 => L=5.610 acc= 70% / t(ms): 1737.1 145.4 2085.0)\n",
      "self.output_loss= tensor(0.0861, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1893, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0027 => L=5.275 acc= 68% / t(ms): 1717.2 144.1 2067.4)\n",
      "self.output_loss= tensor(0.0904, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9119, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0028 => L=6.002 acc= 74% / t(ms): 1723.6 146.0 2104.0)\n",
      "self.output_loss= tensor(0.0830, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2243, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0029 => L=5.307 acc= 59% / t(ms): 1733.4 146.9 2105.2)\n",
      "self.output_loss= tensor(0.1520, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.5683, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0030 => L=5.720 acc= 56% / t(ms): 1754.7 149.0 2158.6)\n",
      "self.output_loss= tensor(0.0582, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1061, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0031 => L=5.164 acc= 71% / t(ms): 1755.4 148.5 2147.2)\n",
      "self.output_loss= tensor(0.1167, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.3120, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0032 => L=5.429 acc= 70% / t(ms): 1728.5 146.9 2139.8)\n",
      "self.output_loss= tensor(0.1426, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.6159, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0033 => L=4.758 acc= 76% / t(ms): 1715.8 145.8 2123.7)\n",
      "self.output_loss= tensor(0.0373, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.6700, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0034 => L=5.707 acc= 35% / t(ms): 1838.0 151.5 2068.3)\n",
      "self.output_loss= tensor(0.1101, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.6386, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0035 => L=4.749 acc= 71% / t(ms): 1827.6 150.5 2073.5)\n",
      "self.output_loss= tensor(0.2070, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2368, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0036 => L=5.444 acc= 33% / t(ms): 1862.5 150.9 2035.9)\n",
      "self.output_loss= tensor(0.0272, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.5737, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0037 => L=5.601 acc= 51% / t(ms): 1863.7 151.0 2016.6)\n",
      "self.output_loss= tensor(0.0520, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.8299, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0038 => L=5.882 acc= 75% / t(ms): 1858.7 151.8 2086.8)\n",
      "self.output_loss= tensor(0.0602, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7697, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0039 => L=5.830 acc= 79% / t(ms): 1840.6 151.8 2136.6)\n",
      "self.output_loss= tensor(0.0463, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.3082, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0040 => L=5.354 acc= 68% / t(ms): 1850.1 153.3 2171.5)\n",
      "self.output_loss= tensor(0.0463, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5602, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0041 => L=4.606 acc= 56% / t(ms): 1862.8 153.1 2128.9)\n",
      "self.output_loss= tensor(0.0283, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.8108, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0042 => L=4.839 acc= 66% / t(ms): 1855.0 152.8 2135.6)\n",
      "self.output_loss= tensor(0.1372, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4766, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0043 => L=5.614 acc= 67% / t(ms): 1845.0 152.7 2169.2)\n",
      "self.output_loss= tensor(0.0609, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9434, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0044 => L=5.004 acc= 72% / t(ms): 1807.7 150.3 2132.9)\n",
      "self.output_loss= tensor(0.1001, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.6005, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0045 => L=5.701 acc= 52% / t(ms): 1837.5 152.4 2175.7)\n",
      "self.output_loss= tensor(0.0348, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2139, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0046 => L=5.249 acc= 69% / t(ms): 1806.5 149.8 2118.2)\n",
      "self.output_loss= tensor(0.1057, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1446, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0047 => L=5.250 acc= 62% / t(ms): 1820.7 151.3 2146.2)\n",
      "self.output_loss= tensor(0.0494, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.7143, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0048 => L=5.764 acc= 76% / t(ms): 1807.0 150.8 2148.6)\n",
      "self.output_loss= tensor(0.0338, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7106, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0049 => L=4.744 acc= 68% / t(ms): 1812.5 152.0 2175.9)\n",
      "self.output_loss= tensor(0.0566, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.6844, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0050 => L=4.741 acc= 77% / t(ms): 1784.3 150.2 2156.0)\n",
      "self.output_loss= tensor(0.0164, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1034, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0051 => L=5.120 acc= 67% / t(ms): 1796.3 151.4 2182.6)\n",
      "self.output_loss= tensor(0.3802, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7710, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0052 => L=5.151 acc= 74% / t(ms): 1763.7 150.1 2182.4)\n",
      "self.output_loss= tensor(0.0388, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9843, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0053 => L=5.023 acc= 65% / t(ms): 1750.9 149.3 2183.1)\n",
      "self.output_loss= tensor(0.0578, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.5145, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0054 => L=5.572 acc= 61% / t(ms): 1761.7 151.2 2206.5)\n",
      "self.output_loss= tensor(0.0796, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4267, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0055 => L=5.506 acc= 38% / t(ms): 1809.0 151.4 2140.4)\n",
      "self.output_loss= tensor(0.3508, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4975, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0056 => L=5.848 acc= 53% / t(ms): 1840.6 152.7 2132.9)\n",
      "self.output_loss= tensor(0.0410, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.5106, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0057 => L=5.552 acc= 81% / t(ms): 1824.0 152.4 2167.7)\n",
      "self.output_loss= tensor(0.2686, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5083, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0058 => L=4.777 acc= 76% / t(ms): 1780.1 150.6 2163.1)\n",
      "self.output_loss= tensor(0.4478, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5117, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0059 => L=4.959 acc= 80% / t(ms): 1744.8 149.4 2173.4)\n",
      "self.output_loss= tensor(0.0618, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9295, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0060 => L=4.991 acc= 76% / t(ms): 1733.0 147.9 2140.9)\n",
      "self.output_loss= tensor(0.1388, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.9752, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0061 => L=6.114 acc= 72% / t(ms): 1720.7 147.2 2147.7)\n",
      "self.output_loss= tensor(0.1222, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2705, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0062 => L=5.393 acc= 69% / t(ms): 1714.8 147.1 2165.2)\n",
      "self.output_loss= tensor(0.0581, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.2234, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0063 => L=5.281 acc= 49% / t(ms): 1736.5 148.2 2110.2)\n",
      "self.output_loss= tensor(0.0397, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9686, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0064 => L=5.008 acc= 66% / t(ms): 1750.6 147.9 2104.5)\n",
      "self.output_loss= tensor(0.0277, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.4410, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0065 => L=4.469 acc= 70% / t(ms): 1746.7 146.7 2074.8)\n",
      "self.output_loss= tensor(0.0717, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.8603, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0066 => L=4.932 acc= 71% / t(ms): 1747.2 146.0 2066.1)\n",
      "self.output_loss= tensor(0.0275, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5547, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0067 => L=4.582 acc= 64% / t(ms): 1751.8 146.8 2067.2)\n",
      "self.output_loss= tensor(0.0242, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.8916, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0068 => L=4.916 acc= 66% / t(ms): 1745.0 147.1 2103.3)\n",
      "self.output_loss= tensor(0.0566, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7951, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0069 => L=4.852 acc= 73% / t(ms): 1716.9 145.3 2083.8)\n",
      "self.output_loss= tensor(0.0795, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1762, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0070 => L=5.256 acc= 81% / t(ms): 1702.8 145.1 2108.1)\n",
      "self.output_loss= tensor(0.0663, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.1852, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0071 => L=5.251 acc= 76% / t(ms): 1701.7 145.5 2135.3)\n",
      "self.output_loss= tensor(0.0345, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7876, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0072 => L=4.822 acc= 65% / t(ms): 1723.9 146.2 2140.3)\n",
      "self.output_loss= tensor(0.0642, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9023, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0073 => L=4.967 acc= 58% / t(ms): 1756.6 148.2 2157.3)\n",
      "self.output_loss= tensor(0.0322, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.0949, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0074 => L=5.127 acc= 64% / t(ms): 1760.0 149.4 2182.7)\n",
      "self.output_loss= tensor(0.1401, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.6223, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0075 => L=4.762 acc= 81% / t(ms): 1731.7 147.9 2165.8)\n",
      "self.output_loss= tensor(0.0287, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5792, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0076 => L=4.608 acc= 64% / t(ms): 1748.3 149.4 2178.6)\n",
      "self.output_loss= tensor(0.1502, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.9027, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0077 => L=5.053 acc= 70% / t(ms): 1753.6 149.0 2171.4)\n",
      "self.output_loss= tensor(0.0836, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7459, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0078 => L=4.830 acc= 75% / t(ms): 1745.1 157.1 2184.9)\n",
      "self.output_loss= tensor(0.0818, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7757, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0079 => L=4.857 acc= 58% / t(ms): 1756.1 155.6 2143.7)\n",
      "self.output_loss= tensor(0.0312, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.8210, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0080 => L=4.852 acc= 58% / t(ms): 1786.7 156.6 2142.8)\n",
      "self.output_loss= tensor(0.0234, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.3658, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0081 => L=4.389 acc= 73% / t(ms): 1787.6 156.2 2158.5)\n",
      "self.output_loss= tensor(0.0791, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.2147, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0082 => L=4.294 acc= 73% / t(ms): 1752.8 153.5 2126.4)\n",
      "self.output_loss= tensor(0.1747, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(5.4689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0083 => L=5.644 acc= 41% / t(ms): 1770.7 153.6 2120.9)\n",
      "self.output_loss= tensor(0.0438, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5664, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0084 => L=4.610 acc= 72% / t(ms): 1788.4 155.9 2191.0)\n",
      "self.output_loss= tensor(0.0297, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.2888, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0085 => L=4.318 acc= 57% / t(ms): 1797.9 155.5 2145.9)\n",
      "self.output_loss= tensor(0.1436, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.1596, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0086 => L=4.303 acc= 67% / t(ms): 1797.2 154.4 2132.4)\n",
      "self.output_loss= tensor(0.3875, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.2785, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0087 => L=4.666 acc= 78% / t(ms): 1759.3 151.8 2093.1)\n",
      "self.output_loss= tensor(0.0162, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.2531, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0088 => L=4.269 acc= 68% / t(ms): 1739.9 150.2 2089.8)\n",
      "self.output_loss= tensor(0.0243, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.2103, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0089 => L=4.235 acc= 55% / t(ms): 1741.5 148.1 2036.4)\n",
      "self.output_loss= tensor(0.1879, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.6266, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0090 => L=4.815 acc= 47% / t(ms): 1766.5 148.4 1997.6)\n",
      "self.output_loss= tensor(0.0757, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.7380, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0091 => L=4.814 acc= 54% / t(ms): 1782.5 149.3 2022.2)\n",
      "self.output_loss= tensor(0.1009, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.3398, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0092 => L=4.441 acc= 58% / t(ms): 1800.5 150.9 2060.9)\n",
      "self.output_loss= tensor(0.0875, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.3457, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0093 => L=4.433 acc= 86% / t(ms): 1779.4 150.2 2075.9)\n",
      "self.output_loss= tensor(0.1598, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.0258, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0094 => L=4.186 acc= 72% / t(ms): 1752.2 148.2 2056.4)\n",
      "self.output_loss= tensor(0.0425, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.3689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0095 => L=4.411 acc= 72% / t(ms): 1748.5 147.5 2053.0)\n",
      "self.output_loss= tensor(0.1342, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(3.8806, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0096 => L=4.015 acc= 65% / t(ms): 1765.3 149.4 2096.5)\n",
      "self.output_loss= tensor(0.0384, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.5222, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0097 => L=4.561 acc= 66% / t(ms): 1750.4 147.8 2069.2)\n",
      "self.output_loss= tensor(0.1073, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.4889, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0098 => L=4.596 acc= 49% / t(ms): 1777.2 148.9 2063.1)\n",
      "self.output_loss= tensor(0.0298, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(4.3494, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e001-i0099 => L=4.379 acc= 82% / t(ms): 1753.9 148.5 2096.9)\n",
      "Validation : 0.0% (timings : 94.83 6.16)\n",
      "Validation : 1.0% (timings : 157.40 10.24)\n",
      "Validation : 2.0% (timings : 215.95 14.12)\n",
      "Validation : 3.0% (timings : 268.59 18.09)\n",
      "Validation : 4.0% (timings : 332.69 22.24)\n",
      "Validation : 5.0% (timings : 393.69 26.58)\n",
      "Validation : 6.0% (timings : 452.30 30.66)\n",
      "Validation : 7.0% (timings : 523.79 34.69)\n",
      "Validation : 8.0% (timings : 579.55 37.80)\n",
      "Validation : 9.0% (timings : 627.72 41.71)\n",
      "Validation : 10.0% (timings : 671.78 44.49)\n",
      "Validation : 11.0% (timings : 712.40 47.49)\n",
      "Validation : 12.0% (timings : 766.08 50.28)\n",
      "Validation : 13.0% (timings : 801.88 52.12)\n",
      "Validation : 14.0% (timings : 813.81 52.74)\n",
      "Validation : 15.0% (timings : 844.25 55.30)\n",
      "Validation : 16.0% (timings : 863.88 56.36)\n",
      "Validation : 17.0% (timings : 884.29 57.76)\n",
      "Validation : 18.0% (timings : 912.02 60.12)\n",
      "Validation : 19.0% (timings : 927.45 61.03)\n",
      "Validation : 20.0% (timings : 953.47 62.52)\n",
      "Validation : 21.0% (timings : 997.19 65.13)\n",
      "Validation : 22.0% (timings : 1020.81 66.14)\n",
      "Validation : 23.0% (timings : 1068.42 69.20)\n",
      "Validation : 24.0% (timings : 1079.11 70.06)\n",
      "Validation : 25.0% (timings : 1096.69 71.66)\n",
      "Validation : 26.0% (timings : 1124.87 73.46)\n",
      "Validation : 27.0% (timings : 1129.39 73.87)\n",
      "Validation : 28.0% (timings : 1154.67 76.01)\n",
      "Validation : 29.0% (timings : 1166.48 76.93)\n",
      "Validation : 30.0% (timings : 1178.03 77.85)\n",
      "Validation : 31.0% (timings : 1172.72 77.51)\n",
      "weird val of l 26\n",
      "weird val of l 26\n",
      "weird val of l 26\n",
      "weird val of l 26\n",
      "weird val of l 26\n",
      "weird val of l 26\n",
      "Validation : 32.0% (timings : 1205.41 80.01)\n",
      "Validation : 33.0% (timings : 1213.99 80.59)\n",
      "Validation : 34.0% (timings : 1223.66 81.11)\n",
      "Validation : 35.0% (timings : 1260.26 82.74)\n",
      "Validation : 36.0% (timings : 1254.24 82.06)\n",
      "Validation : 37.0% (timings : 1259.82 83.02)\n",
      "Validation : 38.0% (timings : 1262.76 83.36)\n",
      "Validation : 39.0% (timings : 1256.82 82.67)\n",
      "Validation : 40.0% (timings : 1249.28 82.05)\n",
      "Validation : 41.0% (timings : 1260.50 83.06)\n",
      "Validation : 42.0% (timings : 1279.21 83.96)\n",
      "Validation : 43.0% (timings : 1280.42 84.03)\n",
      "Validation : 44.0% (timings : 1282.48 84.26)\n",
      "Validation : 45.0% (timings : 1288.45 84.80)\n",
      "Validation : 46.0% (timings : 1296.50 85.52)\n",
      "Validation : 47.0% (timings : 1310.23 86.20)\n",
      "Validation : 48.0% (timings : 1315.47 85.94)\n",
      "Validation : 49.0% (timings : 1332.61 87.01)\n",
      "Validation : 50.0% (timings : 1335.91 87.58)\n",
      "Validation : 51.0% (timings : 1333.79 87.27)\n",
      "Validation : 52.0% (timings : 1341.61 88.51)\n",
      "Validation : 53.0% (timings : 1327.53 87.27)\n",
      "Validation : 54.0% (timings : 1332.88 87.61)\n",
      "Validation : 55.0% (timings : 1361.76 89.01)\n",
      "Validation : 56.0% (timings : 1351.74 88.24)\n",
      "Validation : 57.0% (timings : 1359.39 88.94)\n",
      "Validation : 58.0% (timings : 1366.76 89.09)\n",
      "Validation : 59.0% (timings : 1387.75 90.46)\n",
      "Validation : 60.0% (timings : 1383.60 90.10)\n",
      "Validation : 61.0% (timings : 1376.38 89.67)\n",
      "Validation : 62.0% (timings : 1380.16 90.03)\n",
      "Validation : 63.0% (timings : 1396.02 90.73)\n",
      "Validation : 64.0% (timings : 1387.09 90.26)\n",
      "Validation : 65.0% (timings : 1391.12 90.88)\n",
      "Validation : 66.0% (timings : 1405.26 91.84)\n",
      "Validation : 67.0% (timings : 1394.55 90.59)\n",
      "Validation : 68.0% (timings : 1399.05 91.40)\n",
      "Validation : 69.0% (timings : 1401.52 91.55)\n",
      "Validation : 70.0% (timings : 1396.05 90.78)\n",
      "Validation : 71.0% (timings : 1408.83 91.97)\n",
      "Validation : 72.0% (timings : 1410.44 92.32)\n",
      "Validation : 73.0% (timings : 1400.65 91.76)\n",
      "Validation : 74.0% (timings : 1399.17 91.92)\n",
      "Validation : 75.0% (timings : 1394.22 91.94)\n",
      "Validation : 76.0% (timings : 1392.22 91.74)\n",
      "Validation : 77.0% (timings : 1396.21 92.02)\n",
      "Validation : 78.0% (timings : 1382.96 91.05)\n",
      "Validation : 79.0% (timings : 1384.71 91.82)\n",
      "Validation : 80.0% (timings : 1393.71 92.51)\n",
      "Validation : 81.0% (timings : 1422.47 93.57)\n",
      "Validation : 82.0% (timings : 1413.01 92.74)\n",
      "Validation : 83.0% (timings : 1414.30 92.85)\n",
      "Validation : 84.0% (timings : 1403.66 91.92)\n",
      "Validation : 85.0% (timings : 1412.96 92.62)\n",
      "Validation : 86.0% (timings : 1425.01 93.40)\n",
      "Validation : 87.0% (timings : 1414.78 92.94)\n",
      "Validation : 88.0% (timings : 1423.28 93.88)\n",
      "Validation : 89.0% (timings : 1415.34 93.03)\n",
      "Validation : 90.0% (timings : 1415.32 93.68)\n",
      "Validation : 91.0% (timings : 1397.60 92.11)\n",
      "Validation : 92.0% (timings : 1390.67 91.82)\n",
      "Validation : 93.0% (timings : 1437.02 94.31)\n",
      "Validation : 94.0% (timings : 1431.91 94.72)\n",
      "Validation : 95.0% (timings : 1413.94 93.10)\n",
      "Validation : 96.0% (timings : 1410.19 92.95)\n",
      "Validation : 97.0% (timings : 1392.07 91.58)\n",
      "Validation : 98.0% (timings : 1406.20 92.32)\n",
      "Validation : 99.0% (timings : 1413.38 92.58)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6441,) (6447,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-273df7d3ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net, training_loader, val_loader, config)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(self, net, val_loader, config)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_segmentation_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcloud_segmentation_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mcloud_segmentation_validation\u001b[0;34m(self, net, val_loader, config, debug)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;31m# Confusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mConfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_confusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/metrics.py\u001b[0m in \u001b[0;36mfast_confusion\u001b[0;34m(true, pred, label_values)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Vectorized confusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mvec_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Add possible missing values due to classes not being in pred or true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6441,) (6447,) "
     ]
    }
   ],
   "source": [
    "print('\\nStart training')\n",
    "print('**************')\n",
    "\n",
    "# Training\n",
    "trainer.train(net, training_loader, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Forcing exit now')\n",
    "#os.kill(os.getpid(), signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

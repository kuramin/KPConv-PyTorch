{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      0=================================0\n",
    "#      |    Kernel Point Convolutions    |\n",
    "#      0=================================0\n",
    "#\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Callable script to start a training on AHN dataset\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#      Hugues THOMAS - 06/03/2020\n",
    "#\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Imports and global variables\n",
    "#       \\**********************************/\n",
    "#\n",
    "\n",
    "# Common libs\n",
    "import signal\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "from datasets.AHN import *\n",
    "#from datasets.S3DIS import *  # kuramin added\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.trainer import ModelTrainer\n",
    "from models.architectures import KPFCNN\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Config Class\n",
    "#       \\******************/\n",
    "#\n",
    "class AHNConfig(Config):\n",
    "    \"\"\"\n",
    "    Override the parameters you want to modify for this dataset\n",
    "    \"\"\"\n",
    "\n",
    "    ####################\n",
    "    # Dataset parameters\n",
    "    ####################\n",
    "\n",
    "    # Dataset name\n",
    "    dataset = 'AHN'\n",
    "\n",
    "    # Number of classes in the dataset (This value is overwritten by dataset class when Initializating dataset).\n",
    "    num_classes = None\n",
    "\n",
    "    # Type of task performed on this dataset (also overwritten)\n",
    "    dataset_task = ''\n",
    "\n",
    "    # Number of CPU threads for the input pipeline\n",
    "    input_threads = 0  # 10 kuramin changed\n",
    "\n",
    "    #########################\n",
    "    # Architecture definition\n",
    "    #########################\n",
    "\n",
    "    # Define layers\n",
    "    architecture = ['simple',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb',\n",
    "                    'resnetb',\n",
    "                    'resnetb_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable_strided',\n",
    "                    'resnetb_deformable',\n",
    "                    'resnetb_deformable',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary',\n",
    "                    'nearest_upsample',\n",
    "                    'unary']\n",
    "\n",
    "    ###################\n",
    "    # KPConv parameters\n",
    "    ###################\n",
    "\n",
    "    # Radius of the input sphere\n",
    "    in_radius = 15 #1.5 kuramin changed from s3dis to ahn\n",
    "\n",
    "    # Number of kernel points\n",
    "    num_kernel_points = 15  # kuramin changed back from 9\n",
    "\n",
    "    # Size of the first subsampling grid in meter\n",
    "    first_subsampling_dl = 2.0 #0.03 kuramin changed from s3dis to ahn\n",
    "\n",
    "    # Radius of convolution in \"number grid cell\". (2.5 is the standard value)\n",
    "    conv_radius = 2.5\n",
    "\n",
    "    # Radius of deformable convolution in \"number grid cell\". Larger so that deformed kernel can spread out\n",
    "    deform_radius = 6.0\n",
    "\n",
    "    # Radius of the area of influence of each kernel point in \"number grid cell\". (1.0 is the standard value)\n",
    "    KP_extent = 1.2\n",
    "\n",
    "    # Behavior of convolutions in ('constant', 'linear', 'gaussian')\n",
    "    KP_influence = 'linear'\n",
    "\n",
    "    # Aggregation function of KPConv in ('closest', 'sum')\n",
    "    aggregation_mode = 'sum'\n",
    "\n",
    "    # Choice of input features\n",
    "    first_features_dim = 128 # kuramin changed back from 8\n",
    "    in_features_dim = 5 # kuramin changed back from 4\n",
    "\n",
    "    # Can the network learn modulations\n",
    "    modulated = False\n",
    "\n",
    "    # Batch normalization parameters\n",
    "    use_batch_norm = True\n",
    "    batch_norm_momentum = 0.02\n",
    "\n",
    "    # Deformable offset loss\n",
    "    # 'point2point' fitting geometry by penalizing distance from deform point to input points\n",
    "    # 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)\n",
    "    deform_fitting_mode = 'point2point'\n",
    "    deform_fitting_power = 1.0              # Multiplier for the fitting/repulsive loss\n",
    "    deform_lr_factor = 0.1                  # Multiplier for learning rate applied to the deformations\n",
    "    repulse_extent = 1.2                    # Distance of repulsion for deformed kernel points\n",
    "\n",
    "    #####################\n",
    "    # Training parameters\n",
    "    #####################\n",
    "\n",
    "    # Maximal number of epochs\n",
    "    max_epoch = 10  # 500  kuramin changed\n",
    "\n",
    "    # Learning rate management\n",
    "    learning_rate = 1e-2\n",
    "    momentum = 0.98\n",
    "    lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}\n",
    "    grad_clip_norm = 100.0\n",
    "\n",
    "    # Number of batch\n",
    "    batch_num = 6  # target_aver_batch_size will be set equal to it\n",
    "\n",
    "    # Number of steps per epoch (how many batches will be created from dataloader by enumerate(dataloader))\n",
    "    steps_per_epoch = 50  # kuramin changed back from 100\n",
    "\n",
    "    # Number of validation examples per epoch\n",
    "    validation_size = 50\n",
    "\n",
    "    # Number of epoch between each checkpoint\n",
    "    checkpoint_gap = 50\n",
    "\n",
    "    # Augmentations\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_symmetries = [True, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_scale_min = 0.8\n",
    "    augment_scale_max = 1.2\n",
    "    augment_noise = 0.001\n",
    "    augment_color = 0.8\n",
    "\n",
    "    # The way we balance segmentation loss\n",
    "    #   > 'none': Each point in the whole batch has the same contribution.\n",
    "    #   > 'class': Each class has the same contribution (points are weighted according to class balance)\n",
    "    #   > 'batch': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)\n",
    "    segloss_balance = 'none'\n",
    "\n",
    "    # Do we need to save convergence\n",
    "    saving = True\n",
    "    saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs is 4\n",
      "GPU_ID is 3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#           Main Call\n",
    "#       \\***************/\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "############################\n",
    "# Initialize the environment\n",
    "############################\n",
    "\n",
    "# Set which gpu is going to be used\n",
    "number_of_gpus = str(subprocess.check_output([\"nvidia-smi\", \"-L\"])).count('UUID')\n",
    "print('Number of GPUs is', number_of_gpus)\n",
    "\n",
    "if number_of_gpus == 1:\n",
    "    GPU_ID = '0'\n",
    "else:\n",
    "    GPU_ID = '3'\n",
    "print('GPU_ID is', GPU_ID)\n",
    "\n",
    "# Set GPU visible device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Previous chkp\n",
    "###############\n",
    "\n",
    "# Choose here if you want to start training from a previous snapshot (None for new training)\n",
    "# previous_training_path = 'Log_2020-03-19_19-53-27'\n",
    "previous_training_path = ''\n",
    "\n",
    "# Choose index of checkpoint to start from. If None, uses the latest chkp\n",
    "chkp_idx = None\n",
    "if previous_training_path:\n",
    "\n",
    "    # Find all snapshot in the chosen training folder\n",
    "    chkp_path = os.path.join('results', previous_training_path, 'checkpoints')\n",
    "    chkps = [f for f in os.listdir(chkp_path) if f[:4] == 'chkp']\n",
    "\n",
    "    # Find which snapshot to restore\n",
    "    if chkp_idx is None:\n",
    "        chosen_chkp = 'current_chkp.tar'\n",
    "    else:\n",
    "        chosen_chkp = np.sort(chkps)[chkp_idx]\n",
    "    chosen_chkp = os.path.join('results', previous_training_path, 'checkpoints', chosen_chkp)\n",
    "\n",
    "else:\n",
    "    chosen_chkp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preparation\n",
      "****************\n",
      "self.deform_layers set to [False, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Prepare Data (several cells)\n",
    "##############\n",
    "\n",
    "print()\n",
    "print('Data Preparation')\n",
    "print('****************')\n",
    "\n",
    "# Initialize configuration class\n",
    "config = AHNConfig()\n",
    "if previous_training_path:\n",
    "    config.load(os.path.join('results', previous_training_path))\n",
    "    config.saving_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.saving_path is None\n"
     ]
    }
   ],
   "source": [
    "# Get path from argument if given\n",
    "if len(sys.argv) > 1:\n",
    "    config.saving_path = None  #sys.argv[1]\n",
    "    print('config.saving_path is', config.saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.deform_layers set to []\n",
      "\n",
      "Found KDTree for cloud 1_rgb, subsampled at 2.000\n",
      "5.6 MB loaded in 0.0s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "self.deform_layers set to []\n",
      "\n",
      "Found KDTree for cloud 2_rgb, subsampled at 2.000\n",
      "5.9 MB loaded in 0.0s\n",
      "\n",
      "Preparing potentials\n",
      "Done in 0.0s\n",
      "\n",
      "Preparing reprojection indices for testing\n",
      "2_rgb done in 0.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets\n",
    "training_dataset = AHNDataset(config, set='training', use_potentials=True)  # kuramin commented\n",
    "test_dataset = AHNDataset(config, set='validation', use_potentials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samplers\n",
    "training_sampler = AHNSampler(training_dataset)  # defines the strategy to draw samples from the dataset\n",
    "test_sampler = AHNSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataloader\n",
    "r\"\"\"\n",
    "    dataset (Dataset): dataset from which to load the data.\n",
    "    batch_size (int, optional): how many samples per batch to load\n",
    "        (default: ``1``).\n",
    "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "        at every epoch (default: ``False``).\n",
    "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
    "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
    "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
    "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
    "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
    "    num_workers (int, optional): how many subprocesses to use for data\n",
    "        loading. ``0`` means that the data will be loaded in the main process.\n",
    "        (default: ``0``)\n",
    "    collate_fn (callable, optional): merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset.\n",
    "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "        into CUDA pinned memory before returning them.  If your data elements\n",
    "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "        see the example below.\n",
    "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "        the size of dataset is not divisible by the batch size, then the last batch\n",
    "        will be smaller. (default: ``False``)\n",
    "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "        from workers. Should always be non-negative. (default: ``0``)\n",
    "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "        input, after seeding and before data loading. (default: ``None``)\n",
    "\"\"\"\n",
    "training_loader = DataLoader(training_dataset,\n",
    "                             batch_size=1,\n",
    "                             sampler=training_sampler,\n",
    "                             collate_fn=AHNCollate,\n",
    "                             num_workers=config.input_threads,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=AHNCollate,\n",
    "                         num_workers=config.input_threads,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_15.000_2.000_6\": 1501\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"2.000_5.000\": 48\u001b[0m\n",
      "\u001b[92m\"4.000_10.000\": 38\u001b[0m\n",
      "\u001b[92m\"8.000_48.000\": 31\u001b[0m\n",
      "\u001b[92m\"16.000_96.000\": 8\u001b[0m\n",
      "\u001b[92m\"32.000_192.000\": 6\u001b[0m\n",
      "Calibration done in 0.0s\n",
      "\n",
      "\n",
      "Starting Calibration (use verbose=True for more details)\n",
      "\n",
      "Previous calibration found:\n",
      "Check batch limit dictionary\n",
      "\u001b[92m\"potentials_15.000_2.000_6\": 1501\u001b[0m\n",
      "Check neighbors limit dictionary\n",
      "\u001b[92m\"2.000_5.000\": 48\u001b[0m\n",
      "\u001b[92m\"4.000_10.000\": 38\u001b[0m\n",
      "\u001b[92m\"8.000_48.000\": 31\u001b[0m\n",
      "\u001b[92m\"16.000_96.000\": 8\u001b[0m\n",
      "\u001b[92m\"32.000_192.000\": 6\u001b[0m\n",
      "Calibration done in 0.0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibrate samplers\n",
    "training_sampler.calibration(training_loader, verbose=True)\n",
    "test_sampler.calibration(test_loader, verbose=True)\n",
    "\n",
    "# Optional debug functions\n",
    "# debug_timing(training_dataset, training_loader)\n",
    "# debug_timing(test_dataset, test_loader)\n",
    "# debug_upsampling(training_dataset, training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Preparation\n",
      "*****************\n",
      "encoder_blocks is calculated as ModuleList(\n",
      "  (0): SimpleBlock(\n",
      "    (KPConv): KPConv(radius: 5.00, in_feat: 5, out_feat: 64)\n",
      "    (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (1): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 5.00, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (2): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 5.00, in_feat: 32, out_feat: 32)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (3): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (4): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (5): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (6): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (7): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (8): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (9): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (10): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (11): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (12): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 80.00, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (13): ResnetBottleneckBlock(\n",
      "    (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "    (KPConv): KPConv(radius: 80.00, in_feat: 512, out_feat: 512)\n",
      "    (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "    (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "    (unary_shortcut): Identity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      ")\n",
      "layer after encoder is 4\n",
      "r after encoder is 80.0\n",
      "out_dim after encoder is 2048\n",
      "decoder.blocks is ModuleList(\n",
      "  (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "  (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "  (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "  (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "  (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "  (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "  (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "  (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      ")\n",
      "layer after decoder is 0\n",
      "r after decoder is 5.0\n",
      "out_dim after decoder is 128\n",
      "Initialized the following KPFCNN architecture KPFCNN(\n",
      "  (encoder_blocks): ModuleList(\n",
      "    (0): SimpleBlock(\n",
      "      (KPConv): KPConv(radius: 5.00, in_feat: 5, out_feat: 64)\n",
      "      (batch_norm): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (1): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 64, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 5.00, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 64, out_feat: 128, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (2): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 32, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 5.00, in_feat: 32, out_feat: 32)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 32, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 32, out_feat: 128, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (3): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 128, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 128, out_feat: 256, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (4): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (5): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 64, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 10.00, in_feat: 64, out_feat: 64)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 64, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 64, out_feat: 256, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (6): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 256, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 256, out_feat: 512, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (7): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (8): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 128, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 20.00, in_feat: 128, out_feat: 128)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 128, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 128, out_feat: 512, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (9): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 512, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 512, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (10): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (11): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 256, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 40.00, in_feat: 256, out_feat: 256)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 256, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 256, out_feat: 1024, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (12): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 1024, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 80.00, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): UnaryBlock(in_feat: 1024, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (13): ResnetBottleneckBlock(\n",
      "      (unary1): UnaryBlock(in_feat: 2048, out_feat: 512, BN: True, ReLU: True)\n",
      "      (KPConv): KPConv(radius: 80.00, in_feat: 512, out_feat: 512)\n",
      "      (batch_norm_conv): BatchNormBlock(in_feat: 512, momentum: 0.020, only_bias: False)\n",
      "      (unary2): UnaryBlock(in_feat: 512, out_feat: 2048, BN: True, ReLU: False)\n",
      "      (unary_shortcut): Identity()\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "  )\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0): NearestUpsampleBlock(layer: 4 -> 3)\n",
      "    (1): UnaryBlock(in_feat: 3072, out_feat: 1024, BN: True, ReLU: True)\n",
      "    (2): NearestUpsampleBlock(layer: 3 -> 2)\n",
      "    (3): UnaryBlock(in_feat: 1536, out_feat: 512, BN: True, ReLU: True)\n",
      "    (4): NearestUpsampleBlock(layer: 2 -> 1)\n",
      "    (5): UnaryBlock(in_feat: 768, out_feat: 256, BN: True, ReLU: True)\n",
      "    (6): NearestUpsampleBlock(layer: 1 -> 0)\n",
      "    (7): UnaryBlock(in_feat: 384, out_feat: 128, BN: True, ReLU: True)\n",
      "  )\n",
      "  (head_mlp): UnaryBlock(in_feat: 128, out_feat: 128, BN: False, ReLU: True)\n",
      "  (head_softmax): UnaryBlock(in_feat: 128, out_feat: 9, BN: False, ReLU: True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (l1): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('\\nModel Preparation')\n",
    "print('*****************')\n",
    "\n",
    "# Define network model\n",
    "t1 = time.time()\n",
    "net = KPFCNN(config, training_dataset.label_values, training_dataset.ignored_labels)\n",
    "\n",
    "# debug = False\n",
    "# if debug:\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(net)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     for param in net.parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(param.shape)\n",
    "#     print('\\n*************************************\\n')\n",
    "#     print(\"Model size %i\" % sum(param.numel() for param in net.parameters() if param.requires_grad))\n",
    "#     print('\\n*************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a trainer class\n",
    "trainer = ModelTrainer(net, config, chkp_path=chosen_chkp)\n",
    "print('Done in {:.1f}s\\n'.format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "**************\n",
      "repulsive_loss tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.9474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2014, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.1956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2268, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2476, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(6.6921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3160, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(8.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3607, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(9.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.2441, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(19.3049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0000 => L=21.549 acc=  1% / t(ms):  54.5 936.9  73.1)\n",
      "repulsive_loss tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.8690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.8214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.7373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.8771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3137, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.9671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3541, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(6.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.6174, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(7.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.8899, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(7.9514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.2574, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(16.7927, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.8125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.6647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3307, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.4506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.4505, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.4733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.5271, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.4658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.6398, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.4634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.3235, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.9347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(4.3449, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(6.4349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.2304, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(17.2148, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.7261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.2424, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.4823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.2122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.5436, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.0223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.7389, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.7896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.8961, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.5912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.7766, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.2768, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.4328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.1908, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(17.1424, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.2159, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.6609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.4403, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.3156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.7021, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.8375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(1.1504, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.4647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(1.7118, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.0142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.2376, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.6004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(4.1287, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.4913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(5.3727, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(5.0376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.0992, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(15.4478, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.2616, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.6143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(0.5939, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.1290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(1.0255, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.5322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.1298, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.9951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.7759, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.3501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(4.9708, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.7522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.7571, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.4591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(9.1927, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.8261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(2.0695, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(16.8450, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0005 => L=18.914 acc= 33% / t(ms):  52.2  71.5  85.9)\n",
      "repulsive_loss tensor(0.8097, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.4261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(1.5272, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.8230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.2274, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(4.7201, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.4849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(7.7343, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.7368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(10.1318, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(11.7437, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.6089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(13.4608, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.1256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(1.8068, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(19.7119, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.4902, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.5042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(1.1819, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.8846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.0177, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.1659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(4.0622, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.4871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.0612, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.7665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(7.6786, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.3289, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(9.4844, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.9063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(1.8268, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(17.2971, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.9305, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.4037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.1585, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.4102, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.9659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.1333, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.2379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.1373, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.5080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(10.0226, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.7950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(10.6669, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.7707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(11.4977, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.5454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(1.4583, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(18.5884, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.9142, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.4151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.1537, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.3627, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.0175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(5.4869, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.3772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.7827, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.8278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.2031, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.1935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.7327, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.3671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(9.3064, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.2236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(1.6187, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(17.7537, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "repulsive_loss tensor(0.8654, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.5121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.3540, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.7919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.7658, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(5.1893, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.6084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.4082, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.2610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(7.7214, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.7697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.2949, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.9530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(8.8486, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(4.9360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(1.7360, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(18.7207, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "e000-i0010 => L=20.457 acc= 23% / t(ms):  53.9  71.0  82.0)\n",
      "repulsive_loss tensor(0.8405, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.4234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(2.2616, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.7089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(3.5636, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(0.9153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(5.0623, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.3996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(6.1515, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(1.8508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(7.6165, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(2.2293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(9.0418, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.0165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "repulsive_loss tensor(10.2087, device='cuda:0', grad_fn=<AddBackward0>) fitting_loss tensor(3.6183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "self.output_loss= tensor(0.7650, device='cuda:0', grad_fn=<NllLoss2DBackward>) self.reg_loss= tensor(17.4452, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-273df7d3ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net, training_loader, val_loader, config)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;31m# Check kill signal (running_PID.txt deleted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/datasets/AHN.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, batch_i)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_potentials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotential_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/datasets/AHN.py\u001b[0m in \u001b[0;36mpotential_item\u001b[0;34m(self, batch_i, debug_workers)\u001b[0m\n\u001b[1;32m    427\u001b[0m                                               \u001b[0mstacked_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                                               \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                                               stack_lengths)\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/datasets/common.py\u001b[0m in \u001b[0;36msegmentation_inputs\u001b[0;34m(self, stacked_points, stacked_features, labels, stack_lengths)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;31m# neigh_indices are indices of neighbors for every point in stacked_points (not only barycenters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;31m#print('r before neigh_indices', r)  kuramin_print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mneigh_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;31m# else: #kuramin commented (indentation ends)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diploma/repos/KPConv-pytorch/KPConv-PyTorch/datasets/common.py\u001b[0m in \u001b[0;36mbatch_neighbors\u001b[0;34m(queries, supports, q_batches, s_batches, radius)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_neighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('\\nStart training')\n",
    "print('**************')\n",
    "\n",
    "# Training\n",
    "trainer.train(net, training_loader, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Forcing exit now')\n",
    "#os.kill(os.getpid(), signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
